{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to KServe Documentation \u00b6 KServe provides a Kubernetes Custom Resource Definition for serving machine learning (ML) models on arbitrary frameworks. It aims to solve production model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX. It encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to your ML deployments. It enables a simple, pluggable, and complete story for Production ML Serving including prediction, pre-processing, post-processing and explainability.","title":"Home"},{"location":"#welcome-to-kserve-documentation","text":"KServe provides a Kubernetes Custom Resource Definition for serving machine learning (ML) models on arbitrary frameworks. It aims to solve production model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX. It encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to your ML deployments. It enables a simple, pluggable, and complete story for Production ML Serving including prediction, pre-processing, post-processing and explainability.","title":"Welcome to KServe Documentation"},{"location":"admin/serverless/","text":"Serverless Installation Guide \u00b6 KServe Serverless installation enables autoscaling based on request volume and supports scale down to and from zero. It also supports revision management and canary rollout based on revisions. Kubernetes 1.17 is the minimally recommended version and Knative Serving and Istio should be available on Kubernetes Cluster. 1. Install Istio \u00b6 The minimally required Istio version is 1.9.5 and you can refer to the Istio install guide . 2. Install Knative Serving \u00b6 The minimally required Knative Serving version is 0.19.0 and you can refer to Knative Serving install guide . Note If you are looking to use PodSpec fields such as nodeSelector, affinity or tolerations which are now supported in the v1beta1 API spec, you need to turn on the corresponding feature flags in your Knative configuration. 3. Install Cert Manager \u00b6 The minimally required Cert Manager version is 1.3.0 and you can refer to Cert Manager . Note Cert manager is required to provision webhook certs for production grade installation, alternatively you can run self signed certs generation script. 4. Install KServe \u00b6 kubectl kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.7.0-rc0/kserve.yaml","title":"Serverless installation"},{"location":"admin/serverless/#serverless-installation-guide","text":"KServe Serverless installation enables autoscaling based on request volume and supports scale down to and from zero. It also supports revision management and canary rollout based on revisions. Kubernetes 1.17 is the minimally recommended version and Knative Serving and Istio should be available on Kubernetes Cluster.","title":"Serverless Installation Guide"},{"location":"admin/serverless/#1-install-istio","text":"The minimally required Istio version is 1.9.5 and you can refer to the Istio install guide .","title":"1. Install Istio"},{"location":"admin/serverless/#2-install-knative-serving","text":"The minimally required Knative Serving version is 0.19.0 and you can refer to Knative Serving install guide . Note If you are looking to use PodSpec fields such as nodeSelector, affinity or tolerations which are now supported in the v1beta1 API spec, you need to turn on the corresponding feature flags in your Knative configuration.","title":"2. Install Knative Serving"},{"location":"admin/serverless/#3-install-cert-manager","text":"The minimally required Cert Manager version is 1.3.0 and you can refer to Cert Manager . Note Cert manager is required to provision webhook certs for production grade installation, alternatively you can run self signed certs generation script.","title":"3. Install Cert Manager"},{"location":"admin/serverless/#4-install-kserve","text":"kubectl kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.7.0-rc0/kserve.yaml","title":"4. Install KServe"},{"location":"community/adopters/","text":"Adopters of KServe \u00b6 This page contains a list of organizations who are using KServe either in production, or providing integrations or deployment options with their Cloud or product offerings. If you'd like to be included here, please send a pull request which modifies this file. Please keep the list in alphabetical order. Organization Contact Amazon Web Services Ellis Tarn Bloomberg Dan Sun Cisco Krishna Durai CoreWeave Peter Salanki Gojek Willem Pienaar Halodoc ID Joinal Ahmed IBM Animesh Singh Inspur Qingshan Chen Max Kelsen Jacob O'Farrell Nuance Jeff Griffith NVIDIA David Goodwin One Convergence Subra Ongole Seldon Clive Cox Patterson Consulting Josh Patterson Samsung SDS Hanbae Seo","title":"Adopters"},{"location":"community/adopters/#adopters-of-kserve","text":"This page contains a list of organizations who are using KServe either in production, or providing integrations or deployment options with their Cloud or product offerings. If you'd like to be included here, please send a pull request which modifies this file. Please keep the list in alphabetical order. Organization Contact Amazon Web Services Ellis Tarn Bloomberg Dan Sun Cisco Krishna Durai CoreWeave Peter Salanki Gojek Willem Pienaar Halodoc ID Joinal Ahmed IBM Animesh Singh Inspur Qingshan Chen Max Kelsen Jacob O'Farrell Nuance Jeff Griffith NVIDIA David Goodwin One Convergence Subra Ongole Seldon Clive Cox Patterson Consulting Josh Patterson Samsung SDS Hanbae Seo","title":"Adopters of KServe"},{"location":"developer/developer/","text":"Development \u00b6 This doc explains how to setup a development environment so you can get started contributing . Also take a look at: How to add and run tests Iterating Prerequisites \u00b6 Follow the instructions below to set up your development environment. Once you meet these requirements, you can make changes and deploy your own version of kserve ! Before submitting a PR, see also CONTRIBUTING.md . Install requirements \u00b6 You must install these tools: go : KServe controller is written in Go and requires Go 1.14.0+. git : For source control. Go Module : Go's new dependency management system. ko : For development. kubectl : For managing development environments. kustomize To customize YAMLs for different environments, requires v3.5.4+. yq yq is used in the project makefiles to parse and display YAML output. Please use yq version 3.* . Latest yq version 4.* has remove -d command so doesn't work with the scripts. Install Knative on a Kubernetes cluster \u00b6 KServe currently requires Knative Serving for auto-scaling, canary rollout, Istio for traffic routing and ingress. To install Knative components on your Kubernetes cluster, follow the installation guide or alternatively, use the Knative Operators to manage your installation. Observability, tracing and logging are optional but are often very valuable tools for troubleshooting difficult issues, they can be installed via the directions here . If you start from scratch, KServe requires Kubernetes 1.17+, Knative 0.19+, Istio 1.9+. If you already have Istio or Knative (e.g. from a Kubeflow install) then you don't need to install them explictly, as long as version dependencies are satisfied. Setup your environment \u00b6 To start your environment you'll need to set these environment variables (we recommend adding them to your .bashrc ): GOPATH : If you don't have one, simply pick a directory and add export GOPATH=... $GOPATH/bin on PATH : This is so that tooling installed via go get will work properly. KO_DOCKER_REPO : The docker repository to which developer images should be pushed (e.g. docker.io/<username> ). Note : Set up a docker repository for pushing images. You can use any container image registry by adjusting the authentication methods and repository paths mentioned in the sections below. Google Container Registry quickstart Docker Hub quickstart Azure Container Registry quickstart Note if you are using docker hub to store your images your KO_DOCKER_REPO variable should be docker.io/<username> . Currently Docker Hub doesn't let you create subdirs under your username. .bashrc example: export GOPATH = \" $HOME /go\" export PATH = \" ${ PATH } : ${ GOPATH } /bin\" export KO_DOCKER_REPO = 'docker.io/<username>' Checkout your fork \u00b6 The Go tools require that you clone the repository to the src/github.com/kserve/kserve directory in your GOPATH . To check out this repository: Create your own fork of this repo Clone it to your machine: mkdir -p ${ GOPATH } /src/github.com/kubeflow cd ${ GOPATH } /src/github.com/kubeflow git clone git@github.com: ${ YOUR_GITHUB_USERNAME } /kserve.git cd kserve git remote add upstream git@github.com:kserve/kserve.git git remote set-url --push upstream no_push Adding the upstream remote sets you up nicely for regularly syncing your fork . Once you reach this point you are ready to do a full build and deploy as described below. Deploy KServe \u00b6 Check Knative Serving installation \u00b6 Once you've setup your development environment , you can verify the installation with following: Success $ kubectl -n knative-serving get pods NAME READY STATUS RESTARTS AGE activator-77784645fc-t2pjf 1 /1 Running 0 11d autoscaler-6fddf74d5-z2fzf 1 /1 Running 0 11d autoscaler-hpa-5bf4476cc5-tsbw6 1 /1 Running 0 11d controller-7b8cd7f95c-6jxxj 1 /1 Running 0 11d istio-webhook-866c5bc7f8-t5ztb 1 /1 Running 0 11d networking-istio-54fb8b5d4b-xznwd 1 /1 Running 0 11d webhook-5f5f7bd9b4-cv27c 1 /1 Running 0 11d $ kubectl get gateway -n knative-serving NAME AGE knative-ingress-gateway 11d knative-local-gateway 11d $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .101.196.89 X.X.X.X 15021 :31101/TCP,80:31781/TCP,443:30372/TCP,15443:31067/TCP 11d istiod ClusterIP 10 .101.116.203 <none> 15010 /TCP,15012/TCP,443/TCP,15014/TCP,853/TCP 11d Deploy KServe from master branch \u00b6 We suggest using cert manager for provisioning the certificates for the webhook server. Other solutions should also work as long as they put the certificates in the desired location. You can follow the cert manager documentation to install it. If you don't want to install cert manager, you can set the KSERVE_ENABLE_SELF_SIGNED_CA environment variable to true. KSERVE_ENABLE_SELF_SIGNED_CA will execute a script to create a self-signed CA and patch it to the webhook config. export KSERVE_ENABLE_SELF_SIGNED_CA = true After that you can run following command to deploy KServe , you can skip above step if cert manager is already installed. make deploy Optional you can change CPU and memory limits when deploying KServe . export KSERVE_CONTROLLER_CPU_LIMIT = <cpu_limit> export KSERVE_CONTROLLER_MEMORY_LIMIT = <memory_limit> make deploy Expected Output $ kubectl get pods -n kserve -l control-plane = kserve-controller-manager NAME READY STATUS RESTARTS AGE kserve-controller-manager-0 2/2 Running 0 13m Note By default it installs to kserve namespace with the published controller manager image from master branch. Deploy KServe with your own version \u00b6 Run the following command to deploy KServe controller and model agent with your local change. make deploy-dev Note deploy-dev builds the image from your local code, publishes to KO_DOCKER_REPO and deploys the kserve-controller-manager and model agent with the image digest to your cluster for testing. Please also ensure you are logged in to KO_DOCKER_REPO from your client machine. Run the following command to deploy model server with your local change. make deploy-dev-sklearn make deploy-dev-xgb Run the following command to deploy explainer with your local change. make deploy-dev-alibi Run the following command to deploy storage initializer with your local change. make deploy-dev-storageInitializer Warning The deploy command publishes the image to KO_DOCKER_REPO with the version latest , it changes the InferenceService configmap to point to the newly built image sha. The built image is only for development and testing purpose, the current limitation is that it changes the image impacted and reset all other images including the kserver-controller-manager to use the default ones. Smoke test after deployment \u00b6 Run the following command to smoke test the deployment kubectl apply -f https://github.com/kserve/kserve/tree/master/docs/samples/v1beta1/tensorflow/tensorflow.yaml You should see model serving deployment running under default or your specified namespace. $ kubectl get pods -n default -l serving.kserve.io/inferenceservice=flowers-sample Expected Output NAME READY STATUS RESTARTS AGE flowers-sample-default-htz8r-deployment-8fd979f9b-w2qbv 3/3 Running 0 10s Running unit/integration tests \u00b6 kserver-controller-manager has a few integration tests which requires mock apiserver and etcd, they get installed along with kubebuilder . To run all unit/integration tests: make test Run e2e tests locally \u00b6 To setup from local code, do: ./hack/quick_install.sh make undeploy make deploy-dev Install pytest and test deps: pip3 install pytest==6.0.2 pytest-xdist pytest-rerunfailures pip3 install --upgrade pytest-tornasync pip3 install urllib3==1.24.2 pip3 install --upgrade setuptools Go to python/kserve and install kserve python sdk deps pip3 install -r requirements.txt python3 setup.py install --force --user Then go to test/e2e . Run kubectl create namespace kfserving-ci-e2e-test For KIND/minikube: Run export KSERVE_INGRESS_HOST_PORT=localhost:8080 In a different window run kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80 Note that not all tests will pass as the pytorch test requires gpu. These will show as pending pods at the end or you can add marker to skip the test. Run pytest > testresults.txt Tests may not clean up. To re-run, first do kubectl delete namespace kfserving-ci-e2e-test , recreate namespace and run again. Iterating \u00b6 As you make changes to the code-base, there are two special cases to be aware of: If you change an input to generated code , then you must run make manifests . Inputs include: API type definitions in apis/serving/v1beta1 , Manifests or kustomize patches stored in config . If you want to add new dependencies , then you add the imports and the specific version of the dependency module in go.mod . When it encounters an import of a package not provided by any module in go.mod , the go command automatically looks up the module containing the package and adds it to go.mod using the latest version. If you want to upgrade the dependency , then you run go get command e.g go get golang.org/x/text to upgrade to the latest version, go get golang.org/x/text@v0.3.0 to upgrade to a specific version. make deploy-dev","title":"How to contribute"},{"location":"developer/developer/#development","text":"This doc explains how to setup a development environment so you can get started contributing . Also take a look at: How to add and run tests Iterating","title":"Development"},{"location":"developer/developer/#prerequisites","text":"Follow the instructions below to set up your development environment. Once you meet these requirements, you can make changes and deploy your own version of kserve ! Before submitting a PR, see also CONTRIBUTING.md .","title":"Prerequisites"},{"location":"developer/developer/#install-requirements","text":"You must install these tools: go : KServe controller is written in Go and requires Go 1.14.0+. git : For source control. Go Module : Go's new dependency management system. ko : For development. kubectl : For managing development environments. kustomize To customize YAMLs for different environments, requires v3.5.4+. yq yq is used in the project makefiles to parse and display YAML output. Please use yq version 3.* . Latest yq version 4.* has remove -d command so doesn't work with the scripts.","title":"Install requirements"},{"location":"developer/developer/#install-knative-on-a-kubernetes-cluster","text":"KServe currently requires Knative Serving for auto-scaling, canary rollout, Istio for traffic routing and ingress. To install Knative components on your Kubernetes cluster, follow the installation guide or alternatively, use the Knative Operators to manage your installation. Observability, tracing and logging are optional but are often very valuable tools for troubleshooting difficult issues, they can be installed via the directions here . If you start from scratch, KServe requires Kubernetes 1.17+, Knative 0.19+, Istio 1.9+. If you already have Istio or Knative (e.g. from a Kubeflow install) then you don't need to install them explictly, as long as version dependencies are satisfied.","title":"Install Knative on a Kubernetes cluster"},{"location":"developer/developer/#setup-your-environment","text":"To start your environment you'll need to set these environment variables (we recommend adding them to your .bashrc ): GOPATH : If you don't have one, simply pick a directory and add export GOPATH=... $GOPATH/bin on PATH : This is so that tooling installed via go get will work properly. KO_DOCKER_REPO : The docker repository to which developer images should be pushed (e.g. docker.io/<username> ). Note : Set up a docker repository for pushing images. You can use any container image registry by adjusting the authentication methods and repository paths mentioned in the sections below. Google Container Registry quickstart Docker Hub quickstart Azure Container Registry quickstart Note if you are using docker hub to store your images your KO_DOCKER_REPO variable should be docker.io/<username> . Currently Docker Hub doesn't let you create subdirs under your username. .bashrc example: export GOPATH = \" $HOME /go\" export PATH = \" ${ PATH } : ${ GOPATH } /bin\" export KO_DOCKER_REPO = 'docker.io/<username>'","title":"Setup your environment"},{"location":"developer/developer/#checkout-your-fork","text":"The Go tools require that you clone the repository to the src/github.com/kserve/kserve directory in your GOPATH . To check out this repository: Create your own fork of this repo Clone it to your machine: mkdir -p ${ GOPATH } /src/github.com/kubeflow cd ${ GOPATH } /src/github.com/kubeflow git clone git@github.com: ${ YOUR_GITHUB_USERNAME } /kserve.git cd kserve git remote add upstream git@github.com:kserve/kserve.git git remote set-url --push upstream no_push Adding the upstream remote sets you up nicely for regularly syncing your fork . Once you reach this point you are ready to do a full build and deploy as described below.","title":"Checkout your fork"},{"location":"developer/developer/#deploy-kserve","text":"","title":"Deploy KServe"},{"location":"developer/developer/#check-knative-serving-installation","text":"Once you've setup your development environment , you can verify the installation with following: Success $ kubectl -n knative-serving get pods NAME READY STATUS RESTARTS AGE activator-77784645fc-t2pjf 1 /1 Running 0 11d autoscaler-6fddf74d5-z2fzf 1 /1 Running 0 11d autoscaler-hpa-5bf4476cc5-tsbw6 1 /1 Running 0 11d controller-7b8cd7f95c-6jxxj 1 /1 Running 0 11d istio-webhook-866c5bc7f8-t5ztb 1 /1 Running 0 11d networking-istio-54fb8b5d4b-xznwd 1 /1 Running 0 11d webhook-5f5f7bd9b4-cv27c 1 /1 Running 0 11d $ kubectl get gateway -n knative-serving NAME AGE knative-ingress-gateway 11d knative-local-gateway 11d $ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .101.196.89 X.X.X.X 15021 :31101/TCP,80:31781/TCP,443:30372/TCP,15443:31067/TCP 11d istiod ClusterIP 10 .101.116.203 <none> 15010 /TCP,15012/TCP,443/TCP,15014/TCP,853/TCP 11d","title":"Check Knative Serving installation"},{"location":"developer/developer/#deploy-kserve-from-master-branch","text":"We suggest using cert manager for provisioning the certificates for the webhook server. Other solutions should also work as long as they put the certificates in the desired location. You can follow the cert manager documentation to install it. If you don't want to install cert manager, you can set the KSERVE_ENABLE_SELF_SIGNED_CA environment variable to true. KSERVE_ENABLE_SELF_SIGNED_CA will execute a script to create a self-signed CA and patch it to the webhook config. export KSERVE_ENABLE_SELF_SIGNED_CA = true After that you can run following command to deploy KServe , you can skip above step if cert manager is already installed. make deploy Optional you can change CPU and memory limits when deploying KServe . export KSERVE_CONTROLLER_CPU_LIMIT = <cpu_limit> export KSERVE_CONTROLLER_MEMORY_LIMIT = <memory_limit> make deploy Expected Output $ kubectl get pods -n kserve -l control-plane = kserve-controller-manager NAME READY STATUS RESTARTS AGE kserve-controller-manager-0 2/2 Running 0 13m Note By default it installs to kserve namespace with the published controller manager image from master branch.","title":"Deploy KServe from master branch"},{"location":"developer/developer/#deploy-kserve-with-your-own-version","text":"Run the following command to deploy KServe controller and model agent with your local change. make deploy-dev Note deploy-dev builds the image from your local code, publishes to KO_DOCKER_REPO and deploys the kserve-controller-manager and model agent with the image digest to your cluster for testing. Please also ensure you are logged in to KO_DOCKER_REPO from your client machine. Run the following command to deploy model server with your local change. make deploy-dev-sklearn make deploy-dev-xgb Run the following command to deploy explainer with your local change. make deploy-dev-alibi Run the following command to deploy storage initializer with your local change. make deploy-dev-storageInitializer Warning The deploy command publishes the image to KO_DOCKER_REPO with the version latest , it changes the InferenceService configmap to point to the newly built image sha. The built image is only for development and testing purpose, the current limitation is that it changes the image impacted and reset all other images including the kserver-controller-manager to use the default ones.","title":"Deploy KServe with your own version"},{"location":"developer/developer/#smoke-test-after-deployment","text":"Run the following command to smoke test the deployment kubectl apply -f https://github.com/kserve/kserve/tree/master/docs/samples/v1beta1/tensorflow/tensorflow.yaml You should see model serving deployment running under default or your specified namespace. $ kubectl get pods -n default -l serving.kserve.io/inferenceservice=flowers-sample Expected Output NAME READY STATUS RESTARTS AGE flowers-sample-default-htz8r-deployment-8fd979f9b-w2qbv 3/3 Running 0 10s","title":"Smoke test after deployment"},{"location":"developer/developer/#running-unitintegration-tests","text":"kserver-controller-manager has a few integration tests which requires mock apiserver and etcd, they get installed along with kubebuilder . To run all unit/integration tests: make test","title":"Running unit/integration tests"},{"location":"developer/developer/#run-e2e-tests-locally","text":"To setup from local code, do: ./hack/quick_install.sh make undeploy make deploy-dev Install pytest and test deps: pip3 install pytest==6.0.2 pytest-xdist pytest-rerunfailures pip3 install --upgrade pytest-tornasync pip3 install urllib3==1.24.2 pip3 install --upgrade setuptools Go to python/kserve and install kserve python sdk deps pip3 install -r requirements.txt python3 setup.py install --force --user Then go to test/e2e . Run kubectl create namespace kfserving-ci-e2e-test For KIND/minikube: Run export KSERVE_INGRESS_HOST_PORT=localhost:8080 In a different window run kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80 Note that not all tests will pass as the pytorch test requires gpu. These will show as pending pods at the end or you can add marker to skip the test. Run pytest > testresults.txt Tests may not clean up. To re-run, first do kubectl delete namespace kfserving-ci-e2e-test , recreate namespace and run again.","title":"Run e2e tests locally"},{"location":"developer/developer/#iterating","text":"As you make changes to the code-base, there are two special cases to be aware of: If you change an input to generated code , then you must run make manifests . Inputs include: API type definitions in apis/serving/v1beta1 , Manifests or kustomize patches stored in config . If you want to add new dependencies , then you add the imports and the specific version of the dependency module in go.mod . When it encounters an import of a package not provided by any module in go.mod , the go command automatically looks up the module containing the package and adds it to go.mod using the latest version. If you want to upgrade the dependency , then you run go get command e.g go get golang.org/x/text to upgrade to the latest version, go get golang.org/x/text@v0.3.0 to upgrade to a specific version. make deploy-dev","title":"Iterating"},{"location":"get_started/","text":"Getting Started with KServe \u00b6 Before you begin \u00b6 Warning KServe Quickstart Environments are for experimentation use only. For production installation, see our Administrator's Guide Before you can get started with a KServe Quickstart deployment you must install kind and the Kubernetes CLI. Install Kind (Kubernetes in Docker) \u00b6 You can use kind (Kubernetes in Docker) to run a local Kubernetes cluster with Docker container nodes. Install the Kubernetes CLI \u00b6 The Kubernetes CLI ( kubectl ) , allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. Install the KServe \"Quickstart\" environment \u00b6 You can get started with a local deployment of KServe by using KServe Quick installation script on Kind : hack/quick_install.sh","title":"KServe Quickstart"},{"location":"get_started/#getting-started-with-kserve","text":"","title":"Getting Started with KServe"},{"location":"get_started/#before-you-begin","text":"Warning KServe Quickstart Environments are for experimentation use only. For production installation, see our Administrator's Guide Before you can get started with a KServe Quickstart deployment you must install kind and the Kubernetes CLI.","title":"Before you begin"},{"location":"get_started/#install-kind-kubernetes-in-docker","text":"You can use kind (Kubernetes in Docker) to run a local Kubernetes cluster with Docker container nodes.","title":"Install Kind (Kubernetes in Docker)"},{"location":"get_started/#install-the-kubernetes-cli","text":"The Kubernetes CLI ( kubectl ) , allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.","title":"Install the Kubernetes CLI"},{"location":"get_started/#install-the-kserve-quickstart-environment","text":"You can get started with a local deployment of KServe by using KServe Quick installation script on Kind : hack/quick_install.sh","title":"Install the KServe \"Quickstart\" environment"},{"location":"get_started/first_isvc/","text":"Run your first InferenceService \u00b6 1. Create test InferenceService \u00b6 kubectl create namespace kserve-test kubectl apply -f docs/modelserving/v1beta1/sklearn/v1/sklearn.yaml -n kserve-test 2. Check InferenceService status. \u00b6 kubectl get inferenceservices sklearn-iris -n kserve-test NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE sklearn-iris http://sklearn-iris.kserve-test.example.com True 100 sklearn-iris-predictor-default-47q2g 7d23h If your DNS contains example.com please consult your admin for configuring DNS or using custom domain . 3. Determine the ingress IP and ports \u00b6 Execute the following command to determine if your kubernetes cluster is running in an environment that supports external load balancers $ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.109.129 130 .211.10.121 ... 17h Load Balancer If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway. export INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].port}' ) Node Port If the EXTERNAL-IP value is none (or perpetually pending), your environment does not provide an external load balancer for the ingress gateway. In this case, you can access the gateway using the service\u2019s node port. # GKE export INGRESS_HOST = worker-node-address # Minikube export INGRESS_HOST = $( minikube ip ) # Other environment(On Prem) export INGRESS_HOST = $( kubectl get po -l istio = ingressgateway -n istio-system -o jsonpath = '{.items[0].status.hostIP}' ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) Port Forward Alternatively you can do Port Forward for testing purpose INGRESS_GATEWAY_SERVICE = $( kubectl get svc --namespace istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward --namespace istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 # start another terminal export INGRESS_HOST = localhost export INGRESS_PORT = 8080 4. Curl the InferenceService \u00b6 Real DNS If you have configured the DNS, you can directly curl the InferenceService with the URL obtained from the status print. e.g curl -v http://sklearn-iris.kserve-test.${CUSTOM_DOMAIN}/v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json Magic DNS If you don't want to go through the trouble to get a real domain, you can instead use \"magic\" dns xip.io . The key is to get the external IP for your cluster. kubectl get svc istio-ingressgateway --namespace istio-system Look for the EXTERNAL-IP column's value(in this case 35.237.217.209) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .51.253.94 35 .237.217.209 Next step is to setting up the custom domain: kubectl edit cm config-domain --namespace knative-serving Now in your editor, change example.com to {{external-ip}}.xip.io (make sure to replace {{external-ip}} with the IP you found earlier). With the change applied you can now directly curl the URL curl -v http://sklearn-iris.kserve-test.35.237.217.209.xip.io/v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json From Ingress gateway with HOST Header If you do not have DNS, you can still curl with the ingress gateway external IP using the HOST Header. SERVICE_HOSTNAME = $( kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json From local cluster gateway If you are calling from in cluster you can curl with the internal url with host {{InferenceServiceName}}.{{namespace}} curl -v http://sklearn-iris.kserve-test/v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json 5. Run Performance Test \u00b6 # use kubectl create instead of apply because the job template is using generateName which doesn't work with kubectl apply kubectl create -f docs/modelserving/v1beta1/sklearn/v1/perf.yaml -n kserve-test Expected Outpout kubectl logs load-test8b58n-rgfxr -n kserve-test Requests [total, rate, throughput] 30000, 500.02, 499.99 Duration [total, attack, wait] 1m0s, 59.998s, 3.336ms Latencies [min, mean, 50, 90, 95, 99, max] 1.743ms, 2.748ms, 2.494ms, 3.363ms, 4.091ms, 7.749ms, 46.354ms Bytes In [total, mean] 690000, 23.00 Bytes Out [total, mean] 2460000, 82.00 Success [ratio] 100.00% Status Codes [code:count] 200:30000 Error Set:","title":"First InferenceService"},{"location":"get_started/first_isvc/#run-your-first-inferenceservice","text":"","title":"Run your first InferenceService"},{"location":"get_started/first_isvc/#1-create-test-inferenceservice","text":"kubectl create namespace kserve-test kubectl apply -f docs/modelserving/v1beta1/sklearn/v1/sklearn.yaml -n kserve-test","title":"1. Create test InferenceService"},{"location":"get_started/first_isvc/#2-check-inferenceservice-status","text":"kubectl get inferenceservices sklearn-iris -n kserve-test NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE sklearn-iris http://sklearn-iris.kserve-test.example.com True 100 sklearn-iris-predictor-default-47q2g 7d23h If your DNS contains example.com please consult your admin for configuring DNS or using custom domain .","title":"2. Check InferenceService status."},{"location":"get_started/first_isvc/#3-determine-the-ingress-ip-and-ports","text":"Execute the following command to determine if your kubernetes cluster is running in an environment that supports external load balancers $ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.109.129 130 .211.10.121 ... 17h Load Balancer If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway. export INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].port}' ) Node Port If the EXTERNAL-IP value is none (or perpetually pending), your environment does not provide an external load balancer for the ingress gateway. In this case, you can access the gateway using the service\u2019s node port. # GKE export INGRESS_HOST = worker-node-address # Minikube export INGRESS_HOST = $( minikube ip ) # Other environment(On Prem) export INGRESS_HOST = $( kubectl get po -l istio = ingressgateway -n istio-system -o jsonpath = '{.items[0].status.hostIP}' ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) Port Forward Alternatively you can do Port Forward for testing purpose INGRESS_GATEWAY_SERVICE = $( kubectl get svc --namespace istio-system --selector = \"app=istio-ingressgateway\" --output jsonpath = '{.items[0].metadata.name}' ) kubectl port-forward --namespace istio-system svc/ ${ INGRESS_GATEWAY_SERVICE } 8080 :80 # start another terminal export INGRESS_HOST = localhost export INGRESS_PORT = 8080","title":"3. Determine the ingress IP and ports"},{"location":"get_started/first_isvc/#4-curl-the-inferenceservice","text":"Real DNS If you have configured the DNS, you can directly curl the InferenceService with the URL obtained from the status print. e.g curl -v http://sklearn-iris.kserve-test.${CUSTOM_DOMAIN}/v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json Magic DNS If you don't want to go through the trouble to get a real domain, you can instead use \"magic\" dns xip.io . The key is to get the external IP for your cluster. kubectl get svc istio-ingressgateway --namespace istio-system Look for the EXTERNAL-IP column's value(in this case 35.237.217.209) NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 10 .51.253.94 35 .237.217.209 Next step is to setting up the custom domain: kubectl edit cm config-domain --namespace knative-serving Now in your editor, change example.com to {{external-ip}}.xip.io (make sure to replace {{external-ip}} with the IP you found earlier). With the change applied you can now directly curl the URL curl -v http://sklearn-iris.kserve-test.35.237.217.209.xip.io/v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json From Ingress gateway with HOST Header If you do not have DNS, you can still curl with the ingress gateway external IP using the HOST Header. SERVICE_HOSTNAME = $( kubectl get inferenceservice sklearn-iris -n kserve-test -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json From local cluster gateway If you are calling from in cluster you can curl with the internal url with host {{InferenceServiceName}}.{{namespace}} curl -v http://sklearn-iris.kserve-test/v1/models/sklearn-iris:predict -d @./docs/modelserving/v1beta1/sklearn/v1/iris-input.json","title":"4. Curl the InferenceService"},{"location":"get_started/first_isvc/#5-run-performance-test","text":"# use kubectl create instead of apply because the job template is using generateName which doesn't work with kubectl apply kubectl create -f docs/modelserving/v1beta1/sklearn/v1/perf.yaml -n kserve-test Expected Outpout kubectl logs load-test8b58n-rgfxr -n kserve-test Requests [total, rate, throughput] 30000, 500.02, 499.99 Duration [total, attack, wait] 1m0s, 59.998s, 3.336ms Latencies [min, mean, 50, 90, 95, 99, max] 1.743ms, 2.748ms, 2.494ms, 3.363ms, 4.091ms, 7.749ms, 46.354ms Bytes In [total, mean] 690000, 23.00 Bytes Out [total, mean] 2460000, 82.00 Success [ratio] 100.00% Status Codes [code:count] 200:30000 Error Set:","title":"5. Run Performance Test"},{"location":"help/contributor/github/","text":"GitHub workflow for KServe documentation \u00b6 Learn how to use GitHub and contribute to the kserve/website repo. Set up your local machine \u00b6 To check out your fork of the kserve/website repository: Create your own fork of the kserve/website repo . Configure GitHub access through SSH . Clone your fork to your machine and set the upstream remote to the kserve/website repository: mkdir -p ${ GOPATH } /src/kserve.io cd ${ GOPATH } /src/kserve.io git clone git@github.com: ${ YOUR_GITHUB_USERNAME } /website.git cd docs git remote add upstream https://github.com/kserve/website.git git remote set-url --push upstream no_push You are now able to open PRs, start reviews, and contribute fixes the kserve/website repo. See the following sections to learn more. Important : Remember to regularly syncing your fork . Report documentation issues \u00b6 KServe uses Github issues to track documentation issues and requests. If you see a problem with the documentation that you're not sure how to fix, submit an issue using the following steps: Check the KServe docs issues list before creating an issue to avoid making a duplicate. Use the correct template for your new issue. There are two templates available: Bug report : If you're reporting an error in the existing documentation, use this template. This could be anything from broken samples to typos. When you create a bug report, include as many details as possible and include suggested fixes to the issue. Feature request : For upcoming changes to the documentation or requests for more information on a particular subject. Open PRs to fix documentation issues \u00b6 The KServe documentation follows the standard GitHub collaboration flow for Pull Requests (PRs). Ensure that your fork is up-to-date . Create a branch in your fork . Locate or create the file that you want to fix: If you are updating an existing page, locate that file and begin making changes. For example, from any page on kserve.io , you can click the pencil icon in the upper right corner to open that page in GitHub. If you are adding new content, you must follow the \"new docs\" instructions. To edit a file, use the new branch that you created in your fork. Navigate to that same file within your fork using the GitHub UI. Open that file from in your local clone. Create the Pull Request in the kserve/website repo . Assign an owner to the PR to request a review. Here's what generally happens after you send the PR for review: One of the assigned repo maintainers will triage the PR by assigning relative priority, adding appropriate labels, and performing an initial documentation review. If the PR involves significant technical changes, for example new features, or new and changed sample code, the PR is assigned to a Subject Matter Expert (SME), typically an engineer working on KServe, for technical review and approval. When both the technical writers and SMEs are satisfied with the quality of the writing and the technical accuracy of the content, the PR can be merged. A PR requires two labels before it can merge: lgtm and approved . The SME is responsible for reviewing the technical accuracy and adding the lgtm label. The KServe technical writers are who provide the approved label when the content meets quality, clarity, and organization standards (see Style Guide ). We appreciate contributions to the docs, so if you open a PR we will help you get it merged. Assigning owners and reviewers \u00b6 All PRs should be assigned to a single owner (\" Assignee \"). It's best to set the \"Assignee\" and include other stakeholders as \"Reviewers\" rather than leaving it unassigned or allowing Prow to auto assign reviewers. Use the /assign command to set the owner. For example: /assign @owner_id For code related changes , initially set the owner of your PR to the SME who should review for technical accuracy. If you don't know who the appropriate owner is, nor who your reviewers should be for your PR, you can assign the current working group lead of the related component. If you want to notify and include other stakeholders in your PR review, use the /cc command. For example: /cc @stakeholder_id1 @stakeholder_id2 Reviewing PRs \u00b6 See the KServe community guidelines about reviewing PRs Using Prow to manage PRs and Issues \u00b6 KServe uses several sets of tools to manage pull requests (PR)s and issues in a more fine-grained way than GitHub permissions allow. In particular, you'll regularly interact with Prow to categorize and manage issues and PRs. Prow allows control of specific GitHub functionality without granting full \"write\" access to the repo (which would allow rewriting history and other dangerous operations). You'll most often use the following commands, but Prow will also chime in on most bugs and PRs with a link to all the known commands: /assign @user1 @user2 to assign an issue or PR to specific people for review or approval. /lgtm and /approve to approve a PR. Note that anyone may /lgtm a PR, but only someone listed in an OWNERS file may /approve the PR. A PR needs both an approval and an LGTM\u2009\u2014\u2009the /lgtm review is a good opportunity for non-approvers to practice and develop reviewing skills. /lgtm is removed when a PR is updated, but /approve is sticky\u2009\u2014\u2009once applied, anyone can supply an /lgtm . Both Prow (legacy) and GitHub actions (preferred) can run tests on PRs; once all tests are passing and a PR has the lgtm and approved labels, Prow will submit the PR automatically. You can also use Prow to manage labels on PRs with /kind ... , /good-first-issue , or /area ... See Branches for details about how to use the /cherrypick command. Common GitHub PRs FAQs \u00b6 One or more tests are failing. If you do not see a specific error related to a change you made, and instead the errors are related to timeouts, try re-running the test at a later time. There are running tasks that could result in timeouts or rate limiting if your test runs at the same time. Other Issues/Unsure -- reach out in the Slack channel and someone will be happy to help out.","title":"GitHub workflow for KServe documentation"},{"location":"help/contributor/github/#github-workflow-for-kserve-documentation","text":"Learn how to use GitHub and contribute to the kserve/website repo.","title":"GitHub workflow for KServe documentation"},{"location":"help/contributor/github/#set-up-your-local-machine","text":"To check out your fork of the kserve/website repository: Create your own fork of the kserve/website repo . Configure GitHub access through SSH . Clone your fork to your machine and set the upstream remote to the kserve/website repository: mkdir -p ${ GOPATH } /src/kserve.io cd ${ GOPATH } /src/kserve.io git clone git@github.com: ${ YOUR_GITHUB_USERNAME } /website.git cd docs git remote add upstream https://github.com/kserve/website.git git remote set-url --push upstream no_push You are now able to open PRs, start reviews, and contribute fixes the kserve/website repo. See the following sections to learn more. Important : Remember to regularly syncing your fork .","title":"Set up your local machine"},{"location":"help/contributor/github/#report-documentation-issues","text":"KServe uses Github issues to track documentation issues and requests. If you see a problem with the documentation that you're not sure how to fix, submit an issue using the following steps: Check the KServe docs issues list before creating an issue to avoid making a duplicate. Use the correct template for your new issue. There are two templates available: Bug report : If you're reporting an error in the existing documentation, use this template. This could be anything from broken samples to typos. When you create a bug report, include as many details as possible and include suggested fixes to the issue. Feature request : For upcoming changes to the documentation or requests for more information on a particular subject.","title":"Report documentation issues"},{"location":"help/contributor/github/#open-prs-to-fix-documentation-issues","text":"The KServe documentation follows the standard GitHub collaboration flow for Pull Requests (PRs). Ensure that your fork is up-to-date . Create a branch in your fork . Locate or create the file that you want to fix: If you are updating an existing page, locate that file and begin making changes. For example, from any page on kserve.io , you can click the pencil icon in the upper right corner to open that page in GitHub. If you are adding new content, you must follow the \"new docs\" instructions. To edit a file, use the new branch that you created in your fork. Navigate to that same file within your fork using the GitHub UI. Open that file from in your local clone. Create the Pull Request in the kserve/website repo . Assign an owner to the PR to request a review. Here's what generally happens after you send the PR for review: One of the assigned repo maintainers will triage the PR by assigning relative priority, adding appropriate labels, and performing an initial documentation review. If the PR involves significant technical changes, for example new features, or new and changed sample code, the PR is assigned to a Subject Matter Expert (SME), typically an engineer working on KServe, for technical review and approval. When both the technical writers and SMEs are satisfied with the quality of the writing and the technical accuracy of the content, the PR can be merged. A PR requires two labels before it can merge: lgtm and approved . The SME is responsible for reviewing the technical accuracy and adding the lgtm label. The KServe technical writers are who provide the approved label when the content meets quality, clarity, and organization standards (see Style Guide ). We appreciate contributions to the docs, so if you open a PR we will help you get it merged.","title":"Open PRs to fix documentation issues"},{"location":"help/contributor/github/#assigning-owners-and-reviewers","text":"All PRs should be assigned to a single owner (\" Assignee \"). It's best to set the \"Assignee\" and include other stakeholders as \"Reviewers\" rather than leaving it unassigned or allowing Prow to auto assign reviewers. Use the /assign command to set the owner. For example: /assign @owner_id For code related changes , initially set the owner of your PR to the SME who should review for technical accuracy. If you don't know who the appropriate owner is, nor who your reviewers should be for your PR, you can assign the current working group lead of the related component. If you want to notify and include other stakeholders in your PR review, use the /cc command. For example: /cc @stakeholder_id1 @stakeholder_id2","title":"Assigning owners and reviewers"},{"location":"help/contributor/github/#reviewing-prs","text":"See the KServe community guidelines about reviewing PRs","title":"Reviewing PRs"},{"location":"help/contributor/github/#using-prow-to-manage-prs-and-issues","text":"KServe uses several sets of tools to manage pull requests (PR)s and issues in a more fine-grained way than GitHub permissions allow. In particular, you'll regularly interact with Prow to categorize and manage issues and PRs. Prow allows control of specific GitHub functionality without granting full \"write\" access to the repo (which would allow rewriting history and other dangerous operations). You'll most often use the following commands, but Prow will also chime in on most bugs and PRs with a link to all the known commands: /assign @user1 @user2 to assign an issue or PR to specific people for review or approval. /lgtm and /approve to approve a PR. Note that anyone may /lgtm a PR, but only someone listed in an OWNERS file may /approve the PR. A PR needs both an approval and an LGTM\u2009\u2014\u2009the /lgtm review is a good opportunity for non-approvers to practice and develop reviewing skills. /lgtm is removed when a PR is updated, but /approve is sticky\u2009\u2014\u2009once applied, anyone can supply an /lgtm . Both Prow (legacy) and GitHub actions (preferred) can run tests on PRs; once all tests are passing and a PR has the lgtm and approved labels, Prow will submit the PR automatically. You can also use Prow to manage labels on PRs with /kind ... , /good-first-issue , or /area ... See Branches for details about how to use the /cherrypick command.","title":"Using Prow to manage PRs and Issues"},{"location":"help/contributor/github/#common-github-prs-faqs","text":"One or more tests are failing. If you do not see a specific error related to a change you made, and instead the errors are related to timeouts, try re-running the test at a later time. There are running tasks that could result in timeouts or rate limiting if your test runs at the same time. Other Issues/Unsure -- reach out in the Slack channel and someone will be happy to help out.","title":"Common GitHub PRs FAQs"},{"location":"help/contributor/mkdocs-contributor-guide/","text":"MkDocs Contributions \u00b6 This is a temporary home for contribution guidelines for the MkDocs branch. When MkDocs becomes \"main\" this will be moved to the appropriate place on the website Install Material for MkDocs \u00b6 kserve.io uses Material for MkDocs to render documentation. Material for MkDocs is Python based and uses pip to install most of it's required packages as well as optional add-ons (which we use). You can choose to install MkDocs locally or using a Docker image. pip actually comes pre-installed with Python so it is included in many operating systems (like MacOSx or Ubuntu) but if you don\u2019t have Python, you can install it here: https://www.python.org For some (e.g. folks using RHEL), you may have to use pip3. pip pip install mkdocs-material More detailed instructions can be found here: https://squidfunk.github.io/mkdocs-material/getting-started/#installation pip3 pip3 install mkdocs-material More detailed instructions can be found here: https://squidfunk.github.io/mkdocs-material/getting-started/#installation Install KServe-Specific Extensions \u00b6 KServe uses a number of extensions to MkDocs which can also be installed using pip. If you used pip to install, run the following: pip pip install mkdocs-material-extensions mkdocs-macros-plugin mkdocs-exclude mkdocs-awesome-pages-plugin mkdocs-redirects pip3 pip3 install mkdocs-material-extensions mkdocs-macros-plugin mkdocs-exclude mkdocs-awesome-pages-plugin mkdocs-redirects Setting Up Local Preview \u00b6 Once you have installed Material for MkDocs and all of the extensions, head over to and clone the repo. In your terminal, find your way over to the location of the cloned repo. Once you are in the main folder and run: Local Preview mkdocs serve Local Preview w/ Dirty Reload If you\u2019re only changing a single page in the /docs/ folder (i.e. not the homepage or mkdocs.yml) adding the flag --dirtyreload will make the site rebuild super crazy insta-fast. mkdocs serve --dirtyreload Local Preview including Blog and Community Site First, install the necessary extensions: npm install -g postcss postcss-cli autoprefixer http-server Once you have those npm packages installed, run: ./hack/build-with-blog.sh serve Note Unfortunately, there aren\u2019t live previews for this version of the local preview. After awhile, your terminal should spit out: INFO - Documentation built in 13 .54 seconds [ I 210519 10 :47:10 server:335 ] Serving on http://127.0.0.1:8000 [ I 210519 10 :47:10 handlers:62 ] Start watching changes [ I 210519 10 :47:10 handlers:64 ] Start detecting changes Now access http://127.0.0.1:8000 and you should see the site is built! \ud83c\udf89 Anytime you change any file in your /docs/ repo and hit save, the site will automatically rebuild itself to reflect your changes! Setting Up \"Public\" Preview \u00b6 If, for whatever reason, you want to share your work before submitting a PR (where Netlify would generate a preview for you), you can deploy your changes as a Github Page easily using the following command: mkdocs gh-deploy --force INFO - Documentation built in 14 .29 seconds WARNING - Version check skipped: No version specified in previous deployment. INFO - Your documentation should shortly be available at: https://<your-github-handle>.github.io/docs/ Where <your-github-handle> is your Github handle. After a few moments, your changes should be available for public preview at the link provided by MkDocs! This means you can rapidly prototype and share your changes before making a PR! Navigation \u00b6 Navigation in MkDocs uses the \"mkdocs.yml\" file (found in the /docs directory) to organize navigation. For more in-depth information on Navigation, see: https://www.mkdocs.org/user-guide/writing-your-docs/#configure-pages-and-navigation and https://squidfunk.github.io/mkdocs-material/setup/setting-up-navigation/ Content Tabs \u00b6 Content tabs are handy way to organize lots of information in a visually pleasing way. Some documentation from https://squidfunk.github.io/mkdocs-material/reference/content-tabs/#usage is reproduced here: Grouping Code blocks Code blocks are one of the primary targets to be grouped, and can be considered a special case of content tabs, as tabs with a single code block are always rendered without horizontal spacing. Example: === \"C\" ``` c #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); return 0; } ``` === \"C++\" ``` c++ #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; } ``` Result: C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Grouping other content When a content tab contains more than one code block, it is rendered with horizontal spacing. Vertical spacing is never added, but can be achieved by nesting tabs in other blocks. Example: === \"Unordered list\" * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci === \"Ordered list\" 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result: Unordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci For more information, see: https://squidfunk.github.io/mkdocs-material/reference/content-tabs/#usage File Includes (Content Reuse) \u00b6 KServe strives to reduce duplicative effort by reusing commonly used bits of information, see the docs/snippet directory for some examples. Snippets does not require a specific extension, and as long as a valid file name is specified, it will attempt to process it. Snippets can handle recursive file inclusion. And if Snippets encounters the same file in the current stack, it will avoid re-processing it in order to avoid an infinite loop (or crash on hitting max recursion depth). For more info, see: https://facelessuser.github.io/pymdown-extensions/extensions/snippets/ Admonitions \u00b6 We use the following admonition boxes only. Use admonitions sparingly; too many admonitions can be distracting. Admonitions Note A Note contains information that is useful, but not essential. A reader can skip a note without bypassing required information. If the information suggests an action to take, use a tip instead. Tip A Tip suggests an helpful, but not mandatory, action to take. Warning A Warning draws attention to potential trouble. Formatting !!! note A Note contains information that is useful, but not essential. A reader can skip a note without bypassing required information. If the information suggests an action to take, use a tip instead. !!! tip A Tip suggests a helpful, but not mandatory, action to take. !!! warning A Warning draws attention to potential trouble. Icons and Emojis \u00b6 Material for MkDocs supports using Material Icons and Emojis using easy shortcodes. Emojs Formatting :taco: To search a database of Icons and Emojis (all of which can be used on kserve.io), as well as usage information, see: https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/#search Redirects \u00b6 The KServe site uses mkdocs-redirects to \"redirect\" users from a page that may no longer exist (or has been moved) to their desired location. Adding re-directs to the KServe site is done in one centralized place, docs/config/redirects.yml . The format is shown here: plugins: redirects: redirect_maps: ... path_to_old_or_moved_URL : path_to_new_URL","title":"MkDocs Contributions"},{"location":"help/contributor/mkdocs-contributor-guide/#mkdocs-contributions","text":"This is a temporary home for contribution guidelines for the MkDocs branch. When MkDocs becomes \"main\" this will be moved to the appropriate place on the website","title":"MkDocs Contributions"},{"location":"help/contributor/mkdocs-contributor-guide/#install-material-for-mkdocs","text":"kserve.io uses Material for MkDocs to render documentation. Material for MkDocs is Python based and uses pip to install most of it's required packages as well as optional add-ons (which we use). You can choose to install MkDocs locally or using a Docker image. pip actually comes pre-installed with Python so it is included in many operating systems (like MacOSx or Ubuntu) but if you don\u2019t have Python, you can install it here: https://www.python.org For some (e.g. folks using RHEL), you may have to use pip3. pip pip install mkdocs-material More detailed instructions can be found here: https://squidfunk.github.io/mkdocs-material/getting-started/#installation pip3 pip3 install mkdocs-material More detailed instructions can be found here: https://squidfunk.github.io/mkdocs-material/getting-started/#installation","title":"Install Material for MkDocs"},{"location":"help/contributor/mkdocs-contributor-guide/#install-kserve-specific-extensions","text":"KServe uses a number of extensions to MkDocs which can also be installed using pip. If you used pip to install, run the following: pip pip install mkdocs-material-extensions mkdocs-macros-plugin mkdocs-exclude mkdocs-awesome-pages-plugin mkdocs-redirects pip3 pip3 install mkdocs-material-extensions mkdocs-macros-plugin mkdocs-exclude mkdocs-awesome-pages-plugin mkdocs-redirects","title":"Install KServe-Specific Extensions"},{"location":"help/contributor/mkdocs-contributor-guide/#setting-up-local-preview","text":"Once you have installed Material for MkDocs and all of the extensions, head over to and clone the repo. In your terminal, find your way over to the location of the cloned repo. Once you are in the main folder and run: Local Preview mkdocs serve Local Preview w/ Dirty Reload If you\u2019re only changing a single page in the /docs/ folder (i.e. not the homepage or mkdocs.yml) adding the flag --dirtyreload will make the site rebuild super crazy insta-fast. mkdocs serve --dirtyreload Local Preview including Blog and Community Site First, install the necessary extensions: npm install -g postcss postcss-cli autoprefixer http-server Once you have those npm packages installed, run: ./hack/build-with-blog.sh serve Note Unfortunately, there aren\u2019t live previews for this version of the local preview. After awhile, your terminal should spit out: INFO - Documentation built in 13 .54 seconds [ I 210519 10 :47:10 server:335 ] Serving on http://127.0.0.1:8000 [ I 210519 10 :47:10 handlers:62 ] Start watching changes [ I 210519 10 :47:10 handlers:64 ] Start detecting changes Now access http://127.0.0.1:8000 and you should see the site is built! \ud83c\udf89 Anytime you change any file in your /docs/ repo and hit save, the site will automatically rebuild itself to reflect your changes!","title":"Setting Up Local Preview"},{"location":"help/contributor/mkdocs-contributor-guide/#setting-up-public-preview","text":"If, for whatever reason, you want to share your work before submitting a PR (where Netlify would generate a preview for you), you can deploy your changes as a Github Page easily using the following command: mkdocs gh-deploy --force INFO - Documentation built in 14 .29 seconds WARNING - Version check skipped: No version specified in previous deployment. INFO - Your documentation should shortly be available at: https://<your-github-handle>.github.io/docs/ Where <your-github-handle> is your Github handle. After a few moments, your changes should be available for public preview at the link provided by MkDocs! This means you can rapidly prototype and share your changes before making a PR!","title":"Setting Up \"Public\" Preview"},{"location":"help/contributor/mkdocs-contributor-guide/#navigation","text":"Navigation in MkDocs uses the \"mkdocs.yml\" file (found in the /docs directory) to organize navigation. For more in-depth information on Navigation, see: https://www.mkdocs.org/user-guide/writing-your-docs/#configure-pages-and-navigation and https://squidfunk.github.io/mkdocs-material/setup/setting-up-navigation/","title":"Navigation"},{"location":"help/contributor/mkdocs-contributor-guide/#content-tabs","text":"Content tabs are handy way to organize lots of information in a visually pleasing way. Some documentation from https://squidfunk.github.io/mkdocs-material/reference/content-tabs/#usage is reproduced here: Grouping Code blocks Code blocks are one of the primary targets to be grouped, and can be considered a special case of content tabs, as tabs with a single code block are always rendered without horizontal spacing. Example: === \"C\" ``` c #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); return 0; } ``` === \"C++\" ``` c++ #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; } ``` Result: C #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } C++ #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Grouping other content When a content tab contains more than one code block, it is rendered with horizontal spacing. Vertical spacing is never added, but can be achieved by nesting tabs in other blocks. Example: === \"Unordered list\" * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci === \"Ordered list\" 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result: Unordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci For more information, see: https://squidfunk.github.io/mkdocs-material/reference/content-tabs/#usage","title":"Content Tabs"},{"location":"help/contributor/mkdocs-contributor-guide/#file-includes-content-reuse","text":"KServe strives to reduce duplicative effort by reusing commonly used bits of information, see the docs/snippet directory for some examples. Snippets does not require a specific extension, and as long as a valid file name is specified, it will attempt to process it. Snippets can handle recursive file inclusion. And if Snippets encounters the same file in the current stack, it will avoid re-processing it in order to avoid an infinite loop (or crash on hitting max recursion depth). For more info, see: https://facelessuser.github.io/pymdown-extensions/extensions/snippets/","title":"File Includes (Content Reuse)"},{"location":"help/contributor/mkdocs-contributor-guide/#admonitions","text":"We use the following admonition boxes only. Use admonitions sparingly; too many admonitions can be distracting. Admonitions Note A Note contains information that is useful, but not essential. A reader can skip a note without bypassing required information. If the information suggests an action to take, use a tip instead. Tip A Tip suggests an helpful, but not mandatory, action to take. Warning A Warning draws attention to potential trouble. Formatting !!! note A Note contains information that is useful, but not essential. A reader can skip a note without bypassing required information. If the information suggests an action to take, use a tip instead. !!! tip A Tip suggests a helpful, but not mandatory, action to take. !!! warning A Warning draws attention to potential trouble.","title":"Admonitions"},{"location":"help/contributor/mkdocs-contributor-guide/#icons-and-emojis","text":"Material for MkDocs supports using Material Icons and Emojis using easy shortcodes. Emojs Formatting :taco: To search a database of Icons and Emojis (all of which can be used on kserve.io), as well as usage information, see: https://squidfunk.github.io/mkdocs-material/reference/icons-emojis/#search","title":"Icons and Emojis"},{"location":"help/contributor/mkdocs-contributor-guide/#redirects","text":"The KServe site uses mkdocs-redirects to \"redirect\" users from a page that may no longer exist (or has been moved) to their desired location. Adding re-directs to the KServe site is done in one centralized place, docs/config/redirects.yml . The format is shown here: plugins: redirects: redirect_maps: ... path_to_old_or_moved_URL : path_to_new_URL","title":"Redirects"},{"location":"help/contributor/templates/template-blog/","text":"Blog template instructions \u00b6 An example template with best-practices that you can use to start drafting an entry to post on the KServe blog. Copy a version of this template without the instructions Include a commented-out table with tracking info about reviews and approvals: <!-- | Reviewer | Date | Approval | | ------------------ | ---------- | ------------- | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | --> Blog content body \u00b6 <!-- Introduce the feature you are going to explain: * state what the goal of this blog entry is * how you use the feature * make sure to link to the corresponding docs * why others can find it useful (why its important) --> <!-- Add/create as many distinct Steps or Sections as needed. --> Example step/section 1: \u00b6 <!-- An introductory sentence about this step or section (ie. why its important and what the result is). Don't forget to link to any new or related concepts that you mention here. --> Example step/section 2: \u00b6 <!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. --> Example step/section 3: \u00b6 <!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. --> Example section about results \u00b6 <!-- Tie it all together and briefly revisit the main key points and then the overall result/goal/importance --> Further reading \u00b6 <!-- Add any links to other related resources that users might find useful. What's the next step? --> About the author \u00b6 <!-- Add a short bio of yourself here --> Copy the template \u00b6 <!-- | Reviewer | Date | Approval | | ------------------ | ---------- | ------------- | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | --> # <!-- Insert blog title here --> ## Blog content body <!-- Introduce the feature you are going to explain: * state what the goal of this blog entry is * how you use the feature * make sure to link to the corresponding docs * why others can find it useful (why its important) --> <!-- Add/create as many distinct Steps or Sections as needed. --> ### Example step/section 1: <!-- An introductory sentence about this step or section (ie. why its important and what the result is). Don't forget to link to any new or related concepts that you mention here. --> ### Example step/section 2: <!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. --> ### Example step/section 3: <!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. --> ### Example section about results <!-- Tie it all together and briefly revisit the main key points and then the overall result/goal/importance --> ## Further reading <!-- Add any links to related resources that users might find useful. What's the next step? --> ## About the author <!-- Add a short bio of yourself here -->","title":"Blog template instructions"},{"location":"help/contributor/templates/template-blog/#blog-template-instructions","text":"An example template with best-practices that you can use to start drafting an entry to post on the KServe blog. Copy a version of this template without the instructions Include a commented-out table with tracking info about reviews and approvals: <!-- | Reviewer | Date | Approval | | ------------------ | ---------- | ------------- | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | -->","title":"Blog template instructions"},{"location":"help/contributor/templates/template-blog/#blog-content-body","text":"<!-- Introduce the feature you are going to explain: * state what the goal of this blog entry is * how you use the feature * make sure to link to the corresponding docs * why others can find it useful (why its important) --> <!-- Add/create as many distinct Steps or Sections as needed. -->","title":"Blog content body"},{"location":"help/contributor/templates/template-blog/#example-stepsection-1","text":"<!-- An introductory sentence about this step or section (ie. why its important and what the result is). Don't forget to link to any new or related concepts that you mention here. -->","title":"Example step/section 1:"},{"location":"help/contributor/templates/template-blog/#example-stepsection-2","text":"<!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. -->","title":"Example step/section 2:"},{"location":"help/contributor/templates/template-blog/#example-stepsection-3","text":"<!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. -->","title":"Example step/section 3:"},{"location":"help/contributor/templates/template-blog/#example-section-about-results","text":"<!-- Tie it all together and briefly revisit the main key points and then the overall result/goal/importance -->","title":"Example section about results"},{"location":"help/contributor/templates/template-blog/#further-reading","text":"<!-- Add any links to other related resources that users might find useful. What's the next step? -->","title":"Further reading"},{"location":"help/contributor/templates/template-blog/#about-the-author","text":"<!-- Add a short bio of yourself here -->","title":"About the author"},{"location":"help/contributor/templates/template-blog/#copy-the-template","text":"<!-- | Reviewer | Date | Approval | | ------------------ | ---------- | ------------- | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | | <!-- GitHub ID --> | YYYY-MM-DD | :+1:, :monocle_face:, :-1: | --> # <!-- Insert blog title here --> ## Blog content body <!-- Introduce the feature you are going to explain: * state what the goal of this blog entry is * how you use the feature * make sure to link to the corresponding docs * why others can find it useful (why its important) --> <!-- Add/create as many distinct Steps or Sections as needed. --> ### Example step/section 1: <!-- An introductory sentence about this step or section (ie. why its important and what the result is). Don't forget to link to any new or related concepts that you mention here. --> ### Example step/section 2: <!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. --> ### Example step/section 3: <!-- An introductory sentence about this step or section (ie. why its important, how it relates to the one before, and what the result is) Don't forget to link to any new or related concepts that you mention here. --> ### Example section about results <!-- Tie it all together and briefly revisit the main key points and then the overall result/goal/importance --> ## Further reading <!-- Add any links to related resources that users might find useful. What's the next step? --> ## About the author <!-- Add a short bio of yourself here -->","title":"Copy the template"},{"location":"help/contributor/templates/template-concept/","text":"Concept Template \u00b6 Use this template when writing conceptual topics. Conceptual topics explain how things work or what things mean. They provide helpful context to readers. They do not include procedures. Template \u00b6 The following template includes the standard sections that should appear in conceptual topics, including a topic introduction sentence, an overview, and placeholders for additional sections and subsections. Copy and paste the markdown from the template to use it in your topic. This topic describes... Write a sentence or two that describes the topic itself, not the subject of the topic. The goal of the topic sentence is to help readers understand if this topic is for them. For example, \"This topic describes what KServe is and how it works.\" ## Overview Write a few sentences describing the subject of the topic. ## Section Title Write a sentence or two to describe the content in this section. Create more sections as necessary. Optionally, add two or more subsections to each section. Do not skip header levels: H2 >> H3, not H2 >> H4. ### Subsection Title Write a sentence or two to describe the content in this section. ### Subsection Title Write a sentence or two to describe the content in this section. Conceptual Content Samples \u00b6 This section provides common content types that appear in conceptual topics. Copy and paste the markdown to use it in your topic. Table \u00b6 Introduce the table with a sentence. For example, \u201cThe following table lists which features are available to a KServe supported ML framework.\u201d Markdown Table Template \u00b6 Header 1 Header 2 Data1 Data2 Data3 Data4 Ordered List \u00b6 Write a sentence or two to introduce the content of the list. For example, \u201cIf you want to fix or add content to a past release, you can find the source files in the following folders.\u201d. Optionally, include bold lead-ins before each list item. Markdown Ordered List Templates \u00b6 Item 1 Item 2 Item 3 Lead-in description: Item 1 Lead-in description: Item 2 Lead-in description: Item 3 Unordered List \u00b6 Write a sentence or two to introduce the content of the list. For example, \u201cYour own path to becoming a KServe contributor can begin in any of the following components:\u201d. Optionally, include bold lead-ins before each list item. Markdown Unordered List Template \u00b6 List item List item List item Lead-in : List item Lead-in : List item Lead-in : List item Note \u00b6 Ensure the text beneath the note is indented as much as note is. Note This is a note. Warning \u00b6 If the note regards an issue that could lead to data loss, the note should be a warning. Warning This is a warning.","title":"Concept Template"},{"location":"help/contributor/templates/template-concept/#concept-template","text":"Use this template when writing conceptual topics. Conceptual topics explain how things work or what things mean. They provide helpful context to readers. They do not include procedures.","title":"Concept Template"},{"location":"help/contributor/templates/template-concept/#template","text":"The following template includes the standard sections that should appear in conceptual topics, including a topic introduction sentence, an overview, and placeholders for additional sections and subsections. Copy and paste the markdown from the template to use it in your topic. This topic describes... Write a sentence or two that describes the topic itself, not the subject of the topic. The goal of the topic sentence is to help readers understand if this topic is for them. For example, \"This topic describes what KServe is and how it works.\" ## Overview Write a few sentences describing the subject of the topic. ## Section Title Write a sentence or two to describe the content in this section. Create more sections as necessary. Optionally, add two or more subsections to each section. Do not skip header levels: H2 >> H3, not H2 >> H4. ### Subsection Title Write a sentence or two to describe the content in this section. ### Subsection Title Write a sentence or two to describe the content in this section.","title":"Template"},{"location":"help/contributor/templates/template-concept/#conceptual-content-samples","text":"This section provides common content types that appear in conceptual topics. Copy and paste the markdown to use it in your topic.","title":"Conceptual Content Samples"},{"location":"help/contributor/templates/template-concept/#table","text":"Introduce the table with a sentence. For example, \u201cThe following table lists which features are available to a KServe supported ML framework.\u201d","title":"Table"},{"location":"help/contributor/templates/template-concept/#markdown-table-template","text":"Header 1 Header 2 Data1 Data2 Data3 Data4","title":"Markdown Table Template"},{"location":"help/contributor/templates/template-concept/#ordered-list","text":"Write a sentence or two to introduce the content of the list. For example, \u201cIf you want to fix or add content to a past release, you can find the source files in the following folders.\u201d. Optionally, include bold lead-ins before each list item.","title":"Ordered List"},{"location":"help/contributor/templates/template-concept/#markdown-ordered-list-templates","text":"Item 1 Item 2 Item 3 Lead-in description: Item 1 Lead-in description: Item 2 Lead-in description: Item 3","title":"Markdown Ordered List Templates"},{"location":"help/contributor/templates/template-concept/#unordered-list","text":"Write a sentence or two to introduce the content of the list. For example, \u201cYour own path to becoming a KServe contributor can begin in any of the following components:\u201d. Optionally, include bold lead-ins before each list item.","title":"Unordered List"},{"location":"help/contributor/templates/template-concept/#markdown-unordered-list-template","text":"List item List item List item Lead-in : List item Lead-in : List item Lead-in : List item","title":"Markdown Unordered List Template"},{"location":"help/contributor/templates/template-concept/#note","text":"Ensure the text beneath the note is indented as much as note is. Note This is a note.","title":"Note"},{"location":"help/contributor/templates/template-concept/#warning","text":"If the note regards an issue that could lead to data loss, the note should be a warning. Warning This is a warning.","title":"Warning"},{"location":"help/contributor/templates/template-procedure/","text":"Procedure template \u00b6 Use this template when writing procedural (how-to) topics. Procedural topics include detailed steps to perform a task as well as some context about the task. Template \u00b6 The following template includes the standard sections that should appear in procedural topics, including a topic sentence, an overview section, and sections for each task within the procedure. Copy and paste the markdown from the template to use it in your topic. This topic describes... Write a sentence or two that describes the topic itself, not the subject of the topic. The goal of the topic sentence is to help readers understand if this topic is for them. For example, \"This topic instructs how to serve a TensorFlow model.\" ## Overview Write a few sentences to describe the subject of the topic, if useful. For example, if the topic is about configuring a broker, you might provide some useful context about brokers. If there are multiple tasks in the procedure and they must be completed in order, create an ordered list that contains each task in the topic. Use bullets for sub-tasks. Include anchor links to the headings for each task. To [task]: 1. [Name of Task 1 (for example, Apply default configuration)](#task-1) 1. [Optional: Name of Task 2](#task-2) !!! note Unless the number of tasks in the procedure is particularly high, do not use numbered lead-ins in the task headings. For example, instead of \"Task 1: Apply default configuration\", use \"Apply default configuration\". ## Prerequisites Use one of the following formats for the Prerequisites section. ### Formatting for two or more prerequisites If there are two or more prerequisites, use the following format. Include links for more information, if necessary. Before you [task], you must have/do: * Prerequisite. See [Link](). * Prerequisite. See [Link](). For example: Before you deploy PyTorch model, you must have: * KServe. See [Installing the KServe](link-to-that-topic). * An Apache Kafka cluster. See [Link to Instructions to Download](link-to-that-topic). ### Format for one prerequisite If there is one prerequisite, use the following format. Include a link for more information, if necessary. Before you [task], you must have/do [prerequisite]. See [Link](link). For example: Before you create the `InferenceService`, you must have a Kubernetes cluster with KServe installed and DNS configured. See the [installation instructions](../../../install/README.md) if you need to create one. ## Task 1 Write a few sentences to describe the task and provide additional context on the task. !!! note When writing a single-step procedure, write the step in one sentence and make it a bullet. The signposting is important given readers are strongly inclined to look for numbered steps and bullet points when searching for instructions. If possible, expand the procedure to include at least one more step. Few procedures truly require a single step. [Task]: 1. Step 1 1. Step 2 ## Optional: Task 2 If the task is optional, put \"Optional:\" in the heading. Write a few sentences to describe the task and provide additional context on the task. [Task]: 1. Step 1 2. Step 2 Procedure Content Samples \u00b6 This section provides common content types that appear in procedural topics. Copy and paste the markdown to use it in your topic. \u201cFill-in-the-Fields\u201d Table \u00b6 Where the reader must enter many values in, for example, a YAML file, use a table within the procedure as follows: Open the YAML file. Key1 : Value1 Key2 : Value2 metadata : annotations : # case-sensitive Key3 : Value3 Key4 : Value4 Key5 : Value5 spec : # Configuration specific to this broker. config : Key6 : Value6 Change the relevant values to your needs, using the following table as a guide. Key Value Type Description Key1 String Description Key2 Integer Description Key3 String Description Key4 String Description Key5 Float Description Key6 String Description Table \u00b6 Introduce the table with a sentence. For example, \u201cThe following table lists which features are available to a KServe supported ML framework. Markdown Table Template \u00b6 Header 1 Header 2 Data1 Data2 Data3 Data4 Ordered List \u00b6 Write a sentence or two to introduce the content of the list. For example, \u201cIf you want to fix or add content to a past release, you can find the source files in the following folders.\u201d. Optionally, include bold lead-ins before each list item. Markdown Ordered List Templates \u00b6 Item 1 Item 2 Item 3 Lead-in description: Item 1 Lead-in description: Item 2 Lead-in description: Item 3 Unordered List \u00b6 Write a sentence or two to introduce the content of the list. For example, \u201cYour own path to becoming a KServe contributor can begin in any of the following components:\u201d. Optionally, include bold lead-ins before each list item. Markdown Unordered List Template \u00b6 List item List item List item Lead-in : List item Lead-in : List item Lead-in : List item Note \u00b6 Ensure the text beneath the note is indented as much as note is. Note This is a note. Warning \u00b6 If the note regards an issue that could lead to data loss, the note should be a warning. Warning This is a warning. Markdown Embedded Image \u00b6 The following is an embedded image reference in markdown. Tabs \u00b6 Place multiple versions of the same procedure (such as a CLI procedure vs a YAML procedure) within tabs. Indent the opening tabs tags 3 spaces to make the tabs display properly. == \"tab1 name\" This is a stem: 1. This is a step. ``` This is some code. ``` 1. This is another step. == \"tab2 name\" This is a stem: 1. This is a step. ``` This is some code. ``` 1. This is another step. Documenting Code and Code Snippets \u00b6 For instructions on how to format code and code snippets, see the Style Guide.","title":"Procedure template"},{"location":"help/contributor/templates/template-procedure/#procedure-template","text":"Use this template when writing procedural (how-to) topics. Procedural topics include detailed steps to perform a task as well as some context about the task.","title":"Procedure template"},{"location":"help/contributor/templates/template-procedure/#template","text":"The following template includes the standard sections that should appear in procedural topics, including a topic sentence, an overview section, and sections for each task within the procedure. Copy and paste the markdown from the template to use it in your topic. This topic describes... Write a sentence or two that describes the topic itself, not the subject of the topic. The goal of the topic sentence is to help readers understand if this topic is for them. For example, \"This topic instructs how to serve a TensorFlow model.\" ## Overview Write a few sentences to describe the subject of the topic, if useful. For example, if the topic is about configuring a broker, you might provide some useful context about brokers. If there are multiple tasks in the procedure and they must be completed in order, create an ordered list that contains each task in the topic. Use bullets for sub-tasks. Include anchor links to the headings for each task. To [task]: 1. [Name of Task 1 (for example, Apply default configuration)](#task-1) 1. [Optional: Name of Task 2](#task-2) !!! note Unless the number of tasks in the procedure is particularly high, do not use numbered lead-ins in the task headings. For example, instead of \"Task 1: Apply default configuration\", use \"Apply default configuration\". ## Prerequisites Use one of the following formats for the Prerequisites section. ### Formatting for two or more prerequisites If there are two or more prerequisites, use the following format. Include links for more information, if necessary. Before you [task], you must have/do: * Prerequisite. See [Link](). * Prerequisite. See [Link](). For example: Before you deploy PyTorch model, you must have: * KServe. See [Installing the KServe](link-to-that-topic). * An Apache Kafka cluster. See [Link to Instructions to Download](link-to-that-topic). ### Format for one prerequisite If there is one prerequisite, use the following format. Include a link for more information, if necessary. Before you [task], you must have/do [prerequisite]. See [Link](link). For example: Before you create the `InferenceService`, you must have a Kubernetes cluster with KServe installed and DNS configured. See the [installation instructions](../../../install/README.md) if you need to create one. ## Task 1 Write a few sentences to describe the task and provide additional context on the task. !!! note When writing a single-step procedure, write the step in one sentence and make it a bullet. The signposting is important given readers are strongly inclined to look for numbered steps and bullet points when searching for instructions. If possible, expand the procedure to include at least one more step. Few procedures truly require a single step. [Task]: 1. Step 1 1. Step 2 ## Optional: Task 2 If the task is optional, put \"Optional:\" in the heading. Write a few sentences to describe the task and provide additional context on the task. [Task]: 1. Step 1 2. Step 2","title":"Template"},{"location":"help/contributor/templates/template-procedure/#procedure-content-samples","text":"This section provides common content types that appear in procedural topics. Copy and paste the markdown to use it in your topic.","title":"Procedure Content Samples"},{"location":"help/contributor/templates/template-procedure/#fill-in-the-fields-table","text":"Where the reader must enter many values in, for example, a YAML file, use a table within the procedure as follows: Open the YAML file. Key1 : Value1 Key2 : Value2 metadata : annotations : # case-sensitive Key3 : Value3 Key4 : Value4 Key5 : Value5 spec : # Configuration specific to this broker. config : Key6 : Value6 Change the relevant values to your needs, using the following table as a guide. Key Value Type Description Key1 String Description Key2 Integer Description Key3 String Description Key4 String Description Key5 Float Description Key6 String Description","title":"\u201cFill-in-the-Fields\u201d Table"},{"location":"help/contributor/templates/template-procedure/#table","text":"Introduce the table with a sentence. For example, \u201cThe following table lists which features are available to a KServe supported ML framework.","title":"Table"},{"location":"help/contributor/templates/template-procedure/#markdown-table-template","text":"Header 1 Header 2 Data1 Data2 Data3 Data4","title":"Markdown Table Template"},{"location":"help/contributor/templates/template-procedure/#ordered-list","text":"Write a sentence or two to introduce the content of the list. For example, \u201cIf you want to fix or add content to a past release, you can find the source files in the following folders.\u201d. Optionally, include bold lead-ins before each list item.","title":"Ordered List"},{"location":"help/contributor/templates/template-procedure/#markdown-ordered-list-templates","text":"Item 1 Item 2 Item 3 Lead-in description: Item 1 Lead-in description: Item 2 Lead-in description: Item 3","title":"Markdown Ordered List Templates"},{"location":"help/contributor/templates/template-procedure/#unordered-list","text":"Write a sentence or two to introduce the content of the list. For example, \u201cYour own path to becoming a KServe contributor can begin in any of the following components:\u201d. Optionally, include bold lead-ins before each list item.","title":"Unordered List"},{"location":"help/contributor/templates/template-procedure/#markdown-unordered-list-template","text":"List item List item List item Lead-in : List item Lead-in : List item Lead-in : List item","title":"Markdown Unordered List Template"},{"location":"help/contributor/templates/template-procedure/#note","text":"Ensure the text beneath the note is indented as much as note is. Note This is a note.","title":"Note"},{"location":"help/contributor/templates/template-procedure/#warning","text":"If the note regards an issue that could lead to data loss, the note should be a warning. Warning This is a warning.","title":"Warning"},{"location":"help/contributor/templates/template-procedure/#markdown-embedded-image","text":"The following is an embedded image reference in markdown.","title":"Markdown Embedded Image"},{"location":"help/contributor/templates/template-procedure/#tabs","text":"Place multiple versions of the same procedure (such as a CLI procedure vs a YAML procedure) within tabs. Indent the opening tabs tags 3 spaces to make the tabs display properly. == \"tab1 name\" This is a stem: 1. This is a step. ``` This is some code. ``` 1. This is another step. == \"tab2 name\" This is a stem: 1. This is a step. ``` This is some code. ``` 1. This is another step.","title":"Tabs"},{"location":"help/contributor/templates/template-procedure/#documenting-code-and-code-snippets","text":"For instructions on how to format code and code snippets, see the Style Guide.","title":"Documenting Code and Code Snippets"},{"location":"help/contributor/templates/template-troubleshooting/","text":"Troubleshooting template \u00b6 When writing guidance to help to troubleshoot specific errors, the error must include: Error Description: To describe the error very briefly so that users can search for it easily. Symptom: To describe the error in a way that helps users to diagnose their issue. Include error messages or anything else users might see if they encounter this error. Explanation (or cause): To inform users about why they are seeing this error. This can be omitted if the cause of the error is unknown. Solution: To inform the user about how to fix the error. Example Troubleshooting Table \u00b6 Troubleshooting \u00b6 | Error Description | |----------|------------| | Symptom | During the event something breaks. | | Cause | The thing is broken. | | Solution | To solve this issue, do the following: 1. This. 2. That. |","title":"Troubleshooting template"},{"location":"help/contributor/templates/template-troubleshooting/#troubleshooting-template","text":"When writing guidance to help to troubleshoot specific errors, the error must include: Error Description: To describe the error very briefly so that users can search for it easily. Symptom: To describe the error in a way that helps users to diagnose their issue. Include error messages or anything else users might see if they encounter this error. Explanation (or cause): To inform users about why they are seeing this error. This can be omitted if the cause of the error is unknown. Solution: To inform the user about how to fix the error.","title":"Troubleshooting template"},{"location":"help/contributor/templates/template-troubleshooting/#example-troubleshooting-table","text":"","title":"Example Troubleshooting Table"},{"location":"help/contributor/templates/template-troubleshooting/#troubleshooting","text":"| Error Description | |----------|------------| | Symptom | During the event something breaks. | | Cause | The thing is broken. | | Solution | To solve this issue, do the following: 1. This. 2. That. |","title":"Troubleshooting"},{"location":"help/style-guide/documenting-code/","text":"Documenting Code \u00b6 Words requiring code formatting \u00b6 Apply code formatting only to special-purpose text: Filenames Path names Fields and values from a YAML file Any text that goes into a CLI CLI names Specify the programming language \u00b6 Specify the language your code is in as part of the code block Specify non-language specific code, like CLI commands, with ```bash. See the following examples for formatting. Correct package main import \"fmt\" func main () { fmt . Println ( \"hello world\" ) } Incorrect package main import \"fmt\" func main () { fmt.Println ( \"hello world\" ) } Correct Formatting ```go package main import \"fmt\" func main() { fmt.Println(\"hello world\") } ``` Incorrect Formatting ```bash package main import \"fmt\" func main() { fmt.Println(\"hello world\") } ``` Documenting YAML \u00b6 When documenting YAML, use two steps. Use step 1 to create the YAML file, and step 2 to apply the YAML file. Use kubectl apply for files/objects that the user creates: it works for both \u201ccreate\u201d and \u201cupdate\u201d, and the source of truth is their local files. Use kubectl edit for files which are shipped as part of the KServe software, like the KServe ConfigMaps. Write ```yaml at the beginning of your code block if you are typing YAML code as part of a CLI command. Correct Creating or updating a resource: Create a YAML file using the following template: # YAML FILE CONTENTS Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Editing a ConfigMap: kubectl -n <namespace> edit configmap <resource-name> Incorrect Example 1: cat <<EOF | kubectl create -f - # code EOF Example 2: kubectl apply -f - <<EOF # code EOF Referencing variables in code blocks \u00b6 Format variables in code blocks like so: All lowercase Hyphens between words Explanation for each variable below code block Explanation format is \u201cWhere... <service-name> is\u2026\" Single variable \u00b6 Correct kubectl get isvc <service-name> Where <service-name> is the name of your InferenceService. Incorrect kubectl get isvc { SERVICE_NAME } {SERVICE_NAME} = The name of your service Multiple variables \u00b6 Correct kn create service <service-name> --revision-name <revision-name> Where: <service-name> is the name of your Knative Service. <revision-name> is the desired name of your revision. Incorrect kn create service <service-name> --revision-name <revision-name> Where <service-name> is the name of your Knative Service. Where <revision-name> is the desired name of your revision. CLI output \u00b6 CLI Output should include the custom css \"{ .bash .no-copy }\" in place of \"bash\" which removes the \"Copy to clipboard button\" on the right side of the code block Correct <some-code> Incorrect <some-code> Correct Formatting ```{ .bash .no-copy } <some-code> ``` Incorrect Formatting ```bash <some-code> ```","title":"Documenting Code"},{"location":"help/style-guide/documenting-code/#documenting-code","text":"","title":"Documenting Code"},{"location":"help/style-guide/documenting-code/#words-requiring-code-formatting","text":"Apply code formatting only to special-purpose text: Filenames Path names Fields and values from a YAML file Any text that goes into a CLI CLI names","title":"Words requiring code formatting"},{"location":"help/style-guide/documenting-code/#specify-the-programming-language","text":"Specify the language your code is in as part of the code block Specify non-language specific code, like CLI commands, with ```bash. See the following examples for formatting. Correct package main import \"fmt\" func main () { fmt . Println ( \"hello world\" ) } Incorrect package main import \"fmt\" func main () { fmt.Println ( \"hello world\" ) } Correct Formatting ```go package main import \"fmt\" func main() { fmt.Println(\"hello world\") } ``` Incorrect Formatting ```bash package main import \"fmt\" func main() { fmt.Println(\"hello world\") } ```","title":"Specify the programming language"},{"location":"help/style-guide/documenting-code/#documenting-yaml","text":"When documenting YAML, use two steps. Use step 1 to create the YAML file, and step 2 to apply the YAML file. Use kubectl apply for files/objects that the user creates: it works for both \u201ccreate\u201d and \u201cupdate\u201d, and the source of truth is their local files. Use kubectl edit for files which are shipped as part of the KServe software, like the KServe ConfigMaps. Write ```yaml at the beginning of your code block if you are typing YAML code as part of a CLI command. Correct Creating or updating a resource: Create a YAML file using the following template: # YAML FILE CONTENTS Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Editing a ConfigMap: kubectl -n <namespace> edit configmap <resource-name> Incorrect Example 1: cat <<EOF | kubectl create -f - # code EOF Example 2: kubectl apply -f - <<EOF # code EOF","title":"Documenting YAML"},{"location":"help/style-guide/documenting-code/#referencing-variables-in-code-blocks","text":"Format variables in code blocks like so: All lowercase Hyphens between words Explanation for each variable below code block Explanation format is \u201cWhere... <service-name> is\u2026\"","title":"Referencing variables in code blocks"},{"location":"help/style-guide/documenting-code/#single-variable","text":"Correct kubectl get isvc <service-name> Where <service-name> is the name of your InferenceService. Incorrect kubectl get isvc { SERVICE_NAME } {SERVICE_NAME} = The name of your service","title":"Single variable"},{"location":"help/style-guide/documenting-code/#multiple-variables","text":"Correct kn create service <service-name> --revision-name <revision-name> Where: <service-name> is the name of your Knative Service. <revision-name> is the desired name of your revision. Incorrect kn create service <service-name> --revision-name <revision-name> Where <service-name> is the name of your Knative Service. Where <revision-name> is the desired name of your revision.","title":"Multiple variables"},{"location":"help/style-guide/documenting-code/#cli-output","text":"CLI Output should include the custom css \"{ .bash .no-copy }\" in place of \"bash\" which removes the \"Copy to clipboard button\" on the right side of the code block Correct <some-code> Incorrect <some-code> Correct Formatting ```{ .bash .no-copy } <some-code> ``` Incorrect Formatting ```bash <some-code> ```","title":"CLI output"},{"location":"help/style-guide/style-and-formatting/","text":"Formatting standards and conventions \u00b6 Titles and headings \u00b6 Use sentence case for titles and headings \u00b6 Only capitalize proper nouns, acronyms, and the first word of the heading. Correct Incorrect ## Configure the feature ## Configure the Feature ### Using feature ### Using Feature ### Using HTTPS ### Using https Do not use code formatting inside headings \u00b6 Correct Incorrect ## Configure the class annotation ## Configure the `class` annotation Use imperatives for headings of procedures \u00b6 For consistency, brevity, and to better signpost where action is expected of the reader, make procedure headings imperatives. Correct Incorrect ## Install KServe ## Installation of KServe ### Configure DNS ### Configuring DNS ## Verify the installation ## How to verify the installation Links \u00b6 Describe what the link targets \u00b6 Correct Incorrect For an explanation of what makes a good hyperlink, see this this article . See this article here . Write links in Markdown, not HTML \u00b6 Correct Incorrect [Kafka Broker](../kafka-broker/README.md) <a href=\"../kafka-broker/README.md\">Kafka Broker</a> [Kafka Broker](../kafka-broker/README.md){target=_blank} <a href=\"../kafka-broker/README.md\" target=\"_blank\">Kafka Broker</a> Include the .md extension in internal links \u00b6 Correct Incorrect [Setting up a custom domain](../serving/using-a-custom-domain.md) [Setting up a custom domain](../serving/using-a-custom-domain) Link to files, not folders \u00b6 Correct Incorrect [Kafka Broker](../kafka-broker/README.md) [Kafka Broker](../kafka-broker/) Ensure the letter case is correct \u00b6 Correct Incorrect [Kafka Broker](../kafka-broker/README.md) [Kafka Broker](../kafka-broker/readme.md) Formatting \u00b6 Use nonbreaking spaces in units of measurement other than percent \u00b6 For most units of measurement, when you specify a number with the unit, use a nonbreaking space between the number and the unit. Don't use spacing when the unit of measurement is percent. Correct Incorrect 3 &nbsp GB 3 GB 4 &nbsp CPUs 4 CPUs 14% 14 &nbsp % Use bold for user interface elements \u00b6 Correct Incorrect Click Fork Click \"Fork\" Select Other Select \"Other\" Use tables for definition lists \u00b6 When listing terms and their definitions, use table formatting instead of definition list formatting. Correct Incorrect |Value |Description | |------|---------------------| |Value1|Description of Value1| |Value2|Description of Value2| Value1 : Description of Value1 Value2 : Description of Value2 General style \u00b6 Use upper camel case for KServe API objects \u00b6 Correct Incorrect Explainers explainers Transformer transformer InferenceService Inference Service Only use parentheses for acronym explanations \u00b6 Put an acronym inside parentheses after its explanation. Don\u2019t use parentheses for anything else. Parenthetical statements especially should be avoided because readers skip them. If something is important enough to be in the sentence, it should be fully part of that sentence. Correct Incorrect Custom Resource Definition (CRD) Check your CLI (you should see it there) Knative Serving creates a Revision Knative creates a Revision (a stateless, snapshot in time of your code and configuration) Use the international standard for punctuation inside quotes \u00b6 Correct Incorrect Events are recorded with an associated \"stage\". Events are recorded with an associated \"stage.\" The copy is called a \"fork\". The copy is called a \"fork.\"","title":"Formatting standards and conventions"},{"location":"help/style-guide/style-and-formatting/#formatting-standards-and-conventions","text":"","title":"Formatting standards and conventions"},{"location":"help/style-guide/style-and-formatting/#titles-and-headings","text":"","title":"Titles and headings"},{"location":"help/style-guide/style-and-formatting/#use-sentence-case-for-titles-and-headings","text":"Only capitalize proper nouns, acronyms, and the first word of the heading. Correct Incorrect ## Configure the feature ## Configure the Feature ### Using feature ### Using Feature ### Using HTTPS ### Using https","title":"Use sentence case for titles and headings"},{"location":"help/style-guide/style-and-formatting/#do-not-use-code-formatting-inside-headings","text":"Correct Incorrect ## Configure the class annotation ## Configure the `class` annotation","title":"Do not use code formatting inside headings"},{"location":"help/style-guide/style-and-formatting/#use-imperatives-for-headings-of-procedures","text":"For consistency, brevity, and to better signpost where action is expected of the reader, make procedure headings imperatives. Correct Incorrect ## Install KServe ## Installation of KServe ### Configure DNS ### Configuring DNS ## Verify the installation ## How to verify the installation","title":"Use imperatives for headings of procedures"},{"location":"help/style-guide/style-and-formatting/#links","text":"","title":"Links"},{"location":"help/style-guide/style-and-formatting/#describe-what-the-link-targets","text":"Correct Incorrect For an explanation of what makes a good hyperlink, see this this article . See this article here .","title":"Describe what the link targets"},{"location":"help/style-guide/style-and-formatting/#write-links-in-markdown-not-html","text":"Correct Incorrect [Kafka Broker](../kafka-broker/README.md) <a href=\"../kafka-broker/README.md\">Kafka Broker</a> [Kafka Broker](../kafka-broker/README.md){target=_blank} <a href=\"../kafka-broker/README.md\" target=\"_blank\">Kafka Broker</a>","title":"Write links in Markdown, not HTML"},{"location":"help/style-guide/style-and-formatting/#include-the-md-extension-in-internal-links","text":"Correct Incorrect [Setting up a custom domain](../serving/using-a-custom-domain.md) [Setting up a custom domain](../serving/using-a-custom-domain)","title":"Include the .md extension in internal links"},{"location":"help/style-guide/style-and-formatting/#link-to-files-not-folders","text":"Correct Incorrect [Kafka Broker](../kafka-broker/README.md) [Kafka Broker](../kafka-broker/)","title":"Link to files, not folders"},{"location":"help/style-guide/style-and-formatting/#ensure-the-letter-case-is-correct","text":"Correct Incorrect [Kafka Broker](../kafka-broker/README.md) [Kafka Broker](../kafka-broker/readme.md)","title":"Ensure the letter case is correct"},{"location":"help/style-guide/style-and-formatting/#formatting","text":"","title":"Formatting"},{"location":"help/style-guide/style-and-formatting/#use-nonbreaking-spaces-in-units-of-measurement-other-than-percent","text":"For most units of measurement, when you specify a number with the unit, use a nonbreaking space between the number and the unit. Don't use spacing when the unit of measurement is percent. Correct Incorrect 3 &nbsp GB 3 GB 4 &nbsp CPUs 4 CPUs 14% 14 &nbsp %","title":"Use nonbreaking spaces in units of measurement other than percent"},{"location":"help/style-guide/style-and-formatting/#use-bold-for-user-interface-elements","text":"Correct Incorrect Click Fork Click \"Fork\" Select Other Select \"Other\"","title":"Use bold for user interface elements"},{"location":"help/style-guide/style-and-formatting/#use-tables-for-definition-lists","text":"When listing terms and their definitions, use table formatting instead of definition list formatting. Correct Incorrect |Value |Description | |------|---------------------| |Value1|Description of Value1| |Value2|Description of Value2| Value1 : Description of Value1 Value2 : Description of Value2","title":"Use tables for definition lists"},{"location":"help/style-guide/style-and-formatting/#general-style","text":"","title":"General style"},{"location":"help/style-guide/style-and-formatting/#use-upper-camel-case-for-kserve-api-objects","text":"Correct Incorrect Explainers explainers Transformer transformer InferenceService Inference Service","title":"Use upper camel case for KServe API objects"},{"location":"help/style-guide/style-and-formatting/#only-use-parentheses-for-acronym-explanations","text":"Put an acronym inside parentheses after its explanation. Don\u2019t use parentheses for anything else. Parenthetical statements especially should be avoided because readers skip them. If something is important enough to be in the sentence, it should be fully part of that sentence. Correct Incorrect Custom Resource Definition (CRD) Check your CLI (you should see it there) Knative Serving creates a Revision Knative creates a Revision (a stateless, snapshot in time of your code and configuration)","title":"Only use parentheses for acronym explanations"},{"location":"help/style-guide/style-and-formatting/#use-the-international-standard-for-punctuation-inside-quotes","text":"Correct Incorrect Events are recorded with an associated \"stage\". Events are recorded with an associated \"stage.\" The copy is called a \"fork\". The copy is called a \"fork.\"","title":"Use the international standard for punctuation inside quotes"},{"location":"help/style-guide/voice-and-language/","text":"Voice and language \u00b6 Use present tense \u00b6 Correct Incorrect This command starts a proxy. This command will start a proxy. Use active voice \u00b6 Correct Incorrect You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file. Use simple and direct language \u00b6 Use simple and direct language. Avoid using unnecessary words, such as \"please\". Correct Incorrect To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods. Address the reader as \"you\", not \"we\" \u00b6 Correct Incorrect You can create a Deployment by ... We can create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ... This page teaches you how to use pods. In this page, we are going to learn about pods. Avoid jargon, idioms, and Latin \u00b6 Some readers speak English as a second language. Avoid jargon, idioms, and Latin to help make their understanding easier. Correct Incorrect Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ... For example, ... e.g., ... Enter through the gateway ... Enter via the gateway ... Avoid statements about the future \u00b6 Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a boilerplate under the front matter that identifies the information accordingly. Avoid statements that will soon be out of date \u00b6 Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Correct Incorrect In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ... Avoid words that assume a specific level of understanding \u00b6 Avoid words such as \"just\", \"simply\", \"easy\", \"easily\", or \"simple\". These words do not add value. Correct Incorrect Include one command in ... Include just one command in ... Run the container ... Simply run the container ... You can remove ... You can easily remove ... These steps ... These simple steps ...","title":"Voice and language"},{"location":"help/style-guide/voice-and-language/#voice-and-language","text":"","title":"Voice and language"},{"location":"help/style-guide/voice-and-language/#use-present-tense","text":"Correct Incorrect This command starts a proxy. This command will start a proxy.","title":"Use present tense"},{"location":"help/style-guide/voice-and-language/#use-active-voice","text":"Correct Incorrect You can explore the API using a browser. The API can be explored using a browser. The YAML file specifies the replica count. The replica count is specified in the YAML file.","title":"Use active voice"},{"location":"help/style-guide/voice-and-language/#use-simple-and-direct-language","text":"Use simple and direct language. Avoid using unnecessary words, such as \"please\". Correct Incorrect To create a ReplicaSet , ... In order to create a ReplicaSet , ... See the configuration file. Please see the configuration file. View the Pods. With this next command, we'll view the Pods.","title":"Use simple and direct language"},{"location":"help/style-guide/voice-and-language/#address-the-reader-as-you-not-we","text":"Correct Incorrect You can create a Deployment by ... We can create a Deployment by ... In the preceding output, you can see... In the preceding output, we can see ... This page teaches you how to use pods. In this page, we are going to learn about pods.","title":"Address the reader as \"you\", not \"we\""},{"location":"help/style-guide/voice-and-language/#avoid-jargon-idioms-and-latin","text":"Some readers speak English as a second language. Avoid jargon, idioms, and Latin to help make their understanding easier. Correct Incorrect Internally, ... Under the hood, ... Create a new cluster. Turn up a new cluster. Initially, ... Out of the box, ... For example, ... e.g., ... Enter through the gateway ... Enter via the gateway ...","title":"Avoid jargon, idioms, and Latin"},{"location":"help/style-guide/voice-and-language/#avoid-statements-about-the-future","text":"Avoid making promises or giving hints about the future. If you need to talk about a feature in development, add a boilerplate under the front matter that identifies the information accordingly.","title":"Avoid statements about the future"},{"location":"help/style-guide/voice-and-language/#avoid-statements-that-will-soon-be-out-of-date","text":"Avoid using wording that becomes outdated quickly like \"currently\" and \"new\". A feature that is new today is not new for long. Correct Incorrect In version 1.4, ... In the current version, ... The Federation feature provides ... The new Federation feature provides ...","title":"Avoid statements that will soon be out of date"},{"location":"help/style-guide/voice-and-language/#avoid-words-that-assume-a-specific-level-of-understanding","text":"Avoid words such as \"just\", \"simply\", \"easy\", \"easily\", or \"simple\". These words do not add value. Correct Incorrect Include one command in ... Include just one command in ... Run the container ... Simply run the container ... You can remove ... You can easily remove ... These steps ... These simple steps ...","title":"Avoid words that assume a specific level of understanding"},{"location":"modelserving/control_plane/","text":"Control Plane \u00b6 KServe Control Plane: Responsible for reconciling the InferenceService custom resources. Additionally, it creates the serverless deployment for predictor, transformer, explainer to enable autoscaling based on incoming request workload including scaling down to zero when no traffic is received. Control Plane Components \u00b6 Knative Serving Controller : Responsible for service revision management, creating network routing resources, serverless container with queue proxy to expose traffic metrics and enforce concurrency limit. Knative Activator : Brings back scaled-to-zero pods and forwards requests. Knative Autoscaler(KPA) : Watches traffic flow to the application, and scales replicas up or down based on configured metrics. KServe Controller : Responsible for creating service, ingress resources, model server container and model agent container for request/response logging , batching and model pulling. Ingress Gateway : Gateway for routing external or internal requests.","title":"Control Plane"},{"location":"modelserving/control_plane/#control-plane","text":"KServe Control Plane: Responsible for reconciling the InferenceService custom resources. Additionally, it creates the serverless deployment for predictor, transformer, explainer to enable autoscaling based on incoming request workload including scaling down to zero when no traffic is received.","title":"Control Plane"},{"location":"modelserving/control_plane/#control-plane-components","text":"Knative Serving Controller : Responsible for service revision management, creating network routing resources, serverless container with queue proxy to expose traffic metrics and enforce concurrency limit. Knative Activator : Brings back scaled-to-zero pods and forwards requests. Knative Autoscaler(KPA) : Watches traffic flow to the application, and scales replicas up or down based on configured metrics. KServe Controller : Responsible for creating service, ingress resources, model server container and model agent container for request/response logging , batching and model pulling. Ingress Gateway : Gateway for routing external or internal requests.","title":"Control Plane Components"},{"location":"modelserving/data_plane/","text":"Data Plane \u00b6 The InferenceService Data Plane architecture consists of a static graph of components which coordinate requests for a single model. Advanced features such as Ensembling, A/B testing, and Multi-Arm-Bandits should compose InferenceServices together. Concepts \u00b6 Component : Each endpoint is composed of multiple components: \"predictor\", \"explainer\", and \"transformer\". The only required component is the predictor, which is the core of the system. As KServe evolves, we plan to increase the number of supported components to enable use cases like Outlier Detection. Predictor : The predictor is the workhorse of the InferenceService. It is simply a model and a model server that makes it available at a network endpoint. Explainer : The explainer enables an optional alternate data plane that provides model explanations in addition to predictions. Users may define their own explanation container, which configures with relevant environment variables like prediction endpoint. For common use cases, KServe provides out-of-the-box explainers like Alibi. Transformer : The transformer enables users to define a pre and post processing step before the prediction and explanation workflows. Like the explainer, it is configured with relevant environment variables too. For common use cases, KServe provides out-of-the-box transformers like Feast. Data Plane (V1) \u00b6 KServe has a standardized prediction workflow across all model frameworks. API Verb Path Payload Readiness GET /v1/models/ Response:{\"name\": , \"ready\": true/false} Predict POST /v1/models/ :predict Request:{\"instances\": []} Response:{\"predictions\": []} Explain POST /v1/models/ :explain Request:{\"instances\": []} Response:{\"predictions\": [], \"explainations\": []} Predict \u00b6 All InferenceServices speak the Tensorflow V1 HTTP API . Note: Only Tensorflow models support the fields \"signature_name\" and \"inputs\". Explain \u00b6 All InferenceServices that are deployed with an Explainer support a standardized explanation API. This interface is identical to the Tensorflow V1 HTTP API with the addition of an \":explain\" verb. Data Plane (V2) \u00b6 The second version of the data-plane protocol addresses several issues found with the V1 data-plane protocol, including performance and generality across a large number of model frameworks and servers. Predict \u00b6 The V2 protocol proposes both HTTP/REST and GRPC APIs. See the complete proposal for more information.","title":"Data Plane"},{"location":"modelserving/data_plane/#data-plane","text":"The InferenceService Data Plane architecture consists of a static graph of components which coordinate requests for a single model. Advanced features such as Ensembling, A/B testing, and Multi-Arm-Bandits should compose InferenceServices together.","title":"Data Plane"},{"location":"modelserving/data_plane/#concepts","text":"Component : Each endpoint is composed of multiple components: \"predictor\", \"explainer\", and \"transformer\". The only required component is the predictor, which is the core of the system. As KServe evolves, we plan to increase the number of supported components to enable use cases like Outlier Detection. Predictor : The predictor is the workhorse of the InferenceService. It is simply a model and a model server that makes it available at a network endpoint. Explainer : The explainer enables an optional alternate data plane that provides model explanations in addition to predictions. Users may define their own explanation container, which configures with relevant environment variables like prediction endpoint. For common use cases, KServe provides out-of-the-box explainers like Alibi. Transformer : The transformer enables users to define a pre and post processing step before the prediction and explanation workflows. Like the explainer, it is configured with relevant environment variables too. For common use cases, KServe provides out-of-the-box transformers like Feast.","title":"Concepts"},{"location":"modelserving/data_plane/#data-plane-v1","text":"KServe has a standardized prediction workflow across all model frameworks. API Verb Path Payload Readiness GET /v1/models/ Response:{\"name\": , \"ready\": true/false} Predict POST /v1/models/ :predict Request:{\"instances\": []} Response:{\"predictions\": []} Explain POST /v1/models/ :explain Request:{\"instances\": []} Response:{\"predictions\": [], \"explainations\": []}","title":"Data Plane (V1)"},{"location":"modelserving/data_plane/#predict","text":"All InferenceServices speak the Tensorflow V1 HTTP API . Note: Only Tensorflow models support the fields \"signature_name\" and \"inputs\".","title":"Predict"},{"location":"modelserving/data_plane/#explain","text":"All InferenceServices that are deployed with an Explainer support a standardized explanation API. This interface is identical to the Tensorflow V1 HTTP API with the addition of an \":explain\" verb.","title":"Explain"},{"location":"modelserving/data_plane/#data-plane-v2","text":"The second version of the data-plane protocol addresses several issues found with the V1 data-plane protocol, including performance and generality across a large number of model frameworks and servers.","title":"Data Plane (V2)"},{"location":"modelserving/data_plane/#predict_1","text":"The V2 protocol proposes both HTTP/REST and GRPC APIs. See the complete proposal for more information.","title":"Predict"},{"location":"modelserving/detect/aif/germancredit/","text":"Bias detection on an InferenceService using AIF360 \u00b6 This is an example of how to get bias metrics using AI Fairness 360 (AIF360) on KServe. AI Fairness 360, an LF AI incubation project, is an extensible open source toolkit that can help users examine, report, and mitigate discrimination and bias in machine learning models throughout the AI application lifecycle. We will be using the German Credit dataset maintained by the UC Irvine Machine Learning Repository . The German Credit dataset is a dataset that contains data as to whether or not a creditor gave a loan applicant access to a loan along with data about the applicant. The data includes relevant data on an applicant's credit history, savings, and employment as well as some data on the applicant's demographic such as age, sex, and marital status. Data like credit history, savings, and employment can be used by creditors to accurately predict the probability that an applicant will repay their loans, however, data such as age and sex should not be used to decide whether an applicant should be given a loan. We would like to be able to check if these \"protected classes\" are being used in a model's predictions. In this example we will feed the model some predictions and calculate metrics based off of the predictions the model makes. We will be using KServe payload logging capability collect the metrics. These metrics will give insight as to whether or not the model is biased for or against any protected classes. In this example we will look at the bias our deployed model has on those of age > 25 vs. those of age <= 25 and see if creditors are treating either unfairly. Create the InferenceService \u00b6 Apply the CRD kubectl kubectl apply -f bias.yaml Expected Output $ inferenceservice.serving.kserve.io/german-credit created Deploy the message dumper (sample backend receiver for payload logs) \u00b6 Apply the message-dumper CRD which will collect the logs that are created when running predictions on the inferenceservice. In production setup, instead of message-dumper Kafka can be used to receive payload logs kubectl kubectl apply -f message-dumper.yaml Expected Output service.serving.knative.dev/message-dumper created Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=german-credit SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) python simulate_predicts.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict ${SERVICE_HOSTNAME} Process payload logs for metrics calculation \u00b6 Run json_from_logs.py which will craft a payload that AIF can interpret. First, the events logs are taken from the message-dumper and then those logs are parsed to match inputs with outputs. Then the input/outputs pairs are all combined into a list of inputs and a list of outputs for AIF to interpret. A data.json file should have been created in this folder which contains the json payload. python json_from_logs.py Run an explanation \u00b6 Finally, now that we have collected a number of our model's predictions and their corresponding inputs we will send these to the AIF server to calculate the bias metrics. python query_bias.py http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ $MODEL_NAME :explain ${ SERVICE_HOSTNAME } data.json Interpreting the results \u00b6 Now let's look at one of the metrics. In this example disparate impact represents the ratio between the probability of applicants of the privileged class (age > 25) getting a loan and the probability of applicants of the unprivileged class (age <= 25) getting a loan P(Y=1|D=privileged)/P(Y=1|D=unprivileged) . Since, in the sample output below, the disparate impact is less that 1 then the probability that an applicant whose age is greater than 25 gets a loan is significantly higher than the probability that an applicant whose age is less than or equal to 25 gets a loan. This in and of itself is not proof that the model is biased, but does hint that there may be some bias and a deeper look may be needed. python query_bias.py http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ $MODEL_NAME :explain ${ SERVICE_HOSTNAME } data.json Expected Output Sending bias query... TIME TAKEN: 0.21137404441833496 <Response [200]> base_rate : 0.9329608938547486 consistency : [0.982122905027933] disparate_impact : 0.52 num_instances : 179.0 num_negatives : 12.0 num_positives : 167.0 statistical_parity_difference : -0.48 Dataset \u00b6 The dataset used in this example is the German Credit dataset maintained by the UC Irvine Machine Learning Repository . Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"AIF Bias Detector"},{"location":"modelserving/detect/aif/germancredit/#bias-detection-on-an-inferenceservice-using-aif360","text":"This is an example of how to get bias metrics using AI Fairness 360 (AIF360) on KServe. AI Fairness 360, an LF AI incubation project, is an extensible open source toolkit that can help users examine, report, and mitigate discrimination and bias in machine learning models throughout the AI application lifecycle. We will be using the German Credit dataset maintained by the UC Irvine Machine Learning Repository . The German Credit dataset is a dataset that contains data as to whether or not a creditor gave a loan applicant access to a loan along with data about the applicant. The data includes relevant data on an applicant's credit history, savings, and employment as well as some data on the applicant's demographic such as age, sex, and marital status. Data like credit history, savings, and employment can be used by creditors to accurately predict the probability that an applicant will repay their loans, however, data such as age and sex should not be used to decide whether an applicant should be given a loan. We would like to be able to check if these \"protected classes\" are being used in a model's predictions. In this example we will feed the model some predictions and calculate metrics based off of the predictions the model makes. We will be using KServe payload logging capability collect the metrics. These metrics will give insight as to whether or not the model is biased for or against any protected classes. In this example we will look at the bias our deployed model has on those of age > 25 vs. those of age <= 25 and see if creditors are treating either unfairly.","title":"Bias detection on an InferenceService using AIF360"},{"location":"modelserving/detect/aif/germancredit/#create-the-inferenceservice","text":"Apply the CRD kubectl kubectl apply -f bias.yaml Expected Output $ inferenceservice.serving.kserve.io/german-credit created","title":"Create the InferenceService"},{"location":"modelserving/detect/aif/germancredit/#deploy-the-message-dumper-sample-backend-receiver-for-payload-logs","text":"Apply the message-dumper CRD which will collect the logs that are created when running predictions on the inferenceservice. In production setup, instead of message-dumper Kafka can be used to receive payload logs kubectl kubectl apply -f message-dumper.yaml Expected Output service.serving.knative.dev/message-dumper created","title":"Deploy the message dumper (sample backend receiver for payload logs)"},{"location":"modelserving/detect/aif/germancredit/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=german-credit SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) python simulate_predicts.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict ${SERVICE_HOSTNAME}","title":"Run a prediction"},{"location":"modelserving/detect/aif/germancredit/#process-payload-logs-for-metrics-calculation","text":"Run json_from_logs.py which will craft a payload that AIF can interpret. First, the events logs are taken from the message-dumper and then those logs are parsed to match inputs with outputs. Then the input/outputs pairs are all combined into a list of inputs and a list of outputs for AIF to interpret. A data.json file should have been created in this folder which contains the json payload. python json_from_logs.py","title":"Process payload logs for metrics calculation"},{"location":"modelserving/detect/aif/germancredit/#run-an-explanation","text":"Finally, now that we have collected a number of our model's predictions and their corresponding inputs we will send these to the AIF server to calculate the bias metrics. python query_bias.py http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ $MODEL_NAME :explain ${ SERVICE_HOSTNAME } data.json","title":"Run an explanation"},{"location":"modelserving/detect/aif/germancredit/#interpreting-the-results","text":"Now let's look at one of the metrics. In this example disparate impact represents the ratio between the probability of applicants of the privileged class (age > 25) getting a loan and the probability of applicants of the unprivileged class (age <= 25) getting a loan P(Y=1|D=privileged)/P(Y=1|D=unprivileged) . Since, in the sample output below, the disparate impact is less that 1 then the probability that an applicant whose age is greater than 25 gets a loan is significantly higher than the probability that an applicant whose age is less than or equal to 25 gets a loan. This in and of itself is not proof that the model is biased, but does hint that there may be some bias and a deeper look may be needed. python query_bias.py http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ $MODEL_NAME :explain ${ SERVICE_HOSTNAME } data.json Expected Output Sending bias query... TIME TAKEN: 0.21137404441833496 <Response [200]> base_rate : 0.9329608938547486 consistency : [0.982122905027933] disparate_impact : 0.52 num_instances : 179.0 num_negatives : 12.0 num_positives : 167.0 statistical_parity_difference : -0.48","title":"Interpreting the results"},{"location":"modelserving/detect/aif/germancredit/#dataset","text":"The dataset used in this example is the German Credit dataset maintained by the UC Irvine Machine Learning Repository . Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.","title":"Dataset"},{"location":"modelserving/detect/aif/germancredit/server/","text":"Logistic Regression Model on the German Credit dataset \u00b6 Build a development docker image \u00b6 To build a development image first download these files and move them into the server/ folder - https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data - https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc First build your docker image by changing directory to kserve/python and replacing dockeruser with your docker username in the snippet below (running this will take some time). docker build -t dockeruser/aifserver:latest -f aiffairness.Dockerfile . Then push your docker image to your dockerhub repo (this will take some time) docker push dockeruser/aifserver:latest Once your docker image is pushed you can pull the image from dockeruser/aifserver:latest when deploying an inferenceservice by specifying the image in the yaml file.","title":"Logistic Regression Model on the German Credit dataset"},{"location":"modelserving/detect/aif/germancredit/server/#logistic-regression-model-on-the-german-credit-dataset","text":"","title":"Logistic Regression Model on the German Credit dataset"},{"location":"modelserving/detect/aif/germancredit/server/#build-a-development-docker-image","text":"To build a development image first download these files and move them into the server/ folder - https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data - https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc First build your docker image by changing directory to kserve/python and replacing dockeruser with your docker username in the snippet below (running this will take some time). docker build -t dockeruser/aifserver:latest -f aiffairness.Dockerfile . Then push your docker image to your dockerhub repo (this will take some time) docker push dockeruser/aifserver:latest Once your docker image is pushed you can pull the image from dockeruser/aifserver:latest when deploying an inferenceservice by specifying the image in the yaml file.","title":"Build a development docker image"},{"location":"modelserving/detect/alibi_detect/alibi_detect/","text":"Deploy InferenceService with Alibi Outlier/Drift Detector \u00b6 In order to trust and reliably act on model predictions, it is crucial to monitor the distribution of the incoming requests via various different type of detectors. KServe integrates Alibi Detect with the following components: Drift detector checks when the distribution of incoming requests is diverging from a reference distribution such as that of the training data. Outlier detector flags single instances which do not follow the training distribution. The architecture used is shown below and links the payload logging available within KServe with asynchronous processing of those payloads in KNative to detect outliers. CIFAR10 Outlier Detector \u00b6 A CIFAR10 Outlier Detector. Run the notebook demo to test. The notebook requires KNative Eventing >= 0.18. CIFAR10 Drift Detector \u00b6 A CIFAR10 Drift Detector. Run the notebook demo to test. The notebook requires KNative Eventing >= 0.18.","title":"Alibi Detector"},{"location":"modelserving/detect/alibi_detect/alibi_detect/#deploy-inferenceservice-with-alibi-outlierdrift-detector","text":"In order to trust and reliably act on model predictions, it is crucial to monitor the distribution of the incoming requests via various different type of detectors. KServe integrates Alibi Detect with the following components: Drift detector checks when the distribution of incoming requests is diverging from a reference distribution such as that of the training data. Outlier detector flags single instances which do not follow the training distribution. The architecture used is shown below and links the payload logging available within KServe with asynchronous processing of those payloads in KNative to detect outliers.","title":"Deploy InferenceService with Alibi Outlier/Drift Detector"},{"location":"modelserving/detect/alibi_detect/alibi_detect/#cifar10-outlier-detector","text":"A CIFAR10 Outlier Detector. Run the notebook demo to test. The notebook requires KNative Eventing >= 0.18.","title":"CIFAR10 Outlier Detector"},{"location":"modelserving/detect/alibi_detect/alibi_detect/#cifar10-drift-detector","text":"A CIFAR10 Drift Detector. Run the notebook demo to test. The notebook requires KNative Eventing >= 0.18.","title":"CIFAR10 Drift Detector"},{"location":"modelserving/detect/art/mnist/","text":"Using ART to get adversarial examples for MNIST classifications \u00b6 This is an example to show how adversarially modified inputs can trick models to predict incorrectly to highlight model vulnerability to adversarial attacks. It is using the Adversarial Robustness Toolbox (ART) on KServe. ART provides tools that enable developers to evaluate, defend, and verify ML models and applications against adversarial threats. Apart from giving capabilities to craft adversarial attacks , it also provides algorithms to defend against them. We will be using the MNIST dataset which is a dataset of handwritten digits and find adversarial examples which will can make the model predict a classification incorrectly, thereby showing the vulnerability of the model against adversarial attacks. To deploy the inferenceservice with v1beta1 API kubectl apply -f art.yaml Then find the url. kubectl get inferenceservice NAME URL READY DEFAULT TRAFFIC CANARY TRAFFIC AGE artserver http://artserver.somecluster/v1/models/artserver True 100 40m Explanation \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=artserver SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} After some time you should see a pop up containing the explanation, similar to the image below. If a pop up does not display and the message \"Unable to find an adversarial example.\" appears then an adversarial example could not be found for the image given in a timely manner. If a pop up does display then the image on the left is the original image and the image on the right is the adversarial example. The labels above both images represent what classification the model made for each individual image. The Square Attack method used in this example creates a random update at each iteration and adds this update to the adversarial input if it makes a misclassification more likely (more specifically, if it improves the objective function). Once enough random updates are added together and the model misclassifies then the resulting adversarial input will be returned and displayed. To try a different MNIST example add an integer to the end of the query between 0-9,999. The integer chosen will be the index of the image to be chosen in the MNIST dataset. Or to try a file with custom data add the file path to the end. Keep in mind that the data format must be {\"instances\": [<image>, <label>]} python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} 100 python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} ./input.json Stopping the Inference Service \u00b6 kubectl delete -f art.yaml Build a Development ART Explainer Docker Image \u00b6 If you would like to build a development image for the ART Explainer then follow these instructions Troubleshooting \u00b6 <504> Gateway Timeout <504> - the explainer is probably taking too long and not sending a response back quickly enough. Either there aren't enough resources allocated or the number of samples the explainer is allowed to take needs to be reduced. To fix this go to art.yaml and increase resources. If you see Configuration \"artserver-default\" does not have any ready Revision the container may have taken too long to download. If you run kubectl get revision and see your revision is stuck in ContainerCreating try deleting the inferenceservice and redeploying.","title":"ART Adversial Detector"},{"location":"modelserving/detect/art/mnist/#using-art-to-get-adversarial-examples-for-mnist-classifications","text":"This is an example to show how adversarially modified inputs can trick models to predict incorrectly to highlight model vulnerability to adversarial attacks. It is using the Adversarial Robustness Toolbox (ART) on KServe. ART provides tools that enable developers to evaluate, defend, and verify ML models and applications against adversarial threats. Apart from giving capabilities to craft adversarial attacks , it also provides algorithms to defend against them. We will be using the MNIST dataset which is a dataset of handwritten digits and find adversarial examples which will can make the model predict a classification incorrectly, thereby showing the vulnerability of the model against adversarial attacks. To deploy the inferenceservice with v1beta1 API kubectl apply -f art.yaml Then find the url. kubectl get inferenceservice NAME URL READY DEFAULT TRAFFIC CANARY TRAFFIC AGE artserver http://artserver.somecluster/v1/models/artserver True 100 40m","title":"Using ART to get adversarial examples for MNIST classifications"},{"location":"modelserving/detect/art/mnist/#explanation","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=artserver SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} After some time you should see a pop up containing the explanation, similar to the image below. If a pop up does not display and the message \"Unable to find an adversarial example.\" appears then an adversarial example could not be found for the image given in a timely manner. If a pop up does display then the image on the left is the original image and the image on the right is the adversarial example. The labels above both images represent what classification the model made for each individual image. The Square Attack method used in this example creates a random update at each iteration and adds this update to the adversarial input if it makes a misclassification more likely (more specifically, if it improves the objective function). Once enough random updates are added together and the model misclassifies then the resulting adversarial input will be returned and displayed. To try a different MNIST example add an integer to the end of the query between 0-9,999. The integer chosen will be the index of the image to be chosen in the MNIST dataset. Or to try a file with custom data add the file path to the end. Keep in mind that the data format must be {\"instances\": [<image>, <label>]} python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} 100 python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:explain ${SERVICE_HOSTNAME} ./input.json","title":"Explanation"},{"location":"modelserving/detect/art/mnist/#stopping-the-inference-service","text":"kubectl delete -f art.yaml","title":"Stopping the Inference Service"},{"location":"modelserving/detect/art/mnist/#build-a-development-art-explainer-docker-image","text":"If you would like to build a development image for the ART Explainer then follow these instructions","title":"Build a Development ART Explainer Docker Image"},{"location":"modelserving/detect/art/mnist/#troubleshooting","text":"<504> Gateway Timeout <504> - the explainer is probably taking too long and not sending a response back quickly enough. Either there aren't enough resources allocated or the number of samples the explainer is allowed to take needs to be reduced. To fix this go to art.yaml and increase resources. If you see Configuration \"artserver-default\" does not have any ready Revision the container may have taken too long to download. If you run kubectl get revision and see your revision is stuck in ContainerCreating try deleting the inferenceservice and redeploying.","title":"Troubleshooting"},{"location":"modelserving/detect/art/mnist/sklearnserver/","text":"Multi-layer perceptron classifier Scikit-Learn server \u00b6 This is a scikit-learn server which uses a sklearn.MLPClassifier to make classifications on the MNIST dataset. The model was trained using an adapted version of scikit-learn's Visualization of MLP weights on MNIST . Train a new model \u00b6 Move to the website/docs/modelserving/explainer/art/mnist directory python train_model.py This will train a new model and put the new model in sklearnserver/sklearnserver/example_model/model.pkl . To change the model adapt this line. mlp = MLPClassifier(hidden_layer_sizes=(500,500,500), max_iter=10, alpha=1e-4, solver='sgd', verbose=10, random_state=1, learning_rate_init=.1) Build a Development MLPClassifier Scikit-Learn server Docker Image \u00b6 Replace dockeruser with your docker username in the snippet below. docker build -t dockeruser/mlp-server:latest -f sklearn.Dockerfile . Then push your docker image to your dockerhub repo docker push dockeruser/mlp-server:latest Once your docker image is pushed you can pull the image from dockeruser/mlp-server:latest when deploying an inferenceservice by specifying the image in the yaml file.","title":"Multi-layer perceptron classifier Scikit-Learn server"},{"location":"modelserving/detect/art/mnist/sklearnserver/#multi-layer-perceptron-classifier-scikit-learn-server","text":"This is a scikit-learn server which uses a sklearn.MLPClassifier to make classifications on the MNIST dataset. The model was trained using an adapted version of scikit-learn's Visualization of MLP weights on MNIST .","title":"Multi-layer perceptron classifier Scikit-Learn server"},{"location":"modelserving/detect/art/mnist/sklearnserver/#train-a-new-model","text":"Move to the website/docs/modelserving/explainer/art/mnist directory python train_model.py This will train a new model and put the new model in sklearnserver/sklearnserver/example_model/model.pkl . To change the model adapt this line. mlp = MLPClassifier(hidden_layer_sizes=(500,500,500), max_iter=10, alpha=1e-4, solver='sgd', verbose=10, random_state=1, learning_rate_init=.1)","title":"Train a new model"},{"location":"modelserving/detect/art/mnist/sklearnserver/#build-a-development-mlpclassifier-scikit-learn-server-docker-image","text":"Replace dockeruser with your docker username in the snippet below. docker build -t dockeruser/mlp-server:latest -f sklearn.Dockerfile . Then push your docker image to your dockerhub repo docker push dockeruser/mlp-server:latest Once your docker image is pushed you can pull the image from dockeruser/mlp-server:latest when deploying an inferenceservice by specifying the image in the yaml file.","title":"Build a Development MLPClassifier Scikit-Learn server Docker Image"},{"location":"modelserving/explainer/explainer/","text":"InferenceService Explainer \u00b6 Model explainability answers the question: \"Why did my model make this prediction\" for a given instance. KServe integrates with Alibi Explainer which implements a black-box algorithm by generating a lot of similar looking intances for a given instance and send out to the model server to produce an explanation. Additionally KServe also integrates with The AI Explainability 360 (AIX360) toolkit, an LF AI Foundation incubation project, which is an open-source library that supports the interpretability and explainability of datasets and machine learning models. The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics. In addition to native algorithms, AIX360 also provides algorithms from LIME and Shap. Explainer Examples Deploy Alibi Image Explainer Imagenet Explainer Deploy Alibi Income Explainer Income Explainer Deploy Alibi Text Explainer Alibi Text Explainer","title":"Concept"},{"location":"modelserving/explainer/explainer/#inferenceservice-explainer","text":"Model explainability answers the question: \"Why did my model make this prediction\" for a given instance. KServe integrates with Alibi Explainer which implements a black-box algorithm by generating a lot of similar looking intances for a given instance and send out to the model server to produce an explanation. Additionally KServe also integrates with The AI Explainability 360 (AIX360) toolkit, an LF AI Foundation incubation project, which is an open-source library that supports the interpretability and explainability of datasets and machine learning models. The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics. In addition to native algorithms, AIX360 also provides algorithms from LIME and Shap. Explainer Examples Deploy Alibi Image Explainer Imagenet Explainer Deploy Alibi Income Explainer Income Explainer Deploy Alibi Text Explainer Alibi Text Explainer","title":"InferenceService Explainer"},{"location":"modelserving/explainer/aix/mnist/aix/","text":"Using AIX to get explanations for MNIST classifications \u00b6 This is an example of how to explain model predictions using AI Explainability 360 (AIX360) on KServe. We will be using mnist dataset for handwritten digits for this model and explain how the model decides the predicted results. Create the InferenceService with AIX Explainer \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"aix-explainer\" namespace : default spec : predictor : containers : - name : predictor image : aipipeline/rf-predictor:0.4.1 command : [ \"python\" , \"-m\" , \"rfserver\" , \"--model_name\" , \"aix-explainer\" ] imagePullPolicy : Always explainer : aix : type : LimeImages config : num_samples : \"100\" top_labels : \"10\" min_weight : \"0.01\" To deploy the InferenceService with v1beta1 API kubectl kubectl apply -f aix-explainer.yaml Then find the url. kubectl kubectl get inferenceservice aixserver NAME URL READY DEFAULT TRAFFIC CANARY TRAFFIC AGE aixserver http://aixserver.somecluster/v1/models/aixserver True 100 40m Run Explanation \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=aix-explainer SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} After a bit of time you should see a pop up containing the explanation, similar to the image below. The LIME method used in this example highlights the pixels in red that score above a certain confidence value for indicating a classification. The explanation shown will contain a collection of images that are highlighted paired with a title to describe the context. For each title and image pair, the title will say Positive for <X> Actual <Y> to denote that is the classification that LIME is testing for and is the correct label for that image. To give an example, the top-left image with the title \"Positive for 2 Actual 2\" is the image with pixels highlighted that score above a specified confidence level for indicating a classification of 2 (where 2 is also the correct classification). Similarly, the bottom-right image with the title \"Positive for 0 Actual 2\" is the image with pixels highlighted that score above a specified confidence level for indicating a classification of 0 (where 2 is the correct classification). If the model were to incorrectly classify the image as 0, then you could get an explanation of why by looking at the highlighted pixels as being especially troublesome. By raising and lowering the min_weight parameter in the deployment yaml you can test to see which pixels your model believes are the most and least relevant for each classification. To try a different MNIST example add an integer to the end of the query between 0-10,000. The integer chosen will be the index of the image to be chosen in the MNIST dataset. python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100 To try different parameters with explainer, add another string json argument to specify the parameters. Supported modified parameters: top_labels, segmentation_alg, num_samples, positive_only, and min_weight. python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100 '{\"top_labels\":\"10\"}' Stopping the Inference Service \u00b6 kubectl delete -f aix-explainer.yaml Build a Development AIX Model Explainer Docker Image \u00b6 If you would like to build a development image for the AIX Model Explainer then follow these instructions Troubleshooting \u00b6 <504> Gateway Timeout <504> - the explainer is probably taking too long and not sending a response back quickly enough. Either there aren't enough resources allocated or the number of samples the explainer is allowed to take needs to be reduced. To fix this go to aix-explainer.yaml and increase resources. Or to lower the number of allowed samples go to aix-explainer.yaml and add a flag to explainer: command: '--num_samples' (the default number of samples is 1000) If you see Configuration \"aixserver-explainer-default\" does not have any ready Revision the container may have taken too long to download. If you run kubectl get revision and see your revision is stuck in ContainerCreating try deleting the inferenceservice and redeploying.","title":"AIX Explainer"},{"location":"modelserving/explainer/aix/mnist/aix/#using-aix-to-get-explanations-for-mnist-classifications","text":"This is an example of how to explain model predictions using AI Explainability 360 (AIX360) on KServe. We will be using mnist dataset for handwritten digits for this model and explain how the model decides the predicted results.","title":"Using AIX to get explanations for MNIST classifications"},{"location":"modelserving/explainer/aix/mnist/aix/#create-the-inferenceservice-with-aix-explainer","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"aix-explainer\" namespace : default spec : predictor : containers : - name : predictor image : aipipeline/rf-predictor:0.4.1 command : [ \"python\" , \"-m\" , \"rfserver\" , \"--model_name\" , \"aix-explainer\" ] imagePullPolicy : Always explainer : aix : type : LimeImages config : num_samples : \"100\" top_labels : \"10\" min_weight : \"0.01\" To deploy the InferenceService with v1beta1 API kubectl kubectl apply -f aix-explainer.yaml Then find the url. kubectl kubectl get inferenceservice aixserver NAME URL READY DEFAULT TRAFFIC CANARY TRAFFIC AGE aixserver http://aixserver.somecluster/v1/models/aixserver True 100 40m","title":"Create the InferenceService with AIX Explainer"},{"location":"modelserving/explainer/aix/mnist/aix/#run-explanation","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=aix-explainer SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} After a bit of time you should see a pop up containing the explanation, similar to the image below. The LIME method used in this example highlights the pixels in red that score above a certain confidence value for indicating a classification. The explanation shown will contain a collection of images that are highlighted paired with a title to describe the context. For each title and image pair, the title will say Positive for <X> Actual <Y> to denote that is the classification that LIME is testing for and is the correct label for that image. To give an example, the top-left image with the title \"Positive for 2 Actual 2\" is the image with pixels highlighted that score above a specified confidence level for indicating a classification of 2 (where 2 is also the correct classification). Similarly, the bottom-right image with the title \"Positive for 0 Actual 2\" is the image with pixels highlighted that score above a specified confidence level for indicating a classification of 0 (where 2 is the correct classification). If the model were to incorrectly classify the image as 0, then you could get an explanation of why by looking at the highlighted pixels as being especially troublesome. By raising and lowering the min_weight parameter in the deployment yaml you can test to see which pixels your model believes are the most and least relevant for each classification. To try a different MNIST example add an integer to the end of the query between 0-10,000. The integer chosen will be the index of the image to be chosen in the MNIST dataset. python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100 To try different parameters with explainer, add another string json argument to specify the parameters. Supported modified parameters: top_labels, segmentation_alg, num_samples, positive_only, and min_weight. python query_explain.py http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:explain ${SERVICE_HOSTNAME} 100 '{\"top_labels\":\"10\"}'","title":"Run Explanation"},{"location":"modelserving/explainer/aix/mnist/aix/#stopping-the-inference-service","text":"kubectl delete -f aix-explainer.yaml","title":"Stopping the Inference Service"},{"location":"modelserving/explainer/aix/mnist/aix/#build-a-development-aix-model-explainer-docker-image","text":"If you would like to build a development image for the AIX Model Explainer then follow these instructions","title":"Build a Development AIX Model Explainer Docker Image"},{"location":"modelserving/explainer/aix/mnist/aix/#troubleshooting","text":"<504> Gateway Timeout <504> - the explainer is probably taking too long and not sending a response back quickly enough. Either there aren't enough resources allocated or the number of samples the explainer is allowed to take needs to be reduced. To fix this go to aix-explainer.yaml and increase resources. Or to lower the number of allowed samples go to aix-explainer.yaml and add a flag to explainer: command: '--num_samples' (the default number of samples is 1000) If you see Configuration \"aixserver-explainer-default\" does not have any ready Revision the container may have taken too long to download. If you run kubectl get revision and see your revision is stuck in ContainerCreating try deleting the inferenceservice and redeploying.","title":"Troubleshooting"},{"location":"modelserving/explainer/aix/mnist/rfserver/","text":"Random Forest for MNIST on kserve \u00b6","title":"Random Forest for MNIST on kserve"},{"location":"modelserving/explainer/aix/mnist/rfserver/#random-forest-for-mnist-on-kserve","text":"","title":"Random Forest for MNIST on kserve"},{"location":"modelserving/explainer/alibi/cifar10/","text":"CIFAR10 Image Classifier Explanations \u00b6 We will use a Tensorflow classifier built on CIFAR10 image dataset which is a 10 class image dataset to show the example of explanation on image data. Create the InferenceService with Alibi Explainer \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"cifar10\" spec : predictor : tensorflow : storageUri : \"gs://seldon-models/tfserving/cifar10/resnet32\" resources : requests : cpu : 0.1 memory : 5Gi limits : memory : 10Gi explainer : alibi : type : AnchorImages storageUri : \"gs://seldon-models/tfserving/cifar10/explainer-py36-0.5.2\" config : batch_size : \"40\" stop_on_first : \"True\" resources : requests : cpu : 0.1 memory : 5Gi limits : memory : 10Gi Note The InferenceService resource describes: A pretrained tensorflow model stored on a Google bucket An AnchorImage Seldon Alibi Explainer, see the Alibi Docs for further details. Test on notebook \u00b6 Run this example using the Jupyter notebook . Once created you will be able to test the predictions: And then get an explanation for it:","title":"Image Explainer"},{"location":"modelserving/explainer/alibi/cifar10/#cifar10-image-classifier-explanations","text":"We will use a Tensorflow classifier built on CIFAR10 image dataset which is a 10 class image dataset to show the example of explanation on image data.","title":"CIFAR10 Image Classifier Explanations"},{"location":"modelserving/explainer/alibi/cifar10/#create-the-inferenceservice-with-alibi-explainer","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"cifar10\" spec : predictor : tensorflow : storageUri : \"gs://seldon-models/tfserving/cifar10/resnet32\" resources : requests : cpu : 0.1 memory : 5Gi limits : memory : 10Gi explainer : alibi : type : AnchorImages storageUri : \"gs://seldon-models/tfserving/cifar10/explainer-py36-0.5.2\" config : batch_size : \"40\" stop_on_first : \"True\" resources : requests : cpu : 0.1 memory : 5Gi limits : memory : 10Gi Note The InferenceService resource describes: A pretrained tensorflow model stored on a Google bucket An AnchorImage Seldon Alibi Explainer, see the Alibi Docs for further details.","title":"Create the InferenceService with Alibi Explainer"},{"location":"modelserving/explainer/alibi/cifar10/#test-on-notebook","text":"Run this example using the Jupyter notebook . Once created you will be able to test the predictions: And then get an explanation for it:","title":"Test on notebook"},{"location":"modelserving/explainer/alibi/income/","text":"Example Anchors Tabular Explaination for Income Prediction \u00b6 This example uses a US income dataset to show the example of explanation on tabular data. You can also try out the Jupyter notebook for a visual walkthrough. Create the InferenceService with alibi explainer \u00b6 We can create a InferenceService with a trained sklearn predictor for this dataset and an associated model explainer. The black box explainer algorithm we will use is the Tabular version of Anchors from the Alibi open source library . More details on this algorithm and configuration settings that can be set can be found in the Seldon Alibi documentation . The InferenceService is shown below: apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"income\" spec : predictor : minReplicas : 1 sklearn : storageUri : \"gs://seldon-models/sklearn/income/model\" resources : requests : cpu : 0.1 memory : 1Gi limits : cpu : 1 memory : 1Gi explainer : minReplicas : 1 alibi : type : AnchorTabular storageUri : \"gs://seldon-models/sklearn/income/explainer-py37-0.6.0\" resources : requests : cpu : 0.1 memory : 1Gi limits : cpu : 1 memory : 4Gi Create the InferenceService with above yaml: kubectl kubectl create -f income.yaml Set up some environment variables for the model name and cluster entrypoint. MODEL_NAME=income INGRESS_GATEWAY=istio-ingressgateway CLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}') Run the inference \u00b6 Test the predictor: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d '{\"instances\":[[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}' You should receive the response showing the prediction is for low salary: {\"predictions\": [0]} Run the explanation \u00b6 Now lets get an explanation for this: curl -v -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:explain -d '{\"instances\":[[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}' The returned explanation will be like: { \"names\": [ \"Marital Status = Never-Married\", \"Workclass = State-gov\" ], \"precision\": 0.9724770642201835, \"coverage\": 0.0147, \"raw\": { \"feature\": [ 3, 1 ], \"mean\": [ 0.9129746835443038, 0.9724770642201835 ], \"precision\": [ 0.9129746835443038, 0.9724770642201835 ], \"coverage\": [ 0.3327, 0.0147 ], \"examples\": [ { \"covered\": [ [ 30, \"Self-emp-not-inc\", \"Bachelors\", \"Never-Married\", \"Sales\", \"Unmarried\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 69, \"Private\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", 9386, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 44, \"Local-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 52, \"United-States\" ], [ 59, \"Private\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 50, \"United-States\" ], [ 55, \"Private\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 55, \"United-States\" ], [ 32, \"?\", \"Bachelors\", \"Never-Married\", \"?\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 32, \"United-States\" ], [ 47, \"Private\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"Black\", \"Female\", 6849, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 35, \"Private\", \"Associates\", \"Never-Married\", \"Service\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 65, \"United-States\" ], [ 32, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 48, \"Private\", \"Masters\", \"Never-Married\", \"White-Collar\", \"Husband\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 45, \"United-States\" ] ], \"covered_true\": [ [ 32, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 44, \"Local-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 52, \"United-States\" ], [ 36, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 30, \"United-States\" ], [ 56, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 49, \"Local-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 30, \"United-States\" ], [ 20, \"?\", \"High School grad\", \"Never-Married\", \"?\", \"Own-child\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 10, \"United-States\" ], [ 22, \"?\", \"High School grad\", \"Never-Married\", \"?\", \"Own-child\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", \"Hours per week > 45.00\", \"United-States\" ], [ 29, \"Private\", \"High School grad\", \"Never-Married\", \"Service\", \"Own-child\", \"Asian-Pac-Islander\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 25, \"SE-Asia\" ], [ 45, \"Local-gov\", \"Masters\", \"Never-Married\", \"Professional\", \"Unmarried\", \"White\", \"Female\", 1506, \"Capital Loss <= 0.00\", 45, \"United-States\" ], [ 27, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Not-in-family\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 50, \"United-States\" ] ], \"covered_false\": [ [ 29, \"Private\", \"Bachelors\", \"Never-Married\", \"Service\", \"Husband\", \"White\", \"Male\", 7298, \"Capital Loss <= 0.00\", 42, \"United-States\" ], [ 56, \"Private\", \"Associates\", \"Never-Married\", \"Sales\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 47, \"Private\", \"Masters\", \"Never-Married\", \"Sales\", \"Not-in-family\", \"White\", \"Male\", 27828, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 40, \"Private\", \"Associates\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", 7688, \"Capital Loss <= 0.00\", 44, \"United-States\" ], [ 55, \"Self-emp-not-inc\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Male\", 34095, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 53, \"Private\", \"Masters\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 48, \"United-States\" ], [ 47, \"Federal-gov\", \"Doctorate\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 53, \"Private\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", 1977, 40, \"United-States\" ], [ 46, \"Private\", \"Bachelors\", \"Never-Married\", \"Sales\", \"Not-in-family\", \"White\", \"Male\", 8614, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 44, \"Local-gov\", \"Prof-School\", \"Never-Married\", \"Professional\", \"Not-in-family\", \"White\", \"Male\", 10520, \"Capital Loss <= 0.00\", 40, \"United-States\" ] ], \"uncovered_true\": [], \"uncovered_false\": [] }, { \"covered\": [ [ 41, \"State-gov\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 64, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Not-in-family\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 33, \"State-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Unmarried\", \"Black\", \"Female\", 1831, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 35, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 25, \"State-gov\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Own-child\", \"Black\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 40, \"State-gov\", \"Associates\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 19, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Other-relative\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 20, \"United-States\" ], [ 44, \"State-gov\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 88, \"United-States\" ], [ 80, \"State-gov\", \"Associates\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 24, \"United-States\" ], [ 21, \"State-gov\", \"High School grad\", \"Never-Married\", \"Professional\", \"Own-child\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 20, \"United-States\" ] ], \"covered_true\": [ [ 22, \"State-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 25, \"United-States\" ], [ 49, \"State-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 22, \"State-gov\", \"Bachelors\", \"Never-Married\", \"?\", \"Not-in-family\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 25, \"United-States\" ], [ 31, \"State-gov\", \"Bachelors\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 50, \"United-States\" ], [ 18, \"State-gov\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Not-in-family\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 56, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 26, \"State-gov\", \"Dropout\", \"Never-Married\", \"Service\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 38, \"State-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 52, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 70, \"United-States\" ], [ 25, \"State-gov\", \"Associates\", \"Never-Married\", \"Professional\", \"Wife\", \"White\", \"Female\", \"Capital Gain <= 0.00\", 1887, 40, \"United-States\" ] ], \"covered_false\": [ [ 46, \"State-gov\", \"Prof-School\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 45, \"United-States\" ], [ 42, \"State-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 50, \"United-States\" ], [ 46, \"State-gov\", \"Prof-School\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 54, \"State-gov\", \"Doctorate\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 42, \"State-gov\", \"Masters\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Female\", 14084, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 37, \"State-gov\", \"Masters\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 45, \"United-States\" ] ], \"uncovered_true\": [], \"uncovered_false\": [] } ], \"all_precision\": 0, \"num_preds\": 1000101, \"names\": [ \"Marital Status = Never-Married\", \"Workclass = State-gov\" ], \"instance\": [ [ 39 ], [ 7 ], [ \"28.00 < Age <= 37.00\" ], [ \"28.00 < Age <= 37.00\" ], [ \"28.00 < Age <= 37.00\" ], [ \"28.00 < Age <= 37.00\" ], [ 4 ], [ \"28.00 < Age <= 37.00\" ], [ 2174 ], [ \"Age <= 28.00\" ], [ 40 ], [ 9 ] ], \"prediction\": 0 } }","title":"Income Explainer"},{"location":"modelserving/explainer/alibi/income/#example-anchors-tabular-explaination-for-income-prediction","text":"This example uses a US income dataset to show the example of explanation on tabular data. You can also try out the Jupyter notebook for a visual walkthrough.","title":"Example Anchors Tabular Explaination for Income Prediction"},{"location":"modelserving/explainer/alibi/income/#create-the-inferenceservice-with-alibi-explainer","text":"We can create a InferenceService with a trained sklearn predictor for this dataset and an associated model explainer. The black box explainer algorithm we will use is the Tabular version of Anchors from the Alibi open source library . More details on this algorithm and configuration settings that can be set can be found in the Seldon Alibi documentation . The InferenceService is shown below: apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"income\" spec : predictor : minReplicas : 1 sklearn : storageUri : \"gs://seldon-models/sklearn/income/model\" resources : requests : cpu : 0.1 memory : 1Gi limits : cpu : 1 memory : 1Gi explainer : minReplicas : 1 alibi : type : AnchorTabular storageUri : \"gs://seldon-models/sklearn/income/explainer-py37-0.6.0\" resources : requests : cpu : 0.1 memory : 1Gi limits : cpu : 1 memory : 4Gi Create the InferenceService with above yaml: kubectl kubectl create -f income.yaml Set up some environment variables for the model name and cluster entrypoint. MODEL_NAME=income INGRESS_GATEWAY=istio-ingressgateway CLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}')","title":"Create the InferenceService with alibi explainer"},{"location":"modelserving/explainer/alibi/income/#run-the-inference","text":"Test the predictor: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d '{\"instances\":[[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}' You should receive the response showing the prediction is for low salary: {\"predictions\": [0]}","title":"Run the inference"},{"location":"modelserving/explainer/alibi/income/#run-the-explanation","text":"Now lets get an explanation for this: curl -v -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:explain -d '{\"instances\":[[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}' The returned explanation will be like: { \"names\": [ \"Marital Status = Never-Married\", \"Workclass = State-gov\" ], \"precision\": 0.9724770642201835, \"coverage\": 0.0147, \"raw\": { \"feature\": [ 3, 1 ], \"mean\": [ 0.9129746835443038, 0.9724770642201835 ], \"precision\": [ 0.9129746835443038, 0.9724770642201835 ], \"coverage\": [ 0.3327, 0.0147 ], \"examples\": [ { \"covered\": [ [ 30, \"Self-emp-not-inc\", \"Bachelors\", \"Never-Married\", \"Sales\", \"Unmarried\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 69, \"Private\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", 9386, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 44, \"Local-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 52, \"United-States\" ], [ 59, \"Private\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 50, \"United-States\" ], [ 55, \"Private\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 55, \"United-States\" ], [ 32, \"?\", \"Bachelors\", \"Never-Married\", \"?\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 32, \"United-States\" ], [ 47, \"Private\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"Black\", \"Female\", 6849, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 35, \"Private\", \"Associates\", \"Never-Married\", \"Service\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 65, \"United-States\" ], [ 32, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 48, \"Private\", \"Masters\", \"Never-Married\", \"White-Collar\", \"Husband\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 45, \"United-States\" ] ], \"covered_true\": [ [ 32, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 44, \"Local-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 52, \"United-States\" ], [ 36, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 30, \"United-States\" ], [ 56, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 49, \"Local-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 30, \"United-States\" ], [ 20, \"?\", \"High School grad\", \"Never-Married\", \"?\", \"Own-child\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 10, \"United-States\" ], [ 22, \"?\", \"High School grad\", \"Never-Married\", \"?\", \"Own-child\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", \"Hours per week > 45.00\", \"United-States\" ], [ 29, \"Private\", \"High School grad\", \"Never-Married\", \"Service\", \"Own-child\", \"Asian-Pac-Islander\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 25, \"SE-Asia\" ], [ 45, \"Local-gov\", \"Masters\", \"Never-Married\", \"Professional\", \"Unmarried\", \"White\", \"Female\", 1506, \"Capital Loss <= 0.00\", 45, \"United-States\" ], [ 27, \"Private\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Not-in-family\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 50, \"United-States\" ] ], \"covered_false\": [ [ 29, \"Private\", \"Bachelors\", \"Never-Married\", \"Service\", \"Husband\", \"White\", \"Male\", 7298, \"Capital Loss <= 0.00\", 42, \"United-States\" ], [ 56, \"Private\", \"Associates\", \"Never-Married\", \"Sales\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 47, \"Private\", \"Masters\", \"Never-Married\", \"Sales\", \"Not-in-family\", \"White\", \"Male\", 27828, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 40, \"Private\", \"Associates\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", 7688, \"Capital Loss <= 0.00\", 44, \"United-States\" ], [ 55, \"Self-emp-not-inc\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Male\", 34095, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 53, \"Private\", \"Masters\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 48, \"United-States\" ], [ 47, \"Federal-gov\", \"Doctorate\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 53, \"Private\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", 1977, 40, \"United-States\" ], [ 46, \"Private\", \"Bachelors\", \"Never-Married\", \"Sales\", \"Not-in-family\", \"White\", \"Male\", 8614, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 44, \"Local-gov\", \"Prof-School\", \"Never-Married\", \"Professional\", \"Not-in-family\", \"White\", \"Male\", 10520, \"Capital Loss <= 0.00\", 40, \"United-States\" ] ], \"uncovered_true\": [], \"uncovered_false\": [] }, { \"covered\": [ [ 41, \"State-gov\", \"High School grad\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 64, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Not-in-family\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 33, \"State-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Unmarried\", \"Black\", \"Female\", 1831, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 35, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 25, \"State-gov\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Own-child\", \"Black\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 40, \"State-gov\", \"Associates\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 19, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Other-relative\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 20, \"United-States\" ], [ 44, \"State-gov\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 88, \"United-States\" ], [ 80, \"State-gov\", \"Associates\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 24, \"United-States\" ], [ 21, \"State-gov\", \"High School grad\", \"Never-Married\", \"Professional\", \"Own-child\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 20, \"United-States\" ] ], \"covered_true\": [ [ 22, \"State-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 25, \"United-States\" ], [ 49, \"State-gov\", \"High School grad\", \"Never-Married\", \"Service\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 22, \"State-gov\", \"Bachelors\", \"Never-Married\", \"?\", \"Not-in-family\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 25, \"United-States\" ], [ 31, \"State-gov\", \"Bachelors\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 50, \"United-States\" ], [ 18, \"State-gov\", \"Dropout\", \"Never-Married\", \"Blue-Collar\", \"Not-in-family\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 56, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Unmarried\", \"Black\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 26, \"State-gov\", \"Dropout\", \"Never-Married\", \"Service\", \"Unmarried\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 38, \"State-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 52, \"State-gov\", \"High School grad\", \"Never-Married\", \"Blue-Collar\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 70, \"United-States\" ], [ 25, \"State-gov\", \"Associates\", \"Never-Married\", \"Professional\", \"Wife\", \"White\", \"Female\", \"Capital Gain <= 0.00\", 1887, 40, \"United-States\" ] ], \"covered_false\": [ [ 46, \"State-gov\", \"Prof-School\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 45, \"United-States\" ], [ 42, \"State-gov\", \"Bachelors\", \"Never-Married\", \"White-Collar\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 50, \"United-States\" ], [ 46, \"State-gov\", \"Prof-School\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", 15024, \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 54, \"State-gov\", \"Doctorate\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Female\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 40, \"United-States\" ], [ 42, \"State-gov\", \"Masters\", \"Never-Married\", \"White-Collar\", \"Not-in-family\", \"White\", \"Female\", 14084, \"Capital Loss <= 0.00\", 60, \"United-States\" ], [ 37, \"State-gov\", \"Masters\", \"Never-Married\", \"Professional\", \"Husband\", \"White\", \"Male\", \"Capital Gain <= 0.00\", \"Capital Loss <= 0.00\", 45, \"United-States\" ] ], \"uncovered_true\": [], \"uncovered_false\": [] } ], \"all_precision\": 0, \"num_preds\": 1000101, \"names\": [ \"Marital Status = Never-Married\", \"Workclass = State-gov\" ], \"instance\": [ [ 39 ], [ 7 ], [ \"28.00 < Age <= 37.00\" ], [ \"28.00 < Age <= 37.00\" ], [ \"28.00 < Age <= 37.00\" ], [ \"28.00 < Age <= 37.00\" ], [ 4 ], [ \"28.00 < Age <= 37.00\" ], [ 2174 ], [ \"Age <= 28.00\" ], [ 40 ], [ 9 ] ], \"prediction\": 0 } }","title":"Run the explanation"},{"location":"modelserving/explainer/alibi/moviesentiment/","text":"Example Anchors Text Explaination for Movie Sentiment \u00b6 This example uses a movie sentiment dataset to show the explanation on text data, for a more visual walkthrough please try the Jupyter notebook . Deploy InferenceService with AnchorText Explainer \u00b6 We can create a InferenceService with a trained sklearn predictor for this dataset and an associated explainer. The black box explainer algorithm we will use is the Text version of Anchors from the Alibi open source library . More details on this algorithm and configuration settings that can be set can be found in the Seldon Alibi documentation . The InferenceService is shown below: apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"moviesentiment\" spec : predictor : minReplicas : 1 sklearn : storageUri : \"gs://seldon-models/sklearn/moviesentiment\" resources : requests : cpu : 0.1 memory : 1Gi limits : cpu : 1 memory : 1Gi explainer : minReplicas : 1 alibi : type : AnchorText resources : requests : cpu : 0.1 memory : 6Gi limits : memory : 6Gi Create this InferenceService: kubectl kubectl create -f moviesentiment.yaml Run Inference and Explanation \u00b6 Set up some environment variables for the model name and cluster entrypoint. MODEL_NAME=moviesentiment INGRESS_GATEWAY=istio-ingressgateway CLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}') Test the predictor on an example sentence: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d '{\"instances\":[\"a visually flashy but narratively opaque and emotionally vapid exercise .\"]}' You should receive the response showing negative sentiment: {\"predictions\": [0]} Test on another sentence: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d '{\"instances\":[\"a touching , sophisticated film that almost seems like a documentary in the way it captures an italian immigrant family on the brink of major changes .\"]}' You should receive the response showing positive sentiment: {\"predictions\": [1]} Now lets get an explanation for the first sentence: curl -v -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:explain -d '{\"instances\":[\"a visually flashy but narratively opaque and emotionally vapid exercise .\"]}' Expected Output { \"names\": [ \"exercise\" ], \"precision\": 1, \"coverage\": 0.5005, \"raw\": { \"feature\": [ 9 ], \"mean\": [ 1 ], \"precision\": [ 1 ], \"coverage\": [ 0.5005 ], \"examples\": [ { \"covered\": [ [ \"a visually UNK UNK UNK opaque and emotionally vapid exercise UNK\" ], [ \"a visually flashy but UNK UNK and emotionally UNK exercise .\" ], [ \"a visually flashy but narratively UNK UNK UNK UNK exercise .\" ], [ \"UNK UNK flashy UNK narratively opaque UNK UNK vapid exercise .\" ], [ \"UNK visually UNK UNK UNK UNK and UNK vapid exercise UNK\" ], [ \"UNK UNK UNK but UNK opaque UNK emotionally UNK exercise UNK\" ], [ \"a UNK flashy UNK UNK UNK and emotionally vapid exercise .\" ], [ \"UNK UNK flashy UNK narratively opaque UNK emotionally UNK exercise .\" ], [ \"UNK UNK flashy UNK narratively opaque UNK UNK vapid exercise UNK\" ], [ \"a visually UNK but narratively opaque UNK UNK vapid exercise UNK\" ] ], \"covered_true\": [ [ \"UNK visually flashy but UNK UNK and emotionally vapid exercise .\" ], [ \"UNK visually UNK UNK UNK UNK and UNK UNK exercise .\" ], [ \"a UNK UNK UNK narratively opaque UNK UNK UNK exercise UNK\" ], [ \"a visually UNK UNK narratively opaque UNK UNK UNK exercise UNK\" ], [ \"a UNK UNK UNK UNK UNK and emotionally vapid exercise UNK\" ], [ \"a UNK flashy UNK narratively UNK and UNK vapid exercise UNK\" ], [ \"UNK visually UNK UNK narratively UNK and emotionally UNK exercise .\" ], [ \"UNK visually flashy UNK narratively opaque UNK emotionally UNK exercise UNK\" ], [ \"UNK UNK flashy UNK UNK UNK and UNK vapid exercise UNK\" ], [ \"a UNK flashy UNK UNK UNK and emotionally vapid exercise .\" ] ], \"covered_false\": [], \"uncovered_true\": [], \"uncovered_false\": [] } ], \"all_precision\": 0, \"num_preds\": 1000101, \"names\": [ \"exercise\" ], \"positions\": [ 63 ], \"instance\": \"a visually flashy but narratively opaque and emotionally vapid exercise .\", \"prediction\": 0 } } This shows the key word \"bad\" was indetified and examples show it in context using the default \"UKN\" placeholder for surrounding words. Custom Configuration \u00b6 You can add custom configuration for the Anchor Text explainer in the 'config' section. For example we can change the text explainer to sample from the corpus rather than use UKN placeholders: apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"moviesentiment\" spec: predictor: sklearn: storageUri: \"gs://seldon-models/sklearn/moviesentiment\" resources: requests: cpu: 0.1 explainer: alibi: type: AnchorText config: use_unk: \"false\" sample_proba: \"0.5\" resources: requests: cpu: 0.1 If we apply this: kubectl kubectl create -f moviesentiment2.yaml and then ask for an explanation: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:explain -d '{\"instances\":[\"a visually flashy but narratively opaque and emotionally vapid exercise .\"]}' Expected Output { \"names\": [ \"exercise\" ], \"precision\": 0.9918032786885246, \"coverage\": 0.5072, \"raw\": { \"feature\": [ 9 ], \"mean\": [ 0.9918032786885246 ], \"precision\": [ 0.9918032786885246 ], \"coverage\": [ 0.5072 ], \"examples\": [ { \"covered\": [ [ \"each visually playful but enormously opaque and academically vapid exercise .\" ], [ \"each academically trashy but narratively pigmented and profoundly vapid exercise .\" ], [ \"a masterfully flashy but narratively straightforward and verbally disingenuous exercise .\" ], [ \"a visually gaudy but interestingly opaque and emotionally vapid exercise .\" ], [ \"some concurrently flashy but philosophically voxel and emotionally vapid exercise .\" ], [ \"a visually flashy but delightfully sensible and emotionally snobby exercise .\" ], [ \"a surprisingly bland but fantastically seamless and hideously vapid exercise .\" ], [ \"both visually classy but nonetheless robust and musically vapid exercise .\" ], [ \"a visually fancy but narratively robust and emotionally uninformed exercise .\" ], [ \"a visually flashy but tastefully opaque and weirdly vapid exercise .\" ] ], \"covered_true\": [ [ \"another visually flashy but narratively opaque and emotionally vapid exercise .\" ], [ \"the visually classy but narratively opaque and emotionally vapid exercise .\" ], [ \"the visually arty but overshadow yellowish and emotionally vapid exercise .\" ], [ \"a objectively flashy but genuinely straightforward and emotionally vapid exercise .\" ], [ \"a visually flashy but tastefully opaque and weirdly vapid exercise .\" ], [ \"a emotionally crafty but narratively opaque and emotionally vapid exercise .\" ], [ \"some similarly eclectic but narratively dainty and emotionally illogical exercise .\" ], [ \"a nicely flashy but psychologically opaque and emotionally vapid exercise .\" ], [ \"a visually flashy but narratively colorless and emotionally vapid exercise .\" ], [ \"every properly lavish but logistically opaque and someway incomprehensible exercise .\" ] ], \"covered_false\": [ [ \"another enormously inventive but socially opaque and somewhat idiotic exercise .\" ], [ \"each visually playful but enormously opaque and academically vapid exercise .\" ] ], \"uncovered_true\": [], \"uncovered_false\": [] } ], \"all_precision\": 0, \"num_preds\": 1000101, \"names\": [ \"exercise\" ], \"positions\": [ 63 ], \"instance\": \"a visually flashy but narratively opaque and emotionally vapid exercise .\", \"prediction\": 0 } } Run on Notebook \u00b6 You can also run this example on notebook","title":"Text Explainer"},{"location":"modelserving/explainer/alibi/moviesentiment/#example-anchors-text-explaination-for-movie-sentiment","text":"This example uses a movie sentiment dataset to show the explanation on text data, for a more visual walkthrough please try the Jupyter notebook .","title":"Example Anchors Text Explaination for Movie Sentiment"},{"location":"modelserving/explainer/alibi/moviesentiment/#deploy-inferenceservice-with-anchortext-explainer","text":"We can create a InferenceService with a trained sklearn predictor for this dataset and an associated explainer. The black box explainer algorithm we will use is the Text version of Anchors from the Alibi open source library . More details on this algorithm and configuration settings that can be set can be found in the Seldon Alibi documentation . The InferenceService is shown below: apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"moviesentiment\" spec : predictor : minReplicas : 1 sklearn : storageUri : \"gs://seldon-models/sklearn/moviesentiment\" resources : requests : cpu : 0.1 memory : 1Gi limits : cpu : 1 memory : 1Gi explainer : minReplicas : 1 alibi : type : AnchorText resources : requests : cpu : 0.1 memory : 6Gi limits : memory : 6Gi Create this InferenceService: kubectl kubectl create -f moviesentiment.yaml","title":"Deploy InferenceService with AnchorText Explainer"},{"location":"modelserving/explainer/alibi/moviesentiment/#run-inference-and-explanation","text":"Set up some environment variables for the model name and cluster entrypoint. MODEL_NAME=moviesentiment INGRESS_GATEWAY=istio-ingressgateway CLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}') Test the predictor on an example sentence: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d '{\"instances\":[\"a visually flashy but narratively opaque and emotionally vapid exercise .\"]}' You should receive the response showing negative sentiment: {\"predictions\": [0]} Test on another sentence: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:predict -d '{\"instances\":[\"a touching , sophisticated film that almost seems like a documentary in the way it captures an italian immigrant family on the brink of major changes .\"]}' You should receive the response showing positive sentiment: {\"predictions\": [1]} Now lets get an explanation for the first sentence: curl -v -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:explain -d '{\"instances\":[\"a visually flashy but narratively opaque and emotionally vapid exercise .\"]}' Expected Output { \"names\": [ \"exercise\" ], \"precision\": 1, \"coverage\": 0.5005, \"raw\": { \"feature\": [ 9 ], \"mean\": [ 1 ], \"precision\": [ 1 ], \"coverage\": [ 0.5005 ], \"examples\": [ { \"covered\": [ [ \"a visually UNK UNK UNK opaque and emotionally vapid exercise UNK\" ], [ \"a visually flashy but UNK UNK and emotionally UNK exercise .\" ], [ \"a visually flashy but narratively UNK UNK UNK UNK exercise .\" ], [ \"UNK UNK flashy UNK narratively opaque UNK UNK vapid exercise .\" ], [ \"UNK visually UNK UNK UNK UNK and UNK vapid exercise UNK\" ], [ \"UNK UNK UNK but UNK opaque UNK emotionally UNK exercise UNK\" ], [ \"a UNK flashy UNK UNK UNK and emotionally vapid exercise .\" ], [ \"UNK UNK flashy UNK narratively opaque UNK emotionally UNK exercise .\" ], [ \"UNK UNK flashy UNK narratively opaque UNK UNK vapid exercise UNK\" ], [ \"a visually UNK but narratively opaque UNK UNK vapid exercise UNK\" ] ], \"covered_true\": [ [ \"UNK visually flashy but UNK UNK and emotionally vapid exercise .\" ], [ \"UNK visually UNK UNK UNK UNK and UNK UNK exercise .\" ], [ \"a UNK UNK UNK narratively opaque UNK UNK UNK exercise UNK\" ], [ \"a visually UNK UNK narratively opaque UNK UNK UNK exercise UNK\" ], [ \"a UNK UNK UNK UNK UNK and emotionally vapid exercise UNK\" ], [ \"a UNK flashy UNK narratively UNK and UNK vapid exercise UNK\" ], [ \"UNK visually UNK UNK narratively UNK and emotionally UNK exercise .\" ], [ \"UNK visually flashy UNK narratively opaque UNK emotionally UNK exercise UNK\" ], [ \"UNK UNK flashy UNK UNK UNK and UNK vapid exercise UNK\" ], [ \"a UNK flashy UNK UNK UNK and emotionally vapid exercise .\" ] ], \"covered_false\": [], \"uncovered_true\": [], \"uncovered_false\": [] } ], \"all_precision\": 0, \"num_preds\": 1000101, \"names\": [ \"exercise\" ], \"positions\": [ 63 ], \"instance\": \"a visually flashy but narratively opaque and emotionally vapid exercise .\", \"prediction\": 0 } } This shows the key word \"bad\" was indetified and examples show it in context using the default \"UKN\" placeholder for surrounding words.","title":"Run Inference and Explanation"},{"location":"modelserving/explainer/alibi/moviesentiment/#custom-configuration","text":"You can add custom configuration for the Anchor Text explainer in the 'config' section. For example we can change the text explainer to sample from the corpus rather than use UKN placeholders: apiVersion: \"serving.kserve.io/v1beta1\" kind: \"InferenceService\" metadata: name: \"moviesentiment\" spec: predictor: sklearn: storageUri: \"gs://seldon-models/sklearn/moviesentiment\" resources: requests: cpu: 0.1 explainer: alibi: type: AnchorText config: use_unk: \"false\" sample_proba: \"0.5\" resources: requests: cpu: 0.1 If we apply this: kubectl kubectl create -f moviesentiment2.yaml and then ask for an explanation: curl -H \"Host: ${MODEL_NAME}.default.example.com\" http://$CLUSTER_IP/v1/models/$MODEL_NAME:explain -d '{\"instances\":[\"a visually flashy but narratively opaque and emotionally vapid exercise .\"]}' Expected Output { \"names\": [ \"exercise\" ], \"precision\": 0.9918032786885246, \"coverage\": 0.5072, \"raw\": { \"feature\": [ 9 ], \"mean\": [ 0.9918032786885246 ], \"precision\": [ 0.9918032786885246 ], \"coverage\": [ 0.5072 ], \"examples\": [ { \"covered\": [ [ \"each visually playful but enormously opaque and academically vapid exercise .\" ], [ \"each academically trashy but narratively pigmented and profoundly vapid exercise .\" ], [ \"a masterfully flashy but narratively straightforward and verbally disingenuous exercise .\" ], [ \"a visually gaudy but interestingly opaque and emotionally vapid exercise .\" ], [ \"some concurrently flashy but philosophically voxel and emotionally vapid exercise .\" ], [ \"a visually flashy but delightfully sensible and emotionally snobby exercise .\" ], [ \"a surprisingly bland but fantastically seamless and hideously vapid exercise .\" ], [ \"both visually classy but nonetheless robust and musically vapid exercise .\" ], [ \"a visually fancy but narratively robust and emotionally uninformed exercise .\" ], [ \"a visually flashy but tastefully opaque and weirdly vapid exercise .\" ] ], \"covered_true\": [ [ \"another visually flashy but narratively opaque and emotionally vapid exercise .\" ], [ \"the visually classy but narratively opaque and emotionally vapid exercise .\" ], [ \"the visually arty but overshadow yellowish and emotionally vapid exercise .\" ], [ \"a objectively flashy but genuinely straightforward and emotionally vapid exercise .\" ], [ \"a visually flashy but tastefully opaque and weirdly vapid exercise .\" ], [ \"a emotionally crafty but narratively opaque and emotionally vapid exercise .\" ], [ \"some similarly eclectic but narratively dainty and emotionally illogical exercise .\" ], [ \"a nicely flashy but psychologically opaque and emotionally vapid exercise .\" ], [ \"a visually flashy but narratively colorless and emotionally vapid exercise .\" ], [ \"every properly lavish but logistically opaque and someway incomprehensible exercise .\" ] ], \"covered_false\": [ [ \"another enormously inventive but socially opaque and somewhat idiotic exercise .\" ], [ \"each visually playful but enormously opaque and academically vapid exercise .\" ] ], \"uncovered_true\": [], \"uncovered_false\": [] } ], \"all_precision\": 0, \"num_preds\": 1000101, \"names\": [ \"exercise\" ], \"positions\": [ 63 ], \"instance\": \"a visually flashy but narratively opaque and emotionally vapid exercise .\", \"prediction\": 0 } }","title":"Custom Configuration"},{"location":"modelserving/explainer/alibi/moviesentiment/#run-on-notebook","text":"You can also run this example on notebook","title":"Run on Notebook"},{"location":"modelserving/mms/multi-model-serving/","text":"Multi-Model Serving \u00b6 Multi-model serving is an alpha feature added recently to increase KServe\u2019s scalability. Please assume that the interface is subject to changes. The model deployment scalability problem \u00b6 With machine learning approaches becoming more widely adopted in organizations, there is a trend to deploy a large number of models. For example, a news classification service may train custom models for each news category. Another important reason why organizations desire to train a lot of models is to protect data privacy, as it is safer to isolate each user's data and train models separately. While you get the benefit of better inference accuracy and data privacy by building models for each use case, it is more challenging to deploy thousands to hundreds of thousands of models on a Kubernetes cluster. Furthermore, there are an increasing number of use cases of serving neural network-based models. To achieve reasonable latency, those models are better served on GPUs. However, since GPUs are expensive resources, it is costly to serve many GPU-based models. The original design of KServe deploys one model per InferenceService. But, when dealing with a large number of models, its 'one model, one server' paradigm presents challenges for a Kubernetes cluster. To scale the number of models, we have to scale the number of InferenceServices, something that can quickly challenge the cluster's limits. Multi-model serving is designed to address three types of limitations KServe will run into: Compute resource limitation Maximum pods limitation Maximum IP address limitation. Compute resource limitation \u00b6 Each InferenceService has a resource overhead because of the sidecars injected into each pod. This normally adds about 0.5 CPU and 0.5G Memory resource per InferenceService replica. For example, if we deploy 10 models, each with 2 replicas, then the resource overhead is 10 * 2 * 0.5 = 10 CPU and 10 * 2 * 0.5 = 10 GB memory. Each model\u2019s resource overhead is 1CPU and 1 GB memory. Deploying many models using the current approach will quickly use up a cluster's computing resource. With Multi-model serving, these models can be loaded in one InferenceService, then each model's average overhead is 0.1 CPU and 0.1GB memory. For GPU based models, the number of GPUs required grows linearly as the number of models grows, which is not cost efficient. If multiple models can be loaded in one GPU enabled model server such as TritonServer, we need a lot less GPUs in the cluster. Maximum pods limitation \u00b6 Kubelet has a maximum number of pods per node with the default limit set to 110 . According to Kubernetes best practice , a node shouldn't run more than 100 pods. With this limitation, a typical 50-node cluster with default pod limit can run at most 1000 models assuming each InferenceService has 4 pods on average (two transformer replicas and two predictor replicas). Maximum IP address limitation. \u00b6 Kubernetes clusters also have an IP address limit per cluster. Each pod in InferenceService needs an independent IP. For example a cluster with 4096 IP addresses can deploy at most 1024 models assuming each InferenceService has 4 pods on average (two transformer replicas and two predictor replicas). Benefit of using Multi-Model serving \u00b6 The Multi-model serving feature is designed to address the three limitations above. It decreases the average resource overhead per model so model deployment becomes more cost efficient. And the number of models which can be deployed in a cluster will no longer be limited by the maximum pods limitation and the maximum IP address limitation. The benchmark test results are able to support the statements above. How Multi-Model serving address those limitations \u00b6 We designed a new CustomResource called \"TrainedModel\" which represents a machine learning model. It can be loaded into a designated InferenceService. The common user flow with Multi-Model serving is: Deploy an InferenceService without the \"storageUri\" field i.e. without any models loaded Deploy multiple TrainedModel CRs which load models to a designated InferenceService Resolve the model prediction endpoint from TrainedModel's status object Run prediction using the resolved endpoint Design \u00b6 At high level trained model controller reconciles the TrainedModel CR and writes the model configurations mounted by the InferenceService model agent container. Model agent runs alongside with the model server container to download and send request to the model server for loading/unloading the models. Models can be sharded into different InferenceServices meaning each pod hosts the same set of models or different pod in heterogeneous way that each pod can host a different set of models. For a more in depth details checkout this document . Model Agent is a critical component which can download and deploy models at scale, here is a detailed diagram how models are delivered to the model server from remote model storage in parallel with go routines. Integration with model servers \u00b6 Multi-model serving will work with any model server that implements KServe V2 protocol load/unload endpoints. More specifically, if the model server implements the load , unload and model health check endpoints then it can use KServe's TrainedModel. Currently, Triton, LightGBM, SKLearn, and XGBoost are able to use Multi-model serving. Click on Triton or SKLearn to see examples on how to run multi-model serving!","title":"Concept"},{"location":"modelserving/mms/multi-model-serving/#multi-model-serving","text":"Multi-model serving is an alpha feature added recently to increase KServe\u2019s scalability. Please assume that the interface is subject to changes.","title":"Multi-Model Serving"},{"location":"modelserving/mms/multi-model-serving/#the-model-deployment-scalability-problem","text":"With machine learning approaches becoming more widely adopted in organizations, there is a trend to deploy a large number of models. For example, a news classification service may train custom models for each news category. Another important reason why organizations desire to train a lot of models is to protect data privacy, as it is safer to isolate each user's data and train models separately. While you get the benefit of better inference accuracy and data privacy by building models for each use case, it is more challenging to deploy thousands to hundreds of thousands of models on a Kubernetes cluster. Furthermore, there are an increasing number of use cases of serving neural network-based models. To achieve reasonable latency, those models are better served on GPUs. However, since GPUs are expensive resources, it is costly to serve many GPU-based models. The original design of KServe deploys one model per InferenceService. But, when dealing with a large number of models, its 'one model, one server' paradigm presents challenges for a Kubernetes cluster. To scale the number of models, we have to scale the number of InferenceServices, something that can quickly challenge the cluster's limits. Multi-model serving is designed to address three types of limitations KServe will run into: Compute resource limitation Maximum pods limitation Maximum IP address limitation.","title":"The model deployment scalability problem"},{"location":"modelserving/mms/multi-model-serving/#compute-resource-limitation","text":"Each InferenceService has a resource overhead because of the sidecars injected into each pod. This normally adds about 0.5 CPU and 0.5G Memory resource per InferenceService replica. For example, if we deploy 10 models, each with 2 replicas, then the resource overhead is 10 * 2 * 0.5 = 10 CPU and 10 * 2 * 0.5 = 10 GB memory. Each model\u2019s resource overhead is 1CPU and 1 GB memory. Deploying many models using the current approach will quickly use up a cluster's computing resource. With Multi-model serving, these models can be loaded in one InferenceService, then each model's average overhead is 0.1 CPU and 0.1GB memory. For GPU based models, the number of GPUs required grows linearly as the number of models grows, which is not cost efficient. If multiple models can be loaded in one GPU enabled model server such as TritonServer, we need a lot less GPUs in the cluster.","title":"Compute resource limitation"},{"location":"modelserving/mms/multi-model-serving/#maximum-pods-limitation","text":"Kubelet has a maximum number of pods per node with the default limit set to 110 . According to Kubernetes best practice , a node shouldn't run more than 100 pods. With this limitation, a typical 50-node cluster with default pod limit can run at most 1000 models assuming each InferenceService has 4 pods on average (two transformer replicas and two predictor replicas).","title":"Maximum pods limitation"},{"location":"modelserving/mms/multi-model-serving/#maximum-ip-address-limitation","text":"Kubernetes clusters also have an IP address limit per cluster. Each pod in InferenceService needs an independent IP. For example a cluster with 4096 IP addresses can deploy at most 1024 models assuming each InferenceService has 4 pods on average (two transformer replicas and two predictor replicas).","title":"Maximum IP address limitation."},{"location":"modelserving/mms/multi-model-serving/#benefit-of-using-multi-model-serving","text":"The Multi-model serving feature is designed to address the three limitations above. It decreases the average resource overhead per model so model deployment becomes more cost efficient. And the number of models which can be deployed in a cluster will no longer be limited by the maximum pods limitation and the maximum IP address limitation. The benchmark test results are able to support the statements above.","title":"Benefit of using Multi-Model serving"},{"location":"modelserving/mms/multi-model-serving/#how-multi-model-serving-address-those-limitations","text":"We designed a new CustomResource called \"TrainedModel\" which represents a machine learning model. It can be loaded into a designated InferenceService. The common user flow with Multi-Model serving is: Deploy an InferenceService without the \"storageUri\" field i.e. without any models loaded Deploy multiple TrainedModel CRs which load models to a designated InferenceService Resolve the model prediction endpoint from TrainedModel's status object Run prediction using the resolved endpoint","title":"How Multi-Model serving address those limitations"},{"location":"modelserving/mms/multi-model-serving/#design","text":"At high level trained model controller reconciles the TrainedModel CR and writes the model configurations mounted by the InferenceService model agent container. Model agent runs alongside with the model server container to download and send request to the model server for loading/unloading the models. Models can be sharded into different InferenceServices meaning each pod hosts the same set of models or different pod in heterogeneous way that each pod can host a different set of models. For a more in depth details checkout this document . Model Agent is a critical component which can download and deploy models at scale, here is a detailed diagram how models are delivered to the model server from remote model storage in parallel with go routines.","title":"Design"},{"location":"modelserving/mms/multi-model-serving/#integration-with-model-servers","text":"Multi-model serving will work with any model server that implements KServe V2 protocol load/unload endpoints. More specifically, if the model server implements the load , unload and model health check endpoints then it can use KServe's TrainedModel. Currently, Triton, LightGBM, SKLearn, and XGBoost are able to use Multi-model serving. Click on Triton or SKLearn to see examples on how to run multi-model serving!","title":"Integration with model servers"},{"location":"modelserving/mms/benchmark/BENCHMARK/","text":"Benchmark of Multi-Model Serving \u00b6 The following stress and performance tests have been executed to support that Multi-Model Serving is a solution to the model deployment scalability problem as discussed in the MMS Guide . Stress Test \u00b6 To determine the maximum amount of models that can be deployed - Environment Setup: - kfserving: v0.5.1 - knative: v0.17.0 - istio: v1.7.x - Cluster limitation: - Maximum 2048 IP Addresses - Maximum 0.55 TiB memory per worker node - Maximum ~2000 kubernetes services resources - Limited by istio ingress gateway - Maximum 660 pods on worker nodes - Maximum 110 pods per worker node - 6 total worker nodes - ETCD size of 65 MiB - Inference Service: - apiVersion: serving.kserve.io/v1beta1 - concurrency: 1 - minReplicas: 1 - Triton model server - Queue_proxy: CPU 1, Memory 500Mi - Traditional Model Serving Predictor Spec: CPU 100m, Memory 100Mi - Multi-Model Serving Predictor Spec: CPU 10, Memory 10Gi - Model: - Simple string - Size of 700B - Tensorflow framework Baseline: \u00b6 Test Summary: The traditional model serving was able to deploy 352 InferenceServices Multi-model serving was able to deploy ~10K models using 6 InferenceService which equates to 2800% more models loaded Overall, Multi-Model Serving utilises less CPU and memory while deploying more The traditional model serving used a total of 387 CPU and 211 GB Multi-model serving used a total of 66 CPU and 63 GB Comparing the queue-proxy usage The traditional model serving queue-proxies used 352 CPU and 176 GB Multi-model serving queue-proxies used 6 CPU and 3 GB Adding/removing models in multi-model serving causes new configmap versions to be generated without ETCD getting the chance to remove the previous versions. This can exhaust the memory of ETCD in case of deploying thousands of models Should consider an external storage to store models Performance Test \u00b6 To compare the latency between traditional model serving and multi-model serving - Environment Setup: - kfserving: v0.5.1 - knative: v0.17.0 - istio: v1.7.x - Inference Service: - apiVersion: serving.kserve.io/v1beta1 - concurrency: 1 - minReplicas: 1 - Triton model server - Queue_proxy: 1 CPU and 500Mi - Simple String: - Traditional Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Multi-Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Bert: - Traditional Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Multi-Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Vegeta loading test agent: - Simple String (CPU): - model size of 700B - testing duration 1 min - Queries per second (QPS): 5/sec, 10/sec, 20/sec, 30/sec, 40/sec, 50/sec, 100/sec - Bert (GPU): - testing duration 1 min - ueries per second (QPS): 5/sec, 10/sec, 20/sec, 30/sec, 40/sec Simple String test (CPU) \u00b6 Test Summary: The traditional model serving using 5 inferenceService is able to handle up to 2500 queries per second without scaling Multi-model serving with 1 inferenceservice and 5 trainedModels starts to scale up at 500 queries per second The relative latency between traditional and multi-model serving seems comparable when not scaling up Multi-model serving seems to be more sensitive to autoscaling Test Results: Traditional model serving with 5 inferenceServices Client side QPS QPS per InferenceService (model) mean p50 p90 p95 p99 max 25 5 3.763 3.669 4.089 4.222 4.511 43.877 50 10 3.78 3.546 3.936 4.097 6.956 220.716 100 20 3.594 3.496 3.875 4.008 4.438 84.529 150 30 3.59 3.294 3.642 3.744 6.191 260.923 200 40 3.508 3.169 3.535 3.631 5.659 205.389 250 50 3.882 3.288 3.559 3.642 9.411 302.492 500 100 4.282 3.086 3.471 3.604 52.807 324.815 1000 200 6.266 3.062 3.436 6.7 116.774 308.413 1500 300 10.365 2.828 5.901 55.38 183.41 1039 2000 400 9.338 2.783 4.288 32.912 178.837 1058 2500 500 11.796 2.719 8.509 64.447 217.911 1043 Multi-model serving with 1 inferenceService and 5 TrainedModels Client side QPS QPS per InferenceService QPS per TrainedModel mean p50 p90 p95 p99 max 25 25 5 4.453 3.85 4.181 4.282 26.465 138.671 50 50 10 4.081 3.676 3.934 4.048 11.653 107.775 100 100 20 5.051 3.304 3.625 3.772 72.291 193.219 150 150 30 4.403 3.399 3.684 3.823 46.981 119.896 200 200 40 6.071 3.296 3.63 8.587 90.514 203.785 250 250 50 6.846 3.223 3.575 23.825 100.167 351.26 500 500 100 260.202 2.846 1298 2025 2241 2475 At 500 QPS the multi-model serving starts to scale up which causes the latency to dramatically increase. The benchmark for this case will stop at 500 QPS because 500+ QPS will lead to pods scaling which is not what the focus should be on. Bert test (GPU) \u00b6 Test Summary: The traditional model serving using 5 inferenceService is able to handle up to 200 queries per second without scaling Multi-model serving with 1 inferenceservice and 5 trainedModels is able to handle up to 40 queries per second without scaling The latency is relatively the same for traditional and multi-model serving Multi-model serving seems to be more sensitive to autoscaling Test Results: Traditional model serving with 5 inferenceServices Client side QPS QPS per InferenceService (model) mean p50 p90 p95 p99 max 5 1 33.845ms 32.986ms 36.644ms 41.596ms 100.959ms 131.195ms 10 2 34.212ms 32.058ms 42.791ms 55.192ms 89.832ms 154.485ms 20 4 30.133ms 31.262ms 33.721ms 35.142ms 40.981ms 102.801ms 30 6 30.726ms 30.32ms 33.607ms 37.405ms 64.624ms 700.839ms 40 8 29.646ms 29.785ms 32.164ms 32.856ms 39.188ms 792.061ms 50 10 29.458ms 29.519ms 32.725ms 33.833ms 44.221ms 670.303ms 100 20 26.844ms 25.783ms 31.322ms 32.179ms 39.265ms 811.652ms 200 40 51.627ms 25.389ms 107.946ms 175.522ms 321.154ms 1.616s Multi-Model Serving with 1 inferenceService and 5 trainedmodels Client side QPS QPS per InferenceService QPS per TrainedModel mean p50 p90 p95 p99 max 5 5 1 30.978ms 30.195ms 32.435ms 52.474ms 59.936ms 139.195ms 10 10 2 31.903ms 32.795ms 35.105ms 37.138ms 48.254ms 266.965m 20 20 4 29.782ms 30.777ms 34.452ms 36.409ms 46.074ms 256.518ms 30 30 6 24.929ms 23.548ms 30.218ms 30.935ms 49.506ms 205.663ms 40 40 8 34.087ms 24.483ms 50.588ms 87.01ms 155.853ms 801.393ms","title":"Benchmark of Multi-Model Serving"},{"location":"modelserving/mms/benchmark/BENCHMARK/#benchmark-of-multi-model-serving","text":"The following stress and performance tests have been executed to support that Multi-Model Serving is a solution to the model deployment scalability problem as discussed in the MMS Guide .","title":"Benchmark of Multi-Model Serving"},{"location":"modelserving/mms/benchmark/BENCHMARK/#stress-test","text":"To determine the maximum amount of models that can be deployed - Environment Setup: - kfserving: v0.5.1 - knative: v0.17.0 - istio: v1.7.x - Cluster limitation: - Maximum 2048 IP Addresses - Maximum 0.55 TiB memory per worker node - Maximum ~2000 kubernetes services resources - Limited by istio ingress gateway - Maximum 660 pods on worker nodes - Maximum 110 pods per worker node - 6 total worker nodes - ETCD size of 65 MiB - Inference Service: - apiVersion: serving.kserve.io/v1beta1 - concurrency: 1 - minReplicas: 1 - Triton model server - Queue_proxy: CPU 1, Memory 500Mi - Traditional Model Serving Predictor Spec: CPU 100m, Memory 100Mi - Multi-Model Serving Predictor Spec: CPU 10, Memory 10Gi - Model: - Simple string - Size of 700B - Tensorflow framework","title":"Stress Test"},{"location":"modelserving/mms/benchmark/BENCHMARK/#baseline","text":"Test Summary: The traditional model serving was able to deploy 352 InferenceServices Multi-model serving was able to deploy ~10K models using 6 InferenceService which equates to 2800% more models loaded Overall, Multi-Model Serving utilises less CPU and memory while deploying more The traditional model serving used a total of 387 CPU and 211 GB Multi-model serving used a total of 66 CPU and 63 GB Comparing the queue-proxy usage The traditional model serving queue-proxies used 352 CPU and 176 GB Multi-model serving queue-proxies used 6 CPU and 3 GB Adding/removing models in multi-model serving causes new configmap versions to be generated without ETCD getting the chance to remove the previous versions. This can exhaust the memory of ETCD in case of deploying thousands of models Should consider an external storage to store models","title":"Baseline:"},{"location":"modelserving/mms/benchmark/BENCHMARK/#performance-test","text":"To compare the latency between traditional model serving and multi-model serving - Environment Setup: - kfserving: v0.5.1 - knative: v0.17.0 - istio: v1.7.x - Inference Service: - apiVersion: serving.kserve.io/v1beta1 - concurrency: 1 - minReplicas: 1 - Triton model server - Queue_proxy: 1 CPU and 500Mi - Simple String: - Traditional Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Multi-Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Bert: - Traditional Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Multi-Model Serving Predictor Spec: CPU 1, Memory 16GB, GPU 1 - Vegeta loading test agent: - Simple String (CPU): - model size of 700B - testing duration 1 min - Queries per second (QPS): 5/sec, 10/sec, 20/sec, 30/sec, 40/sec, 50/sec, 100/sec - Bert (GPU): - testing duration 1 min - ueries per second (QPS): 5/sec, 10/sec, 20/sec, 30/sec, 40/sec","title":"Performance Test"},{"location":"modelserving/mms/benchmark/BENCHMARK/#simple-string-test-cpu","text":"Test Summary: The traditional model serving using 5 inferenceService is able to handle up to 2500 queries per second without scaling Multi-model serving with 1 inferenceservice and 5 trainedModels starts to scale up at 500 queries per second The relative latency between traditional and multi-model serving seems comparable when not scaling up Multi-model serving seems to be more sensitive to autoscaling Test Results: Traditional model serving with 5 inferenceServices Client side QPS QPS per InferenceService (model) mean p50 p90 p95 p99 max 25 5 3.763 3.669 4.089 4.222 4.511 43.877 50 10 3.78 3.546 3.936 4.097 6.956 220.716 100 20 3.594 3.496 3.875 4.008 4.438 84.529 150 30 3.59 3.294 3.642 3.744 6.191 260.923 200 40 3.508 3.169 3.535 3.631 5.659 205.389 250 50 3.882 3.288 3.559 3.642 9.411 302.492 500 100 4.282 3.086 3.471 3.604 52.807 324.815 1000 200 6.266 3.062 3.436 6.7 116.774 308.413 1500 300 10.365 2.828 5.901 55.38 183.41 1039 2000 400 9.338 2.783 4.288 32.912 178.837 1058 2500 500 11.796 2.719 8.509 64.447 217.911 1043 Multi-model serving with 1 inferenceService and 5 TrainedModels Client side QPS QPS per InferenceService QPS per TrainedModel mean p50 p90 p95 p99 max 25 25 5 4.453 3.85 4.181 4.282 26.465 138.671 50 50 10 4.081 3.676 3.934 4.048 11.653 107.775 100 100 20 5.051 3.304 3.625 3.772 72.291 193.219 150 150 30 4.403 3.399 3.684 3.823 46.981 119.896 200 200 40 6.071 3.296 3.63 8.587 90.514 203.785 250 250 50 6.846 3.223 3.575 23.825 100.167 351.26 500 500 100 260.202 2.846 1298 2025 2241 2475 At 500 QPS the multi-model serving starts to scale up which causes the latency to dramatically increase. The benchmark for this case will stop at 500 QPS because 500+ QPS will lead to pods scaling which is not what the focus should be on.","title":"Simple String test (CPU)"},{"location":"modelserving/mms/benchmark/BENCHMARK/#bert-test-gpu","text":"Test Summary: The traditional model serving using 5 inferenceService is able to handle up to 200 queries per second without scaling Multi-model serving with 1 inferenceservice and 5 trainedModels is able to handle up to 40 queries per second without scaling The latency is relatively the same for traditional and multi-model serving Multi-model serving seems to be more sensitive to autoscaling Test Results: Traditional model serving with 5 inferenceServices Client side QPS QPS per InferenceService (model) mean p50 p90 p95 p99 max 5 1 33.845ms 32.986ms 36.644ms 41.596ms 100.959ms 131.195ms 10 2 34.212ms 32.058ms 42.791ms 55.192ms 89.832ms 154.485ms 20 4 30.133ms 31.262ms 33.721ms 35.142ms 40.981ms 102.801ms 30 6 30.726ms 30.32ms 33.607ms 37.405ms 64.624ms 700.839ms 40 8 29.646ms 29.785ms 32.164ms 32.856ms 39.188ms 792.061ms 50 10 29.458ms 29.519ms 32.725ms 33.833ms 44.221ms 670.303ms 100 20 26.844ms 25.783ms 31.322ms 32.179ms 39.265ms 811.652ms 200 40 51.627ms 25.389ms 107.946ms 175.522ms 321.154ms 1.616s Multi-Model Serving with 1 inferenceService and 5 trainedmodels Client side QPS QPS per InferenceService QPS per TrainedModel mean p50 p90 p95 p99 max 5 5 1 30.978ms 30.195ms 32.435ms 52.474ms 59.936ms 139.195ms 10 10 2 31.903ms 32.795ms 35.105ms 37.138ms 48.254ms 266.965m 20 20 4 29.782ms 30.777ms 34.452ms 36.409ms 46.074ms 256.518ms 30 30 6 24.929ms 23.548ms 30.218ms 30.935ms 49.506ms 205.663ms 40 40 8 34.087ms 24.483ms 50.588ms 87.01ms 155.853ms 801.393ms","title":"Bert test (GPU)"},{"location":"modelserving/mms/sklearn/","text":"Multi-Model Serving with scikit-learn \u00b6 Deploy InferenceService with scikit-learn Runtime \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"sklearn-iris-example\" spec : predictor : minReplicas : 1 sklearn : protocolVersion : v1 name : \"sklearn-iris-predictor\" resources : limits : cpu : 100m memory : 512Mi requests : cpu : 100m memory : 512Mi Create the InfrenceService: kubectl kubectl apply -f inferenceservice.yaml Check if the InfrenceService is ready kubectl kubectl get isvc sklearn-iris-example NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE sklearn-iris-example http://sklearn-iris-example.default.example.com True 100 sklearn-iris-example-predictor-default-kgtql 22s Deploy TrainedModel \u00b6 apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"model1-sklearn\" spec : inferenceService : \"sklearn-iris-example\" model : storageUri : \"gs://kfserving-samples/models/sklearn/iris\" framework : \"sklearn\" memory : \"256Mi\" --- apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"model2-sklearn\" spec : inferenceService : \"sklearn-iris-example\" model : storageUri : \"gs://kfserving-samples/models/sklearn/iris\" framework : \"sklearn\" memory : \"256Mi\" Create the trained models: kubectl kubectl apply -f trainedmodels.yaml Check model agent logs to check if the models are properly loaded. You should get the similar output as below, wait a few minutes and try again if you do not see \"Downloading model\". kubectl kubectl logs <name-of-predictor-pod> -c agent { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.421Z\" , \"caller\" : \"agent/puller.go:129\" , \"msg\" : \"Downloading model from gs://kfserving-samples/models/sklearn/iris\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.421Z\" , \"caller\" : \"agent/downloader.go:47\" , \"msg\" : \"Downloading gs://kfserving-samples/models/sklearn/iris to model dir /mnt/models\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.424Z\" , \"caller\" : \"agent/puller.go:121\" , \"msg\" : \"Worker is started for model1-sklearn\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.424Z\" , \"caller\" : \"agent/puller.go:129\" , \"msg\" : \"Downloading model from gs://kfserving-samples/models/sklearn/iris\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.424Z\" , \"caller\" : \"agent/downloader.go:47\" , \"msg\" : \"Downloading gs://kfserving-samples/models/sklearn/iris to model dir /mnt/models\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.255Z\" , \"caller\" : \"agent/puller.go:146\" , \"msg\" : \"Successfully loaded model model2-sklearn\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.256Z\" , \"caller\" : \"agent/puller.go:114\" , \"msg\" : \"completion event for model model2-sklearn, in flight ops 0\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.260Z\" , \"caller\" : \"agent/puller.go:146\" , \"msg\" : \"Successfully loaded model model1-sklearn\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.260Z\" , \"caller\" : \"agent/puller.go:114\" , \"msg\" : \"completion event for model model1-sklearn, in flight ops 0\" } The models will be ready to serve once they are successfully loaded. Run Inference \u00b6 Now, assuming that your ingress can be accessed at ${INGRESS_HOST}:${INGRESS_PORT} or you can follow this instruction to find out your ingress IP and port. curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/model1-sklearn:predict -d @./iris-input.json curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/model2-sklearn:predict -d @./iris-input.json The outputs should be { \"predictions\" : [ 1 , 1 ]} Delete the InferenceService \u00b6 To remove the resources, run the command kubectl delete inferenceservice sklearn-iris-example . This will delete the inference service and result in the trained models being deleted.","title":"scikit-learn"},{"location":"modelserving/mms/sklearn/#multi-model-serving-with-scikit-learn","text":"","title":"Multi-Model Serving with scikit-learn"},{"location":"modelserving/mms/sklearn/#deploy-inferenceservice-with-scikit-learn-runtime","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"sklearn-iris-example\" spec : predictor : minReplicas : 1 sklearn : protocolVersion : v1 name : \"sklearn-iris-predictor\" resources : limits : cpu : 100m memory : 512Mi requests : cpu : 100m memory : 512Mi Create the InfrenceService: kubectl kubectl apply -f inferenceservice.yaml Check if the InfrenceService is ready kubectl kubectl get isvc sklearn-iris-example NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE sklearn-iris-example http://sklearn-iris-example.default.example.com True 100 sklearn-iris-example-predictor-default-kgtql 22s","title":"Deploy InferenceService with scikit-learn Runtime"},{"location":"modelserving/mms/sklearn/#deploy-trainedmodel","text":"apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"model1-sklearn\" spec : inferenceService : \"sklearn-iris-example\" model : storageUri : \"gs://kfserving-samples/models/sklearn/iris\" framework : \"sklearn\" memory : \"256Mi\" --- apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"model2-sklearn\" spec : inferenceService : \"sklearn-iris-example\" model : storageUri : \"gs://kfserving-samples/models/sklearn/iris\" framework : \"sklearn\" memory : \"256Mi\" Create the trained models: kubectl kubectl apply -f trainedmodels.yaml Check model agent logs to check if the models are properly loaded. You should get the similar output as below, wait a few minutes and try again if you do not see \"Downloading model\". kubectl kubectl logs <name-of-predictor-pod> -c agent { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.421Z\" , \"caller\" : \"agent/puller.go:129\" , \"msg\" : \"Downloading model from gs://kfserving-samples/models/sklearn/iris\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.421Z\" , \"caller\" : \"agent/downloader.go:47\" , \"msg\" : \"Downloading gs://kfserving-samples/models/sklearn/iris to model dir /mnt/models\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.424Z\" , \"caller\" : \"agent/puller.go:121\" , \"msg\" : \"Worker is started for model1-sklearn\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.424Z\" , \"caller\" : \"agent/puller.go:129\" , \"msg\" : \"Downloading model from gs://kfserving-samples/models/sklearn/iris\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:00.424Z\" , \"caller\" : \"agent/downloader.go:47\" , \"msg\" : \"Downloading gs://kfserving-samples/models/sklearn/iris to model dir /mnt/models\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.255Z\" , \"caller\" : \"agent/puller.go:146\" , \"msg\" : \"Successfully loaded model model2-sklearn\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.256Z\" , \"caller\" : \"agent/puller.go:114\" , \"msg\" : \"completion event for model model2-sklearn, in flight ops 0\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.260Z\" , \"caller\" : \"agent/puller.go:146\" , \"msg\" : \"Successfully loaded model model1-sklearn\" } { \"level\" : \"info\" , \"ts\" : \"2021-01-20T16:24:09.260Z\" , \"caller\" : \"agent/puller.go:114\" , \"msg\" : \"completion event for model model1-sklearn, in flight ops 0\" } The models will be ready to serve once they are successfully loaded.","title":"Deploy TrainedModel"},{"location":"modelserving/mms/sklearn/#run-inference","text":"Now, assuming that your ingress can be accessed at ${INGRESS_HOST}:${INGRESS_PORT} or you can follow this instruction to find out your ingress IP and port. curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/model1-sklearn:predict -d @./iris-input.json curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/model2-sklearn:predict -d @./iris-input.json The outputs should be { \"predictions\" : [ 1 , 1 ]}","title":"Run Inference"},{"location":"modelserving/mms/sklearn/#delete-the-inferenceservice","text":"To remove the resources, run the command kubectl delete inferenceservice sklearn-iris-example . This will delete the inference service and result in the trained models being deleted.","title":"Delete the InferenceService"},{"location":"modelserving/mms/triton/","text":"Multi Model Serving with Triton(Alpha) \u00b6 There are growing use cases of developing per-user or per-category ML models instead of cohort model. For example a news classification service trains custom model based on each news category, a recommendation model trains on each user's usage history to personalize the recommendation. While you get the benefit of better inference accuracy by building models for each use case, the cost of deploying models increase significantly because you may train anywhere from hundreds to thousands of custom models, and it becomes difficult to manage so many models on production. These challenges become more pronounced when you don\u2019t access all models at the same time but still need them to be available at all times. KServe multi model serving design addresses these issues and gives a scalable yet cost-effective solution to deploy multiple models. Setup \u00b6 Skip tag resolution for nvcr.io which requires auth to resolve triton inference server image digest kubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving Increase progress deadline since pulling triton image and big bert model may longer than default timeout for 120s, this setting requires knative 0.15.0+ kubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving Create the hosting InferenceService \u00b6 KServe Multi Model Serving design decouples the trained model artifact from the hosting InferenceService . You first create a hosting InferenceService without StoragegUri and then deploy multiple TrainedModel CRs onto the designated InferenceService . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"triton-mms\" spec : predictor : triton : args : - --log-verbose=1 resources : limits : cpu : \"1\" memory : 2Gi requests : cpu : \"1\" memory : 2Gi Note that you create the hosting InferenceService with enough memory resource for hosting multiple models. kubectl apply -f multi_model_triton.yaml Check the InferenceService status kubectl get isvc triton-mms NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE triton-mms http://triton-mms.default.example.com True 100 triton-mms-predictor-default-00001 23h Deploy Trained Model \u00b6 Now you have an InferenceService running with 2Gi memory but no model is deployed on InferenceService yet, let's deploy the trained models on InferenceService by applying the TrainedModel CRs. On TrainedModel CR you specify the following fields: - Hosting InferenceService : The InferenceService you want the trained model to deploy to. - Model framework: the ML framework you trained with, the trained model validation webhook validates the framework if it is supported by the model server in this case Triton Inference Server . - Estimated model memory resource: the trained model validation webhook validates that the summed memory of all trained models do no exceed the parent InferenceService memory limit. Deploy Cifar10 TorchScript Model \u00b6 apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"cifar10\" spec : inferenceService : triton-mms model : framework : pytorch storageUri : gs://kfserving-examples/models/torchscript/cifar10 memory : 1Gi kubectl apply -f trained_model.yaml After TrainedModel CR is applied, check the model agent sidecar logs, the model agent downloads the model from specified StorageUri to the mounted model repository and then calls the load endpoint of Triton Inference Server . kubectl logs triton-mms-predictor-default-2g8lg-deployment-69c9964bc4-mfg92 agent { \"level\" : \"info\" , \"ts\" :1621717148.93849, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/watcher.go:173\" , \"msg\" : \"adding model cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717148.939014, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:136\" , \"msg\" : \"Worker is started for cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717148.9393005, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:144\" , \"msg\" : \"Downloading model from gs://kfserving-examples/models/torchscript/cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717148.939781, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/downloader.go:48\" , \"msg\" : \"Downloading gs://kfserving-examples/models/torchscript/cifar10 to model dir /mnt/models\" } { \"level\" : \"info\" , \"ts\" :1621717149.4635677, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/downloader.go:68\" , \"msg\" : \"Creating successFile /mnt/models/cifar10/SUCCESS.71f376a9daa07a04ae1bd52cbe7f3a2c46ceb701350d9dffc73381df5a230923\" } { \"level\" : \"info\" , \"ts\" :1621717149.6402793, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:161\" , \"msg\" : \"Successfully loaded model cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717149.6404483, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:129\" , \"msg\" : \"completion event for model cifar10, in flight ops 0\" } Check the Triton Inference Service log you will see that the model is loaded into the memory. I0522 20 :59:09.469834 1 model_repository_manager.cc:737 ] loading: cifar10:1 I0522 20 :59:09.471278 1 libtorch_backend.cc:217 ] Creating instance cifar10_0_0_cpu on CPU using model.pt I0522 20 :59:09.638318 1 model_repository_manager.cc:925 ] successfully loaded 'cifar10' version 1 Check the TrainedModel CR status kubectl get tm cifar10 NAME URL READY AGE cifar10 http://triton-mms.default.example.com/v2/models/cifar10/infer True 3h45m # to show more detailed status kubectl get tm cifar10 -oyaml status: address: url: http://triton-mms.default.svc.cluster.local/v2/models/cifar10/infer conditions: - lastTransitionTime: \"2021-05-22T20:56:12Z\" status: \"True\" type: FrameworkSupported - lastTransitionTime: \"2021-05-22T20:56:12Z\" status: \"True\" type: InferenceServiceReady - lastTransitionTime: \"2021-05-22T20:56:12Z\" status: \"True\" type: IsMMSPredictor - lastTransitionTime: \"2021-05-22T20:58:56Z\" status: \"True\" type: MemoryResourceAvailable - lastTransitionTime: \"2021-05-22T20:58:56Z\" status: \"True\" type: Ready url: http://triton-mms.default.example.com/v2/models/cifar10/infer Currently the trained model CR does not reflect download and load/unload status, it has been worked on in the issue for adding the probing component . Now you can curl the model metadata endpoint MODEL_NAME = cifar10 SERVICE_HOSTNAME = $( kubectl get inferenceservices triton-mms -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/ $MODEL_NAME { \"name\" : \"cifar10\" , \"versions\" : [ \"1\" ] , \"platform\" : \"pytorch_libtorch\" , \"inputs\" : [{ \"name\" : \"INPUT__0\" , \"datatype\" : \"FP32\" , \"shape\" : [ -1,3,32,32 ]}] , \"outputs\" : [{ \"name\" : \"OUTPUT__0\" , \"datatype\" : \"FP32\" , \"shape\" : [ -1,10 ]}]} Deploy Simple String Tensorflow Model \u00b6 Next let's deploy another model to the same InferenceService . The TrainModel resource name must be the same as the model name specified in triton model configuration. apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"simple-string\" spec : inferenceService : triton-mms model : framework : tensorflow storageUri : gs://kfserving-examples/models/tensorrt/simple_string memory : 1Gi Check the Triton Inference Service log you will see that the simple-string model is also loaded into the memory. I0523 00 :04:19.298966 1 model_repository_manager.cc:737 ] loading: simple-string:1 I0523 00 :04:20.367808 1 tensorflow.cc:1281 ] Creating instance simple-string on CPU using model.graphdef I0523 00 :04:20.497748 1 model_repository_manager.cc:925 ] successfully loaded 'simple-string' version 1 Check the TrainedModel CR status kubectl get tm simple-string NAME URL READY AGE simple-string http://triton-mms.default.example.com/v2/models/simple-string/infer True 20h # to show more detailed status kubectl get tm cifar10 -oyaml status: address: url: http://triton-mms.default.svc.cluster.local/v2/models/simple-string/infer conditions: - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: FrameworkSupported - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: InferenceServiceReady - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: IsMMSPredictor - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: MemoryResourceAvailable - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: Ready url: http://triton-mms.default.example.com/v2/models/simple-string/infer Now you can curl the simple-string model metadata endpoint MODEL_NAME = simple-string SERVICE_HOSTNAME = $( kubectl get inferenceservices triton-mms -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/ $MODEL_NAME { \"name\" : \"simple-string\" , \"versions\" : [ \"1\" ] , \"platform\" : \"tensorflow_graphdef\" , \"inputs\" : [{ \"name\" : \"INPUT0\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]} , { \"name\" : \"INPUT1\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]}] , \"outputs\" : [{ \"name\" : \"OUTPUT0\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]} , { \"name\" : \"OUTPUT1\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]}]} Run Performance Test \u00b6 The performance job runs vegeta load testing to the MultiModelInferenceService with model cifar10 . kubectl create -f perf.yaml Requests [ total, rate, throughput ] 600 , 10 .02, 10 .01 Duration [ total, attack, wait ] 59 .912s, 59 .9s, 11 .755ms Latencies [ min, mean, 50 , 90 , 95 , 99 , max ] 5 .893ms, 11 .262ms, 10 .252ms, 16 .077ms, 18 .804ms, 26 .745ms, 39 .202ms Bytes In [ total, mean ] 189000 , 315 .00 Bytes Out [ total, mean ] 66587400 , 110979 .00 Success [ ratio ] 100 .00% Status Codes [ code:count ] 200 :600 Error Set: Homogeneous model allocation and Autoscaling \u00b6 The current MMS implementation uses the homogeneous approach meaning that each InferenceService replica holds the same set of models. Autoscaling is based on the aggregated traffic for this set of models NOT the request volume for individual model, this set of models is always scaled up together. The downside of this approach is that traffic spike for one model can result in scaling out entire set of models despite the low request volume for other models hosted on the same InferenceService , this may not be desirable for InferenceService that hosts a set of big models. The other approach is to use the heterogeneous allocation where each replica can host a different set of models, models are scaled up/down individually based on its own traffic and in this way it ensures better resource utilization. Delete Trained Models \u00b6 To remove the resources, run the command kubectl delete inferenceservice triton-mms . This will delete the inference service and result in the trained models deleted. To delete individual TrainedModel you can run the command kubectl delete tm $MODEL_NAME .","title":"Triton inference server"},{"location":"modelserving/mms/triton/#multi-model-serving-with-tritonalpha","text":"There are growing use cases of developing per-user or per-category ML models instead of cohort model. For example a news classification service trains custom model based on each news category, a recommendation model trains on each user's usage history to personalize the recommendation. While you get the benefit of better inference accuracy by building models for each use case, the cost of deploying models increase significantly because you may train anywhere from hundreds to thousands of custom models, and it becomes difficult to manage so many models on production. These challenges become more pronounced when you don\u2019t access all models at the same time but still need them to be available at all times. KServe multi model serving design addresses these issues and gives a scalable yet cost-effective solution to deploy multiple models.","title":"Multi Model Serving with Triton(Alpha)"},{"location":"modelserving/mms/triton/#setup","text":"Skip tag resolution for nvcr.io which requires auth to resolve triton inference server image digest kubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving Increase progress deadline since pulling triton image and big bert model may longer than default timeout for 120s, this setting requires knative 0.15.0+ kubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving","title":"Setup"},{"location":"modelserving/mms/triton/#create-the-hosting-inferenceservice","text":"KServe Multi Model Serving design decouples the trained model artifact from the hosting InferenceService . You first create a hosting InferenceService without StoragegUri and then deploy multiple TrainedModel CRs onto the designated InferenceService . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"triton-mms\" spec : predictor : triton : args : - --log-verbose=1 resources : limits : cpu : \"1\" memory : 2Gi requests : cpu : \"1\" memory : 2Gi Note that you create the hosting InferenceService with enough memory resource for hosting multiple models. kubectl apply -f multi_model_triton.yaml Check the InferenceService status kubectl get isvc triton-mms NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE triton-mms http://triton-mms.default.example.com True 100 triton-mms-predictor-default-00001 23h","title":"Create the hosting InferenceService"},{"location":"modelserving/mms/triton/#deploy-trained-model","text":"Now you have an InferenceService running with 2Gi memory but no model is deployed on InferenceService yet, let's deploy the trained models on InferenceService by applying the TrainedModel CRs. On TrainedModel CR you specify the following fields: - Hosting InferenceService : The InferenceService you want the trained model to deploy to. - Model framework: the ML framework you trained with, the trained model validation webhook validates the framework if it is supported by the model server in this case Triton Inference Server . - Estimated model memory resource: the trained model validation webhook validates that the summed memory of all trained models do no exceed the parent InferenceService memory limit.","title":"Deploy Trained Model"},{"location":"modelserving/mms/triton/#deploy-cifar10-torchscript-model","text":"apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"cifar10\" spec : inferenceService : triton-mms model : framework : pytorch storageUri : gs://kfserving-examples/models/torchscript/cifar10 memory : 1Gi kubectl apply -f trained_model.yaml After TrainedModel CR is applied, check the model agent sidecar logs, the model agent downloads the model from specified StorageUri to the mounted model repository and then calls the load endpoint of Triton Inference Server . kubectl logs triton-mms-predictor-default-2g8lg-deployment-69c9964bc4-mfg92 agent { \"level\" : \"info\" , \"ts\" :1621717148.93849, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/watcher.go:173\" , \"msg\" : \"adding model cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717148.939014, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:136\" , \"msg\" : \"Worker is started for cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717148.9393005, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:144\" , \"msg\" : \"Downloading model from gs://kfserving-examples/models/torchscript/cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717148.939781, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/downloader.go:48\" , \"msg\" : \"Downloading gs://kfserving-examples/models/torchscript/cifar10 to model dir /mnt/models\" } { \"level\" : \"info\" , \"ts\" :1621717149.4635677, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/downloader.go:68\" , \"msg\" : \"Creating successFile /mnt/models/cifar10/SUCCESS.71f376a9daa07a04ae1bd52cbe7f3a2c46ceb701350d9dffc73381df5a230923\" } { \"level\" : \"info\" , \"ts\" :1621717149.6402793, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:161\" , \"msg\" : \"Successfully loaded model cifar10\" } { \"level\" : \"info\" , \"ts\" :1621717149.6404483, \"logger\" : \"fallback-logger\" , \"caller\" : \"agent/puller.go:129\" , \"msg\" : \"completion event for model cifar10, in flight ops 0\" } Check the Triton Inference Service log you will see that the model is loaded into the memory. I0522 20 :59:09.469834 1 model_repository_manager.cc:737 ] loading: cifar10:1 I0522 20 :59:09.471278 1 libtorch_backend.cc:217 ] Creating instance cifar10_0_0_cpu on CPU using model.pt I0522 20 :59:09.638318 1 model_repository_manager.cc:925 ] successfully loaded 'cifar10' version 1 Check the TrainedModel CR status kubectl get tm cifar10 NAME URL READY AGE cifar10 http://triton-mms.default.example.com/v2/models/cifar10/infer True 3h45m # to show more detailed status kubectl get tm cifar10 -oyaml status: address: url: http://triton-mms.default.svc.cluster.local/v2/models/cifar10/infer conditions: - lastTransitionTime: \"2021-05-22T20:56:12Z\" status: \"True\" type: FrameworkSupported - lastTransitionTime: \"2021-05-22T20:56:12Z\" status: \"True\" type: InferenceServiceReady - lastTransitionTime: \"2021-05-22T20:56:12Z\" status: \"True\" type: IsMMSPredictor - lastTransitionTime: \"2021-05-22T20:58:56Z\" status: \"True\" type: MemoryResourceAvailable - lastTransitionTime: \"2021-05-22T20:58:56Z\" status: \"True\" type: Ready url: http://triton-mms.default.example.com/v2/models/cifar10/infer Currently the trained model CR does not reflect download and load/unload status, it has been worked on in the issue for adding the probing component . Now you can curl the model metadata endpoint MODEL_NAME = cifar10 SERVICE_HOSTNAME = $( kubectl get inferenceservices triton-mms -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/ $MODEL_NAME { \"name\" : \"cifar10\" , \"versions\" : [ \"1\" ] , \"platform\" : \"pytorch_libtorch\" , \"inputs\" : [{ \"name\" : \"INPUT__0\" , \"datatype\" : \"FP32\" , \"shape\" : [ -1,3,32,32 ]}] , \"outputs\" : [{ \"name\" : \"OUTPUT__0\" , \"datatype\" : \"FP32\" , \"shape\" : [ -1,10 ]}]}","title":"Deploy Cifar10 TorchScript Model"},{"location":"modelserving/mms/triton/#deploy-simple-string-tensorflow-model","text":"Next let's deploy another model to the same InferenceService . The TrainModel resource name must be the same as the model name specified in triton model configuration. apiVersion : \"serving.kserve.io/v1alpha1\" kind : \"TrainedModel\" metadata : name : \"simple-string\" spec : inferenceService : triton-mms model : framework : tensorflow storageUri : gs://kfserving-examples/models/tensorrt/simple_string memory : 1Gi Check the Triton Inference Service log you will see that the simple-string model is also loaded into the memory. I0523 00 :04:19.298966 1 model_repository_manager.cc:737 ] loading: simple-string:1 I0523 00 :04:20.367808 1 tensorflow.cc:1281 ] Creating instance simple-string on CPU using model.graphdef I0523 00 :04:20.497748 1 model_repository_manager.cc:925 ] successfully loaded 'simple-string' version 1 Check the TrainedModel CR status kubectl get tm simple-string NAME URL READY AGE simple-string http://triton-mms.default.example.com/v2/models/simple-string/infer True 20h # to show more detailed status kubectl get tm cifar10 -oyaml status: address: url: http://triton-mms.default.svc.cluster.local/v2/models/simple-string/infer conditions: - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: FrameworkSupported - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: InferenceServiceReady - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: IsMMSPredictor - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: MemoryResourceAvailable - lastTransitionTime: \"2021-05-23T00:02:42Z\" status: \"True\" type: Ready url: http://triton-mms.default.example.com/v2/models/simple-string/infer Now you can curl the simple-string model metadata endpoint MODEL_NAME = simple-string SERVICE_HOSTNAME = $( kubectl get inferenceservices triton-mms -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/ $MODEL_NAME { \"name\" : \"simple-string\" , \"versions\" : [ \"1\" ] , \"platform\" : \"tensorflow_graphdef\" , \"inputs\" : [{ \"name\" : \"INPUT0\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]} , { \"name\" : \"INPUT1\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]}] , \"outputs\" : [{ \"name\" : \"OUTPUT0\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]} , { \"name\" : \"OUTPUT1\" , \"datatype\" : \"BYTES\" , \"shape\" : [ -1,16 ]}]}","title":"Deploy Simple String Tensorflow Model"},{"location":"modelserving/mms/triton/#run-performance-test","text":"The performance job runs vegeta load testing to the MultiModelInferenceService with model cifar10 . kubectl create -f perf.yaml Requests [ total, rate, throughput ] 600 , 10 .02, 10 .01 Duration [ total, attack, wait ] 59 .912s, 59 .9s, 11 .755ms Latencies [ min, mean, 50 , 90 , 95 , 99 , max ] 5 .893ms, 11 .262ms, 10 .252ms, 16 .077ms, 18 .804ms, 26 .745ms, 39 .202ms Bytes In [ total, mean ] 189000 , 315 .00 Bytes Out [ total, mean ] 66587400 , 110979 .00 Success [ ratio ] 100 .00% Status Codes [ code:count ] 200 :600 Error Set:","title":"Run Performance Test"},{"location":"modelserving/mms/triton/#homogeneous-model-allocation-and-autoscaling","text":"The current MMS implementation uses the homogeneous approach meaning that each InferenceService replica holds the same set of models. Autoscaling is based on the aggregated traffic for this set of models NOT the request volume for individual model, this set of models is always scaled up together. The downside of this approach is that traffic spike for one model can result in scaling out entire set of models despite the low request volume for other models hosted on the same InferenceService , this may not be desirable for InferenceService that hosts a set of big models. The other approach is to use the heterogeneous allocation where each replica can host a different set of models, models are scaled up/down individually based on its own traffic and in this way it ensures better resource utilization.","title":"Homogeneous model allocation and Autoscaling"},{"location":"modelserving/mms/triton/#delete-trained-models","text":"To remove the resources, run the command kubectl delete inferenceservice triton-mms . This will delete the inference service and result in the trained models deleted. To delete individual TrainedModel you can run the command kubectl delete tm $MODEL_NAME .","title":"Delete Trained Models"},{"location":"modelserving/v1beta1/custom/custom_model/","text":"Deploy Custom Python Model Server with InferenceService \u00b6 When out of the box model server does not fit your need, you can build your own model server using KFServer API and use the following source to serving workflow to deploy your custom models to KServe. Setup \u00b6 Install pack CLI to build your custom model server image. Create your custom Model Server by extending KFModel \u00b6 KServe.KFModel base class mainly defines three handlers preprocess , predict and postprocess , these handlers are executed in sequence, the output of the preprocess is passed to predict as the input, the predictor handler should execute the inference for your model, the postprocess handler then turns the raw prediction result into user-friendly inference response. There is an additional load handler which is used for writing custom code to load your model into the memory from local file system or remote model storage, a general good practice is to call the load handler in the model server class __init__ function, so your model is loaded on startup and ready to serve when user is making the prediction calls. import kserve from typing import Dict class AlexNetModel ( kserve . KFModel ): def __init__ ( self , name : str ): super () . __init__ ( name ) self . name = name self . load () def load ( self ): pass def predict ( self , request : Dict ) -> Dict : pass if __name__ == \"__main__\" : model = AlexNetModel ( \"custom-model\" ) kserve . KFServer () . start ([ model ]) Build the custom image with Buildpacks \u00b6 Buildpacks allows you to transform your inference code into images that can be deployed on KServe without needing to define the Dockerfile . Buildpacks automatically determines the python application and then install the dependencies from the requirements.txt file, it looks at the Procfile to determine how to start the model server. Here we are showing how to build the serving image manually with pack , you can also choose to use kpack to run the image build on the cloud and continuously build/deploy new versions from your source git repository. Use pack to build and push the custom model server image \u00b6 pack build --builder = heroku/buildpacks:20 ${ DOCKER_USER } /custom-model:v1 docker push ${ DOCKER_USER } /custom-model:v1 Parallel Inference \u00b6 By default the model is loaded and inference is ran in the same process as tornado http server, if you are hosting multiple models the inference can only be run for one model at a time which limits the concurrency when you share the container for the models. KServe integrates RayServe which provides a programmable API to deploy models as separate python workers so the inference can be ran in parallel. import kserve from typing import Dict from ray import serve @serve . deployment ( name = \"custom-model\" , config = { \"num_replicas\" : 2 }) class AlexNetModel ( kserve . KFModel ): def __init__ ( self ): self . name = \"custom-model\" super () . __init__ ( self . name ) self . load () def load ( self ): pass def predict ( self , request : Dict ) -> Dict : pass if __name__ == \"__main__\" : kserve . KFServer () . start ({ \"custom-model\" : AlexNetModel }) Modify the Procfile to web: python -m model_remote and then run the above pack command, it builds the serving image which launches each model as separate python worker and tornado webserver routes to the model workers by name. Deploy Locally and Test \u00b6 Launch the docker image built from last step with buildpack . docker run -ePORT = 8080 -p8080:8080 ${ DOCKER_USER } /custom-model:v1 Send a test inference request locally curl localhost:8080/v1/models/custom-model:predict -d @./input.json { \"predictions\" : [[ 14 .861763000488281, 13 .94291877746582, 13 .924378395080566, 12 .182709693908691, 12 .00634765625 ]]} Deploy the Custom Predictor on KServe \u00b6 Create the InferenceService \u00b6 apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : custom-model spec : predictor : containers : - name : kserve-container image : { username } /custom-model:v1 In the custom.yaml file edit the container image and replace {username} with your Docker Hub username. Apply the yaml to create the InferenceService !!! \"kubectl\" kubectl apply -f custom.yaml Expected Output $ inferenceservice.serving.kserve.io/custom-model created Arguments and Environment Variables \u00b6 You can supply additional command arguments on the container spec to configure the model server. --workers : fork the specified number of model server workers(multi-processing), the default value is 1. If you start the server after model is loaded you need to make sure model object is fork friendly for multi-processing to work. Alternatively you can decorate your model server class with replicas and in this case each model server is created as a python worker independent of the server. --http_port : the http port model server is listening on, the default port is 8080 --max_buffer_size : Max socker buffer size for tornado http client, the default limit is 10Mi. --max_asyncio_workers : Max number of workers to spawn for python async io loop, by default it is min(32,cpu.limit + 4) Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=custom-model INPUT_PATH=@./input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d $INPUT_PATH Expected Output * Trying 169.47.250.204... * TCP_NODELAY set * Connected to 169.47.250.204 (169.47.250.204) port 80 (#0) > POST /v1/models/custom-model:predict HTTP/1.1 > Host: custom-model.default.example.com > User-Agent: curl/7.64.1 > Accept: */* > Content-Length: 105339 > Content-Type: application/x-www-form-urlencoded > Expect: 100-continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < content-length: 232 < content-type: text/html; charset=UTF-8 < date: Wed, 26 Feb 2020 15:19:15 GMT < server: istio-envoy < x-envoy-upstream-service-time: 213 < * Connection #0 to host 169.47.250.204 left intact {\"predictions\": [[14.861762046813965, 13.942917823791504, 13.9243803024292, 12.182711601257324, 12.00634765625]]} Delete the InferenceService \u00b6 kubectl delete -f custom.yaml","title":"How to write a custom predictor"},{"location":"modelserving/v1beta1/custom/custom_model/#deploy-custom-python-model-server-with-inferenceservice","text":"When out of the box model server does not fit your need, you can build your own model server using KFServer API and use the following source to serving workflow to deploy your custom models to KServe.","title":"Deploy Custom Python Model Server with InferenceService"},{"location":"modelserving/v1beta1/custom/custom_model/#setup","text":"Install pack CLI to build your custom model server image.","title":"Setup"},{"location":"modelserving/v1beta1/custom/custom_model/#create-your-custom-model-server-by-extending-kfmodel","text":"KServe.KFModel base class mainly defines three handlers preprocess , predict and postprocess , these handlers are executed in sequence, the output of the preprocess is passed to predict as the input, the predictor handler should execute the inference for your model, the postprocess handler then turns the raw prediction result into user-friendly inference response. There is an additional load handler which is used for writing custom code to load your model into the memory from local file system or remote model storage, a general good practice is to call the load handler in the model server class __init__ function, so your model is loaded on startup and ready to serve when user is making the prediction calls. import kserve from typing import Dict class AlexNetModel ( kserve . KFModel ): def __init__ ( self , name : str ): super () . __init__ ( name ) self . name = name self . load () def load ( self ): pass def predict ( self , request : Dict ) -> Dict : pass if __name__ == \"__main__\" : model = AlexNetModel ( \"custom-model\" ) kserve . KFServer () . start ([ model ])","title":"Create your custom Model Server by extending KFModel"},{"location":"modelserving/v1beta1/custom/custom_model/#build-the-custom-image-with-buildpacks","text":"Buildpacks allows you to transform your inference code into images that can be deployed on KServe without needing to define the Dockerfile . Buildpacks automatically determines the python application and then install the dependencies from the requirements.txt file, it looks at the Procfile to determine how to start the model server. Here we are showing how to build the serving image manually with pack , you can also choose to use kpack to run the image build on the cloud and continuously build/deploy new versions from your source git repository.","title":"Build the custom image with Buildpacks"},{"location":"modelserving/v1beta1/custom/custom_model/#use-pack-to-build-and-push-the-custom-model-server-image","text":"pack build --builder = heroku/buildpacks:20 ${ DOCKER_USER } /custom-model:v1 docker push ${ DOCKER_USER } /custom-model:v1","title":"Use pack to build and push the custom model server image"},{"location":"modelserving/v1beta1/custom/custom_model/#parallel-inference","text":"By default the model is loaded and inference is ran in the same process as tornado http server, if you are hosting multiple models the inference can only be run for one model at a time which limits the concurrency when you share the container for the models. KServe integrates RayServe which provides a programmable API to deploy models as separate python workers so the inference can be ran in parallel. import kserve from typing import Dict from ray import serve @serve . deployment ( name = \"custom-model\" , config = { \"num_replicas\" : 2 }) class AlexNetModel ( kserve . KFModel ): def __init__ ( self ): self . name = \"custom-model\" super () . __init__ ( self . name ) self . load () def load ( self ): pass def predict ( self , request : Dict ) -> Dict : pass if __name__ == \"__main__\" : kserve . KFServer () . start ({ \"custom-model\" : AlexNetModel }) Modify the Procfile to web: python -m model_remote and then run the above pack command, it builds the serving image which launches each model as separate python worker and tornado webserver routes to the model workers by name.","title":"Parallel Inference"},{"location":"modelserving/v1beta1/custom/custom_model/#deploy-locally-and-test","text":"Launch the docker image built from last step with buildpack . docker run -ePORT = 8080 -p8080:8080 ${ DOCKER_USER } /custom-model:v1 Send a test inference request locally curl localhost:8080/v1/models/custom-model:predict -d @./input.json { \"predictions\" : [[ 14 .861763000488281, 13 .94291877746582, 13 .924378395080566, 12 .182709693908691, 12 .00634765625 ]]}","title":"Deploy Locally and Test"},{"location":"modelserving/v1beta1/custom/custom_model/#deploy-the-custom-predictor-on-kserve","text":"","title":"Deploy the Custom Predictor on KServe"},{"location":"modelserving/v1beta1/custom/custom_model/#create-the-inferenceservice","text":"apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : custom-model spec : predictor : containers : - name : kserve-container image : { username } /custom-model:v1 In the custom.yaml file edit the container image and replace {username} with your Docker Hub username. Apply the yaml to create the InferenceService !!! \"kubectl\" kubectl apply -f custom.yaml Expected Output $ inferenceservice.serving.kserve.io/custom-model created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/custom/custom_model/#arguments-and-environment-variables","text":"You can supply additional command arguments on the container spec to configure the model server. --workers : fork the specified number of model server workers(multi-processing), the default value is 1. If you start the server after model is loaded you need to make sure model object is fork friendly for multi-processing to work. Alternatively you can decorate your model server class with replicas and in this case each model server is created as a python worker independent of the server. --http_port : the http port model server is listening on, the default port is 8080 --max_buffer_size : Max socker buffer size for tornado http client, the default limit is 10Mi. --max_asyncio_workers : Max number of workers to spawn for python async io loop, by default it is min(32,cpu.limit + 4)","title":"Arguments and Environment Variables"},{"location":"modelserving/v1beta1/custom/custom_model/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=custom-model INPUT_PATH=@./input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict -d $INPUT_PATH Expected Output * Trying 169.47.250.204... * TCP_NODELAY set * Connected to 169.47.250.204 (169.47.250.204) port 80 (#0) > POST /v1/models/custom-model:predict HTTP/1.1 > Host: custom-model.default.example.com > User-Agent: curl/7.64.1 > Accept: */* > Content-Length: 105339 > Content-Type: application/x-www-form-urlencoded > Expect: 100-continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < content-length: 232 < content-type: text/html; charset=UTF-8 < date: Wed, 26 Feb 2020 15:19:15 GMT < server: istio-envoy < x-envoy-upstream-service-time: 213 < * Connection #0 to host 169.47.250.204 left intact {\"predictions\": [[14.861762046813965, 13.942917823791504, 13.9243803024292, 12.182711601257324, 12.00634765625]]}","title":"Run a prediction"},{"location":"modelserving/v1beta1/custom/custom_model/#delete-the-inferenceservice","text":"kubectl delete -f custom.yaml","title":"Delete the InferenceService"},{"location":"modelserving/v1beta1/lightgbm/","text":"Deploy Lightgbm model with InferenceService \u00b6 Creating your own model and testing the LightGBM server. \u00b6 To test the LightGBM Server, first we need to generate a simple LightGBM model using Python. import lightgbm as lgb from sklearn.datasets import load_iris import os model_dir = \".\" BST_FILE = \"model.bst\" iris = load_iris () y = iris [ 'target' ] X = iris [ 'data' ] dtrain = lgb . Dataset ( X , label = y ) params = { 'objective' : 'multiclass' , 'metric' : 'softmax' , 'num_class' : 3 } lgb_model = lgb . train ( params = params , train_set = dtrain ) model_file = os . path . join ( model_dir , BST_FILE ) lgb_model . save_model ( model_file ) Then, we can install and run the LightGBM Server using the generated model and test for prediction. Models can be on local filesystem, S3 compatible object storage, Azure Blob Storage, or Google Cloud Storage. python -m lgbserver --model_dir /path/to/model_dir --model_name lgb We can also do some simple predictions import requests request = { 'sepal_width_(cm)' : { 0 : 3.5 }, 'petal_length_(cm)' : { 0 : 1.4 }, 'petal_width_(cm)' : { 0 : 0.2 }, 'sepal_length_(cm)' : { 0 : 5.1 } } formData = { 'inputs' : [ request ] } res = requests . post ( 'http://localhost:8080/v1/models/lgb:predict' , json = formData ) print ( res ) print ( res . text ) Create the InferenceService \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"lightgbm-iris\" spec : predictor : lightgbm : storageUri : \"gs://kfserving-examples/models/lightgbm/iris\" Apply the above yaml to create the InferenceService kubectl apply -f lightgbm.yaml Expected Output $ inferenceservice.serving.kserve.io/lightgbm-iris created Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=lightgbm-iris INPUT_PATH=@./iris-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice lightgbm-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * Trying 169.63.251.68... * TCP_NODELAY set * Connected to 169.63.251.68 (169.63.251.68) port 80 (#0) > POST /models/lightgbm-iris:predict HTTP/1.1 > Host: lightgbm-iris.default.svc.cluster.local > User-Agent: curl/7.60.0 > Accept: */* > Content-Length: 76 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 76 out of 76 bytes < HTTP/1.1 200 OK < content-length: 27 < content-type: application/json; charset=UTF-8 < date: Tue, 21 May 2019 22:40:09 GMT < server: istio-envoy < x-envoy-upstream-service-time: 13032 < * Connection #0 to host 169.63.251.68 left intact {\"predictions\": [[0.9, 0.05, 0.05]]} Run LightGBM InferenceService with your own image \u00b6 Since the KServe LightGBM image is built from a specific version of lightgbm pip package, sometimes it might not be compatible with the pickled model you saved from your training environment, however you can build your own lgbserver image following this instruction . To use your lgbserver image: - Add the image to the KServe configmap \"lightgbm\" : { \"image\" : \"<your-dockerhub-id>/kserve/lgbserver\" , } , - Specify the runtimeVersion on InferenceService spec apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"lightgbm-iris\" spec : predictor : lightgbm : storageUri : \"gs://kfserving-examples/models/lightgbm/iris\" runtimeVersion : X.X.X","title":"Lightgbm"},{"location":"modelserving/v1beta1/lightgbm/#deploy-lightgbm-model-with-inferenceservice","text":"","title":"Deploy Lightgbm model with InferenceService"},{"location":"modelserving/v1beta1/lightgbm/#creating-your-own-model-and-testing-the-lightgbm-server","text":"To test the LightGBM Server, first we need to generate a simple LightGBM model using Python. import lightgbm as lgb from sklearn.datasets import load_iris import os model_dir = \".\" BST_FILE = \"model.bst\" iris = load_iris () y = iris [ 'target' ] X = iris [ 'data' ] dtrain = lgb . Dataset ( X , label = y ) params = { 'objective' : 'multiclass' , 'metric' : 'softmax' , 'num_class' : 3 } lgb_model = lgb . train ( params = params , train_set = dtrain ) model_file = os . path . join ( model_dir , BST_FILE ) lgb_model . save_model ( model_file ) Then, we can install and run the LightGBM Server using the generated model and test for prediction. Models can be on local filesystem, S3 compatible object storage, Azure Blob Storage, or Google Cloud Storage. python -m lgbserver --model_dir /path/to/model_dir --model_name lgb We can also do some simple predictions import requests request = { 'sepal_width_(cm)' : { 0 : 3.5 }, 'petal_length_(cm)' : { 0 : 1.4 }, 'petal_width_(cm)' : { 0 : 0.2 }, 'sepal_length_(cm)' : { 0 : 5.1 } } formData = { 'inputs' : [ request ] } res = requests . post ( 'http://localhost:8080/v1/models/lgb:predict' , json = formData ) print ( res ) print ( res . text )","title":"Creating your own model and testing the LightGBM server."},{"location":"modelserving/v1beta1/lightgbm/#create-the-inferenceservice","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"lightgbm-iris\" spec : predictor : lightgbm : storageUri : \"gs://kfserving-examples/models/lightgbm/iris\" Apply the above yaml to create the InferenceService kubectl apply -f lightgbm.yaml Expected Output $ inferenceservice.serving.kserve.io/lightgbm-iris created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/lightgbm/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=lightgbm-iris INPUT_PATH=@./iris-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice lightgbm-iris -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * Trying 169.63.251.68... * TCP_NODELAY set * Connected to 169.63.251.68 (169.63.251.68) port 80 (#0) > POST /models/lightgbm-iris:predict HTTP/1.1 > Host: lightgbm-iris.default.svc.cluster.local > User-Agent: curl/7.60.0 > Accept: */* > Content-Length: 76 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 76 out of 76 bytes < HTTP/1.1 200 OK < content-length: 27 < content-type: application/json; charset=UTF-8 < date: Tue, 21 May 2019 22:40:09 GMT < server: istio-envoy < x-envoy-upstream-service-time: 13032 < * Connection #0 to host 169.63.251.68 left intact {\"predictions\": [[0.9, 0.05, 0.05]]}","title":"Run a prediction"},{"location":"modelserving/v1beta1/lightgbm/#run-lightgbm-inferenceservice-with-your-own-image","text":"Since the KServe LightGBM image is built from a specific version of lightgbm pip package, sometimes it might not be compatible with the pickled model you saved from your training environment, however you can build your own lgbserver image following this instruction . To use your lgbserver image: - Add the image to the KServe configmap \"lightgbm\" : { \"image\" : \"<your-dockerhub-id>/kserve/lgbserver\" , } , - Specify the runtimeVersion on InferenceService spec apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"lightgbm-iris\" spec : predictor : lightgbm : storageUri : \"gs://kfserving-examples/models/lightgbm/iris\" runtimeVersion : X.X.X","title":"Run LightGBM InferenceService with your own image"},{"location":"modelserving/v1beta1/paddle/","text":"Deploy paddle model with InferenceService \u00b6 In this example, we use a trained paddle resnet50 model to classify images by running an inference service with Paddle predictor. Create the InferenceService \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"paddle-resnet50\" spec : predictor : paddle : storageUri : \"https://zhouti-mcp-edge.cdn.bcebos.com/resnet50.tar.gz\" Apply the above yaml to create the InferenceService kubectl apply -f paddle.yaml Expected Output inferenceservice.serving.kserve.io/paddle-resnet50 created Run a Prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = paddle-resnet50 SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./jay.json Expected Output * Trying 127.0.0.1:80... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 80 (#0) > POST /v1/models/paddle-resnet50:predict HTTP/1.1 > Host: paddle-resnet50.default.example.com > User-Agent: curl/7.68.0 > Accept: */* > Content-Length: 3010209 > Content-Type: application/x-www-form-urlencoded > Expect: 100 -continue > * Mark bundle as not supporting multiuse < HTTP/1.1 100 Continue * We are completely uploaded and fine * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 23399 < content-type: application/json; charset=UTF-8 < date: Mon, 17 May 2021 03:34:58 GMT < server: istio-envoy < x-envoy-upstream-service-time: 511 < {\"predictions\": [[6.736678770380422e-09, 1.1535990829258935e-08, 5.142250714129659e-08, 6.647170636142619e-08, 4.094492567219277e-08, 1.3402451770616608e-07, 9.355561303436843e-08, 2.8935891904779965e-08, 6.845367295227334e-08, 7.680615965455218e-08, 2.0334689452283783e-06, 1.1085678579547675e-06, 2.3477592492326949e-07, 6.582037030966603e-07, 0.00012373103527352214, 4.2878804151769145e-07, 6.419959845516132e-06, 0.9993496537208557, 7.372002437477931e-05, 3.101135735050775e-05, 5.6028093240456656e-06, 2.1862508674530545e-06, 1.9544044604913324e-08, 3.728893887000595e-07, 4.2903633357127546e-07, 1.8251179767503345e-07, 7.159925985433802e-08, 9.231618136595898e-09, 6.469241498052725e-07, 7.031690341108288e-09, 4.451231561120039e-08, 1.2455971898361895e-07, 9.44632745358831e-08, 4.347704418705689e-08, 4.658220120745682e-07, 6.797721141538204e-08, 2.1060276367279585e-07, 2.2605123106700376e-08, 1.4311490303953178e-07, 7.951298641728499e-08, 1.2341783417468832e-07, 1.0921713737843675e-06, 1.5243892448779661e-05, 3.1173343018053856e-07, 2.4152058131221565e-07, 6.863762536113427e-08, 8.467682022228473e-08, 9.4246772164297e-08, 1.0219210366813058e-08, 3.3770753304906975e-08, 3.6928835100979995e-08, 1.3694031508748594e-07, 1.0674284567357972e-07, 2.599483650556067e-07, 3.4866405940192635e-07, 3.132053549848024e-08, 3.574873232992104e-07, 6.64843895492595e-08, 3.1638955988455564e-07, 1.2095878219042788e-06, 8.66409024524728e-08, 4.0144172430700564e-08, 1.2544761318622477e-07, 3.3201178695208e-08, 1.9731444922399533e-07, 3.806405572959193e-07, 1.3827865075199952e-07, 2.300225965257141e-08, 7.14422512260171e-08, 2.851114544455413e-08, 2.982567437470607e-08, 8.936032713791064e-08, 6.22388370175031e-07, 6.478838798784636e-08, 1.3663023423760023e-07, 9.973181391842445e-08, 2.5761554667269593e-08, 4.130220077058766e-08, 3.9384463690339544e-08, 1.2158079698565416e-07, 4.302821707824478e-06, 1.8179063090428826e-06, 1.8520155435908237e-06, 1.6246107179540559e-06, 1.6448313544970006e-05, 1.0544916221988387e-05, 3.993061909568496e-06, 2.646479799750523e-07, 1.9193475964129902e-05, 4.803242745765601e-07, 1.696285067964709e-07, 4.550505764200352e-06, 4.235929372953251e-05, 4.443338639248395e-06, 5.104009687784128e-06, 1.3506396498996764e-05, 4.1758724478313525e-07, 4.494491463447048e-07, 3.156698369366495e-07, 1.0557599807725637e-06, 1.336463917311903e-08, 1.3893659556174498e-08, 6.770379457066156e-08, 1.4129696523923485e-07, 7.170518756538513e-08, 7.934466594861078e-08, 2.639154317307657e-08, 2.6134321373660896e-08, 7.196725881897237e-09, 2.1752363466021052e-08, 6.684639686227456e-08, 3.417795824134373e-08, 1.6228275967478112e-07, 4.107114648377319e-07, 6.472135396506928e-07, 2.951379372007068e-07, 5.653474133282543e-09, 4.830144462175667e-08, 8.887481861563629e-09, 3.7306168820805397e-08, 1.7784264727538357e-08, 4.641905082536368e-09, 3.413118676576232e-08, 1.937393818707278e-07, 1.2980176506971475e-06, 3.5641004814124244e-08, 2.149332445355867e-08, 3.055293689158134e-07, 1.5532516783878236e-07, 1.4520978766086046e-06, 3.488464628276233e-08, 3.825438398052938e-05, 4.5088432898410247e-07, 4.1766969616219285e-07, 6.770622462681786e-07, 1.4142248971893423e-07, 1.4235997696232516e-05, 6.293820433711517e-07, 4.762866865348769e-06, 9.024900577969674e-07, 9.058987870957935e-07, 1.5713684433649178e-06, 1.5720647184025438e-07, 1.818536503606083e-07, 7.193188622522939e-08, 1.1952824934269302e-06, 8.874837362782273e-07, 2.0870831463071227e-07, 9.906239029078279e-08, 7.793621747964607e-09, 1.0058498389753368e-07, 4.2059440374941914e-07, 1.843624630737395e-07, 1.6437947181202617e-07, 7.025352743994517e-08, 2.570448600636155e-07, 7.586877615040066e-08, 7.841313731660193e-07, 2.495309274763713e-07, 5.157681925993529e-08, 4.0674127177453556e-08, 7.531796519799627e-09, 4.797485431140558e-08, 1.7419973019627832e-08, 1.7958679165985814e-07, 1.2566392371127222e-08, 8.975440124459055e-08, 3.26965476915575e-08, 1.1208359751435637e-07, 3.906746215420753e-08, 4.6769045525252295e-08, 1.8523553535487736e-07, 1.4833052830454108e-07, 1.2279349448363064e-07, 1.0729105497375713e-06, 3.6538490011395197e-09, 1.6198403329781286e-07, 1.6190719875908144e-08, 1.2004933580556099e-07, 1.4800277448046018e-08, 4.02294837442696e-08, 2.15060893538066e-07, 1.1925696696835075e-07, 4.8982514044837444e-08, 7.608920071788816e-08, 2.3137479487900237e-08, 8.521050176568679e-08, 9.586213423062873e-08, 1.3351650807180704e-07, 3.021699157557123e-08, 4.423876376336011e-08, 2.610667060309879e-08, 2.3977091245797055e-07, 1.3192564551900432e-07, 1.6734931662654162e-08, 1.588336999702733e-07, 4.0643516285854275e-07, 8.753454494581092e-08, 8.366999395548191e-07, 3.437598650180007e-08, 7.847892646850596e-08, 8.526394701391382e-09, 9.601382799928615e-08, 5.258924034023948e-07, 1.3557448141909845e-07, 1.0307226716577134e-07, 1.0429813457335513e-08, 5.187714435805901e-08, 2.187001335585137e-08, 1.1791439824548888e-08, 2.98065643278278e-08, 4.338393466696289e-08, 2.9991046091026874e-08, 2.8507610494443725e-08, 3.058665143385042e-08, 6.441099031917474e-08, 1.5364101102477434e-08, 1.5973883549236234e-08, 2.5736850872704053e-08, 1.0903765712555469e-07, 3.2118737891551064e-08, 6.819742992547617e-09, 1.9251311300649832e-07, 5.8258109447706374e-08, 1.8765761922168167e-07, 4.0070790419122204e-07, 1.5791577823165426e-08, 1.950158434738114e-07, 1.0142063189277906e-08, 2.744815041921811e-08, 1.2843531571604672e-08, 3.7297493094001766e-08, 7.407496838141014e-08, 4.20607833007125e-08, 1.6924804668860816e-08, 1.459203531339881e-07, 4.344977000414474e-08, 1.7191403856031684e-07, 3.5817443233554513e-08, 8.440249388286247e-09, 4.194829728021432e-08, 2.514032360068086e-08, 2.8340199520471288e-08, 8.747196034164517e-08, 8.277125651545703e-09, 1.1676293709683705e-08, 1.4548514570833504e-07, 7.200282148289716e-09, 2.623600948936655e-06, 5.675736929333652e-07, 1.9483527466945816e-06, 6.752595282932816e-08, 8.168475318370838e-08, 1.0933046468153407e-07, 1.670913718498923e-07, 3.1387276777650186e-08, 2.973524537708272e-08, 5.752163900751839e-08, 5.850877471402782e-08, 3.2544622285968217e-07, 3.330221431951941e-08, 4.186786668469722e-07, 1.5085906568401697e-07, 2.3346819943981245e-07, 2.86402780602657e-07, 2.2940319865938363e-07, 1.8537603807544656e-07, 3.151798182443599e-07, 1.1075967449869495e-06, 1.5369782602192572e-07, 1.9237509718550427e-07, 1.64044664074936e-07, 2.900835340824415e-07, 1.246654903752642e-07, 5.802622027317739e-08, 5.186220519703966e-08, 6.0094205167615655e-09, 1.2333241272699524e-07, 1.3798474185477971e-07, 1.7370231830682314e-07, 5.617761189569137e-07, 5.1604470030497396e-08, 4.813277598714194e-08, 8.032698417537176e-08, 2.0645263703045202e-06, 5.638597713186755e-07, 8.794199857220519e-07, 3.4785980460583232e-06, 2.972389268052211e-07, 3.3904532870110415e-07, 9.469074058188198e-08, 3.754845678827223e-08, 1.5679037801419327e-07, 8.203105039683578e-08, 6.847962641387539e-09, 1.8251624211984563e-08, 6.050240841659615e-08, 3.956342808919544e-08, 1.0699947949888156e-07, 3.2566634899922065e-07, 3.5369430406717584e-07, 7.326295303755614e-08, 4.85765610847011e-07, 7.717713401689252e-07, 3.4567779749750116e-08, 3.246204585138912e-07, 3.1608601602783892e-06, 5.33099466792919e-08, 3.645687343123427e-07, 5.48158936908294e-07, 4.62306957160763e-08, 1.3466177506415988e-07, 4.3529482240955986e-08, 1.6404105451783835e-07, 2.463695381038633e-08, 5.958712634424046e-08, 9.493651020875404e-08, 5.523462576206839e-08, 5.7412357534758485e-08, 1.1850350347231142e-05, 5.8263944993086625e-06, 7.4208674050169066e-06, 9.127966222877149e-07, 2.0019581370434025e-06, 1.033498961078294e-06, 3.5146850763112525e-08, 2.058995278275688e-06, 3.5655509122989315e-07, 6.873234070781109e-08, 2.1935298022413008e-09, 5.560363547374436e-08, 3.3266996979364194e-07, 1.307369217329324e-07, 2.718762992515167e-08, 1.0462929189714032e-08, 7.466680358447775e-07, 6.923166040451179e-08, 1.6145664361033596e-08, 8.568521003837759e-09, 4.76221018175238e-09, 1.233977116044116e-07, 8.340628632197422e-09, 3.2649041248333788e-09, 5.0632489312363305e-09, 4.0704994930251814e-09, 1.2043538610839732e-08, 5.105608380517879e-09, 7.267142887457112e-09, 1.184516307262129e-07, 7.53557927168913e-08, 6.386964201965384e-08, 1.6212936770898523e-08, 2.610429419291904e-07, 6.979425393183192e-07, 6.647513117741255e-08, 7.717492849224072e-07, 6.651206945207377e-07, 3.324495310152997e-07, 3.707282019149716e-07, 3.99564243025452e-07, 6.411632114122767e-08, 7.107352217872176e-08, 1.6380016631956096e-07, 6.876800995314625e-08, 3.462474467141874e-07, 2.0256503319160402e-07, 6.19610148078209e-07, 2.6841073363925716e-08, 6.720335363752383e-07, 1.1348340649419697e-06, 1.8397931853542104e-06, 6.397251581802266e-07, 7.257533241045167e-08, 4.2213909523525217e-07, 3.9657925299252383e-07, 1.4037439655112394e-07, 3.249856774800719e-07, 1.5857655455420172e-07, 1.1122217102865761e-07, 7.391420808744442e-08, 3.42322238111592e-07, 5.39796154441774e-08, 8.517296379295658e-08, 4.061009803990601e-06, 1.4478755474556237e-05, 7.317032757470088e-09, 6.9484960008026064e-09, 4.468917325084476e-08, 9.23141172393116e-08, 5.411982328951126e-08, 2.2242811326123046e-07, 1.7609554703312824e-08, 2.0906279374344194e-08, 3.6797682678724186e-09, 6.177919686933819e-08, 1.7920288541972695e-07, 2.6279179721200308e-08, 2.6988200119149042e-08, 1.6432807115052128e-07, 1.2827612749788386e-07, 4.468908798571647e-08, 6.316552969565237e-08, 1.9461760203398626e-08, 2.087125849925542e-08, 2.2414580413965268e-08, 2.4765244077684656e-08, 6.785398465325443e-09, 2.4248794971981624e-08, 4.554979504689527e-09, 2.8977037658250993e-08, 2.0402325162649504e-08, 1.600950270130852e-07, 2.0199709638291097e-07, 1.611188515937556e-08, 5.964113825029926e-08, 4.098318573397819e-09, 3.9080127578472457e-08, 7.511338218080255e-09, 5.965624154669058e-07, 1.6478223585636442e-07, 1.4106989354445432e-08, 3.2855584919389e-08, 3.3387166364917675e-09, 1.220043444050134e-08, 4.624639160510924e-08, 6.842309385746148e-09, 1.74262879681919e-08, 4.6611329906909305e-08, 9.331947836699328e-08, 1.2306078644996887e-07, 1.2359445022980253e-08, 1.1173199254699284e-08, 2.7724862405875683e-08, 2.419210147763806e-07, 3.451186785241589e-07, 2.593766978975509e-08, 9.964568192799561e-08, 9.797809674694236e-09, 1.9085564417764544e-07, 3.972706252852731e-08, 2.6639204619982593e-08, 6.874148805735558e-09, 3.146993776681484e-08, 2.4086594407890516e-07, 1.3126927456141857e-07, 2.1254339799270383e-07, 2.050203384840188e-08, 3.694976058454813e-08, 6.563175816154398e-07, 2.560050127442537e-08, 2.6882981174480847e-08, 6.880636078676616e-07, 2.0092733166166e-07, 2.788039665801989e-08, 2.628409134786125e-08, 5.1678345158734373e-08, 1.8935413947929192e-07, 4.61852835087484e-07, 1.1086777718105623e-08, 1.4542604276357451e-07, 2.8737009216683873e-08, 6.105167926762078e-07, 1.2016463379893594e-08, 1.3944705301582871e-07, 2.093712758721722e-08, 4.3801410498645055e-08, 1.966320795077081e-08, 6.654448991838535e-09, 1.1149590584125235e-08, 6.424939158478082e-08, 6.971554888934861e-09, 3.260019587614238e-09, 1.4260189473702667e-08, 2.7895078247297533e-08, 8.11578289017234e-08, 2.5995715802196173e-08, 2.2855578762914774e-08, 1.055962854934478e-07, 8.145542551574181e-08, 3.7793686402665116e-08, 4.881891513264236e-08, 2.342062366267328e-08, 1.059935517133681e-08, 3.604105103249822e-08, 5.062430830093945e-08, 3.6804440384230475e-08, 1.501580193519203e-09, 1.4475033367489232e-06, 1.076210423889279e-06, 1.304991315009829e-07, 3.073601462233455e-08, 1.7184021317007137e-08, 2.0421090596300928e-08, 7.904992216367646e-09, 1.6902052379919041e-07, 1.2416506933732308e-08, 5.4758292122869534e-08, 2.6250422280327257e-08, 1.3261367115546818e-08, 6.29807459517906e-08, 1.270998595259698e-08, 2.0171681569536304e-07, 4.386637186826192e-08, 6.962349630157405e-08, 2.9565120485131047e-07, 7.925131626507209e-07, 2.0868920103112032e-07, 1.7341794489311724e-07, 4.2942417621816276e-08, 4.213406956665722e-09, 8.824785169281313e-08, 1.7341569957807224e-08, 7.321587247588468e-08, 1.7941774288487977e-08, 1.1245148101579616e-07, 4.242405395871174e-07, 8.259573469615589e-09, 1.1336403105133286e-07, 8.268798978861014e-08, 2.2186977588489754e-08, 1.9539720952366224e-08, 1.0675703876472653e-08, 3.288517547161973e-08, 2.4340963022950746e-08, 6.639137239972115e-08, 5.604687380866835e-09, 1.386604697728444e-08, 6.675873720496384e-08, 1.1355886009312144e-08, 3.132159633878473e-07, 3.12451788886392e-08, 1.502181845580708e-07, 1.3461754377885882e-08, 1.8882955998833495e-07, 4.645742279762999e-08, 4.6453880742092224e-08, 7.714453964524637e-09, 3.5857155467056145e-08, 7.60832108426257e-09, 4.221501370693659e-08, 4.3407251126836854e-09, 1.340157496088068e-08, 8.565600495558101e-08, 1.7045413969185574e-08, 5.4221903411644234e-08, 3.021912675649219e-08, 6.153376119755194e-08, 3.938857240370908e-09, 4.135628017820636e-08, 1.781920389021252e-08, 4.3105885083605244e-08, 3.903354972578654e-09, 7.663085455078544e-08, 1.1890405993142394e-08, 9.304217840622186e-09, 1.0968062014171664e-09, 1.0536767902635802e-08, 1.1516804221400889e-07, 8.134522886393825e-07, 5.952623993721318e-08, 2.806350174466843e-08, 1.2833099027886874e-08, 1.0605690192733164e-07, 7.872949936427176e-07, 2.7501393162765453e-08, 3.936289072470345e-09, 2.0519442145428002e-08, 7.394815870753746e-09, 3.598397313453461e-08, 2.5378517065632877e-08, 4.698972233541099e-08, 7.54952989012736e-09, 6.322805461422831e-07, 5.582006412652163e-09, 1.29640980617296e-07, 1.5874988434916304e-08, 3.3837810775594335e-08, 6.474512037613067e-09, 9.121148281110436e-08, 1.3918511676536127e-08, 8.230025549949005e-09, 2.7061290097663004e-08, 2.6095918315149902e-08, 5.722363471960534e-09, 6.963475698285038e-07, 4.685091781198025e-08, 9.590579885809802e-09, 2.099205858030473e-07, 3.082160660028421e-08, 3.563162565001221e-08, 7.326312925215461e-07, 2.1759731225756695e-06, 2.407518309155421e-07, 2.974515780351794e-07, 2.529018416908002e-08, 7.667950718825978e-09, 2.663289251358947e-07, 3.4358880185436647e-08, 2.3130198201215535e-08, 3.1239693498719134e-08, 2.8691621878351725e-07, 3.895845068768722e-08, 2.4184130253956937e-08, 1.1582445225144511e-08, 5.1545349322168477e-08, 2.034345492063494e-08, 8.201963197507212e-08, 1.164153573540716e-08, 5.496356720868789e-07, 1.1682151246361627e-08, 4.7576914852243135e-08, 1.6349824605299546e-08, 4.090862759653646e-08, 2.1271189609706198e-07, 1.6697286753242224e-07, 3.989708119433999e-08, 2.852450279533514e-06, 1.2500372292834072e-07, 2.4846613655427063e-07, 1.245429093188477e-08, 2.9700272463628608e-08, 4.250991558762962e-09, 1.61443480806156e-07, 2.6386018703306036e-07, 7.638056409575711e-09, 3.4455793773702226e-09, 7.273289526210647e-08, 1.7631434090503717e-08, 7.58661311550668e-09, 2.1547013062672704e-08, 1.2675349125856883e-07, 2.5637149292379036e-08, 3.500976220038865e-08, 6.472243541111311e-08, 8.387915251262257e-09, 3.069512288789156e-08, 7.520387867998579e-08, 1.5724964441687916e-07, 1.9634005354873807e-07, 1.2290831818972947e-07, 1.112118730439704e-09, 1.546895944670723e-08, 9.91701032404535e-09, 6.882473257974198e-07, 8.267616635748709e-08, 4.469531234008173e-08, 2.075201344098332e-08, 8.649378457903367e-08, 5.202766573120243e-08, 4.5564942041664835e-08, 2.0319955496006514e-08, 8.705182352741758e-09, 6.452066969586667e-08, 2.1777438519166026e-08, 1.030954166481024e-08, 3.211904342492744e-08, 2.3336936294526822e-07, 8.054096056753224e-09, 1.9623354319264763e-07, 1.2888089884199871e-07, 1.5392496166555247e-08, 1.401903038100727e-09, 5.696818305978013e-08, 6.080025372057207e-09, 1.0782793324892737e-08, 2.4260730313585555e-08, 1.9388659566743627e-08, 2.2970310453729326e-07, 1.9971754028347277e-08, 2.8477993296860404e-08, 5.2273552597625894e-08, 2.7392806600801123e-07, 9.857291161097237e-08, 3.12910977129377e-08, 4.151442212219081e-08, 5.251196366629074e-09, 1.580681100676884e-06, 8.547603442821128e-07, 1.068913135782168e-08, 1.0621830597301596e-06, 7.737313012512459e-08, 6.394216711669287e-08, 1.1698345758759388e-07, 1.0486609625104393e-07, 2.1161000063329993e-07, 1.53396815250062e-08, 5.094453570109181e-08, 1.4005379966874898e-08, 2.6282036102998063e-08, 8.778433624456738e-08, 7.772066545896905e-09, 4.228875383205377e-08, 3.3243779284930497e-07, 7.729244799747903e-08, 7.636901111496286e-10, 5.989500806435899e-08, 1.326090597331131e-07, 1.2853634245857393e-07, 8.844242671557367e-09, 1.0194374766570036e-07, 2.493779334145074e-07, 1.6547971881664125e-07, 1.1762754326127833e-08, 1.1496195639892903e-07, 2.9342709240154363e-07, 1.326124099421122e-08, 8.630262726683213e-08, 5.7394842656322e-08, 1.1094081031615133e-07, 2.2933713239581266e-07, 3.4706170026765903e-07, 1.4751107357824367e-07, 1.502495017291494e-08, 6.454319390059027e-08, 5.164533689594464e-08, 6.23741556182722e-08, 1.293601457064142e-07, 1.4052071506398534e-08, 5.386946000385251e-08, 2.0827554791935654e-08, 1.3040637902861363e-08, 1.0578981601838677e-07, 1.5079727688771527e-08, 8.92632726845477e-07, 4.6374381668101705e-08, 7.481006036869076e-07, 5.883147302654379e-09, 2.8707685117979054e-09, 8.381598490814213e-07, 7.341958596640552e-09, 1.4245998158912698e-08, 1.0926417104428765e-07, 1.1308178216040687e-07, 2.52339901862797e-07, 1.1782835684925885e-07, 4.6678056975224536e-08, 2.7959197179683315e-09, 3.4363861090014325e-08, 1.4674496640054713e-07, 3.5396915620822256e-08, 2.0581127557761647e-07, 7.18387909159901e-08, 2.7693943138729082e-08, 4.5493386835460115e-08, 1.9559182717898693e-08, 1.5359708172013598e-08, 1.2336623278486059e-08, 2.9570605519779747e-08, 2.877552560676122e-07, 9.051845495378075e-07, 2.3732602016934834e-07, 1.6521676471370483e-08, 1.5478875070584763e-08, 3.526786329643983e-08, 3.616410637619083e-08, 1.61590953950963e-08, 7.65007328595857e-08, 1.9661483108279754e-08, 4.917534823789538e-08, 1.1712612746350715e-07, 1.0889253054813253e-08, 1.494120169809321e-06, 1.018585660261806e-08, 3.7575969003000864e-08, 2.097097784314883e-08, 3.368558054717141e-08, 4.845588819080149e-09, 6.039624622644624e-07, 1.037331109898787e-08, 2.841650257323636e-07, 4.4990630954089283e-07, 3.463186004637464e-08, 7.720684180867465e-08, 1.471122175189521e-07, 1.1601575522490748e-07, 4.007488030310924e-07, 3.025649775167949e-08, 6.706784461130155e-08, 2.0128741340386114e-08, 1.5987744461654074e-09, 4.1919822280078733e-08, 1.3167154477855547e-08, 3.231814815762846e-08, 9.247659704669786e-08, 1.3075300842047e-07, 1.0574301256838226e-07, 3.762165334819656e-08, 1.0942246575496029e-07, 7.001474955359299e-08, 2.742706151082075e-08, 2.0766625752344225e-08, 4.5403403703403455e-08, 3.39040298058535e-08, 1.0469661759771043e-07, 2.8271578855765256e-08, 3.406226767310727e-07, 5.146206945028098e-07, 6.740708613506285e-07, 6.382248063374618e-09, 3.63878704945364e-08, 3.626059807970705e-08, 1.6065602892467723e-07, 3.639055989879125e-07, 6.232691696084203e-09, 4.805490050330263e-08, 3.372633727849461e-08, 6.328880317596486e-07, 6.480631498106959e-08, 2.1165197949812864e-07, 8.38779143919055e-08, 1.7589144363228115e-08, 2.729027670511641e-09, 2.144795097080987e-08, 7.861271456022223e-08, 2.0118186228046397e-08, 2.8407685093156942e-08, 2.4922530883486615e-07, 2.0156670998972004e-08, 2.6551649767725394e-08, 2.7848242822869906e-08, 6.907123761834555e-09, 1.880543720744754e-08, 1.3006903998302732e-08, 3.685918272822164e-07, 3.967941211158177e-07, 2.7592133022835696e-08, 2.5228947819755376e-08, 1.547002881352455e-07, 3.689306637966183e-08, 1.440177199718562e-09, 2.1504929392790473e-08, 5.068111263994979e-08, 5.081711407228795e-08, 1.171875219085905e-08, 5.409278358570191e-08, 7.138276600926474e-07, 2.5237213208129106e-07, 7.072044638789521e-08, 7.199763984999663e-08, 1.2525473103153217e-08, 3.4803417747752974e-07, 1.9591827538079087e-07, 1.2404700555634918e-07, 1.234617457157583e-07, 1.9201337408958352e-08, 1.9895249181445251e-07, 3.7876677794201896e-08, 1.0629785052174157e-08, 1.2437127772102485e-08, 2.1861892207653e-07, 2.6181456291851646e-07, 1.112900775979142e-07, 1.0776630432474121e-07, 6.380325157095967e-09, 3.895085143312826e-09, 1.5762756788717525e-07, 2.909027019271093e-09, 1.0381050685737137e-08, 2.8135211493918177e-08, 1.0778002490496874e-08, 1.3605974125141529e-08, 2.9236465692861202e-08, 1.9189795352758665e-07, 2.199506354827463e-07, 1.326399790002597e-08, 4.9004846403022384e-08, 2.980837132682268e-09, 8.926045680368588e-09, 1.0996975774446582e-08, 7.71560149104289e-09, 7.454491246505768e-09, 5.086162246925596e-08, 1.5129764108223753e-07, 1.1960075596562092e-08, 1.1323334270230134e-08, 9.391332156383214e-09, 9.585701832293125e-08, 1.905532798218701e-08, 1.8105303922766325e-08, 6.179227796110354e-08, 6.389401363549041e-08, 1.1853179771037503e-08, 9.37277544466042e-09, 1.2332148457971925e-07, 1.6522022860954166e-08, 1.246116454467483e-07, 4.196171854431441e-09, 3.996593278543514e-08, 1.2554556505506298e-08, 1.4302138140465104e-08, 6.631793780798034e-09, 5.964224669696705e-09, 5.556936244488497e-09, 1.4192455921602232e-07, 1.7613080771639034e-08, 3.380189639301534e-07, 7.85651934620546e-08, 2.966783085867064e-08, 2.8992105853831163e-06, 1.3787366697215475e-06, 5.313622430946907e-09, 2.512852859126724e-08, 8.406627216572815e-08, 4.492839167369311e-08, 5.408793057881667e-08, 2.4239175999696272e-08, 4.016805235096399e-07, 4.1083545454512205e-08, 5.4153481698904216e-08, 8.640767212853007e-09, 5.773256717134245e-08, 2.6443152023603034e-07, 8.953217047746875e-07, 2.7994001783326894e-08, 5.889480014786841e-09, 4.1788819515886644e-08, 2.8880645430717777e-08, 2.135752907861388e-08, 2.3024175277441827e-07, 8.786625471657317e-08, 2.0697297209437693e-09, 2.236410523437371e-08, 3.203276310870251e-09, 1.176874686592555e-08, 6.963571053120177e-08, 2.271932153519174e-08, 7.360382525689602e-09, 6.922528772435044e-09, 3.213871480056696e-08, 1.370577820125618e-07, 1.9815049157045905e-08, 1.0578956377571558e-08, 2.7049420481262132e-08, 2.9755937713815683e-09, 2.1773699288019088e-08, 1.09755387001087e-08, 1.991872444762066e-08, 2.3882098076910552e-08, 2.1357365653784655e-08, 6.109098560358461e-09, 1.1890497475519624e-08, 1.1459891702259029e-08, 3.73173456580389e-08, 1.572620256240498e-08, 3.404023374287135e-08, 3.6921580459647885e-08, 9.281765045443535e-08, 1.2323201303843234e-07, 4.2347593876002065e-08, 1.7423728237986325e-08, 5.8113389656000436e-08, 3.931436154402945e-08, 2.3690461148362374e-08, 1.792850135018398e-08, 1.440664210150544e-08, 7.019830494670032e-09, 6.041522482291839e-08, 4.867479930226182e-08, 1.0685319296044327e-08, 1.0051243393149889e-08, 4.2426261614991745e-08, 2.607815297039906e-08, 5.136670200300* Connection #0 to host localhost left intact 841e-09, 1.69729952315123e-09, 1.9131586981302462e-08, 2.111743526711507e-07, 1.337269672774255e-08, 2.0002481448955223e-08, 1.0454256482717028e-07, 2.8144228281234973e-08, 2.1344791889532644e-07, 2.1046110632028103e-08, 1.9114453664315079e-07, 3.957693550660224e-08, 2.931631826186276e-08, 1.105203111251285e-07, 4.84007678380749e-08, 5.583606110803885e-08, 1.2130111315400427e-07, 1.77621615193857e-08, 2.5610853882085394e-08, 1.203865309662433e-07, 4.674859610531712e-09, 1.5916098661250544e-08, 3.147594185293201e-08, 6.147686093527227e-08, 2.204641802450169e-08, 3.257763410147163e-07, 1.198914532096751e-07, 2.3818989802748547e-07, 1.4909986134625797e-08, 5.10168831624469e-08, 5.5142201915714395e-08, 2.288550327023131e-08, 5.714110073995471e-08, 5.185095801607531e-07, 4.977285783525076e-08, 1.1049896109227575e-08, 1.264099296349741e-07, 8.174881571676451e-08]]}","title":"PaddleServer"},{"location":"modelserving/v1beta1/paddle/#deploy-paddle-model-with-inferenceservice","text":"In this example, we use a trained paddle resnet50 model to classify images by running an inference service with Paddle predictor.","title":"Deploy paddle model with InferenceService"},{"location":"modelserving/v1beta1/paddle/#create-the-inferenceservice","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"paddle-resnet50\" spec : predictor : paddle : storageUri : \"https://zhouti-mcp-edge.cdn.bcebos.com/resnet50.tar.gz\" Apply the above yaml to create the InferenceService kubectl apply -f paddle.yaml Expected Output inferenceservice.serving.kserve.io/paddle-resnet50 created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/paddle/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = paddle-resnet50 SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./jay.json Expected Output * Trying 127.0.0.1:80... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 80 (#0) > POST /v1/models/paddle-resnet50:predict HTTP/1.1 > Host: paddle-resnet50.default.example.com > User-Agent: curl/7.68.0 > Accept: */* > Content-Length: 3010209 > Content-Type: application/x-www-form-urlencoded > Expect: 100 -continue > * Mark bundle as not supporting multiuse < HTTP/1.1 100 Continue * We are completely uploaded and fine * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 23399 < content-type: application/json; charset=UTF-8 < date: Mon, 17 May 2021 03:34:58 GMT < server: istio-envoy < x-envoy-upstream-service-time: 511 < {\"predictions\": [[6.736678770380422e-09, 1.1535990829258935e-08, 5.142250714129659e-08, 6.647170636142619e-08, 4.094492567219277e-08, 1.3402451770616608e-07, 9.355561303436843e-08, 2.8935891904779965e-08, 6.845367295227334e-08, 7.680615965455218e-08, 2.0334689452283783e-06, 1.1085678579547675e-06, 2.3477592492326949e-07, 6.582037030966603e-07, 0.00012373103527352214, 4.2878804151769145e-07, 6.419959845516132e-06, 0.9993496537208557, 7.372002437477931e-05, 3.101135735050775e-05, 5.6028093240456656e-06, 2.1862508674530545e-06, 1.9544044604913324e-08, 3.728893887000595e-07, 4.2903633357127546e-07, 1.8251179767503345e-07, 7.159925985433802e-08, 9.231618136595898e-09, 6.469241498052725e-07, 7.031690341108288e-09, 4.451231561120039e-08, 1.2455971898361895e-07, 9.44632745358831e-08, 4.347704418705689e-08, 4.658220120745682e-07, 6.797721141538204e-08, 2.1060276367279585e-07, 2.2605123106700376e-08, 1.4311490303953178e-07, 7.951298641728499e-08, 1.2341783417468832e-07, 1.0921713737843675e-06, 1.5243892448779661e-05, 3.1173343018053856e-07, 2.4152058131221565e-07, 6.863762536113427e-08, 8.467682022228473e-08, 9.4246772164297e-08, 1.0219210366813058e-08, 3.3770753304906975e-08, 3.6928835100979995e-08, 1.3694031508748594e-07, 1.0674284567357972e-07, 2.599483650556067e-07, 3.4866405940192635e-07, 3.132053549848024e-08, 3.574873232992104e-07, 6.64843895492595e-08, 3.1638955988455564e-07, 1.2095878219042788e-06, 8.66409024524728e-08, 4.0144172430700564e-08, 1.2544761318622477e-07, 3.3201178695208e-08, 1.9731444922399533e-07, 3.806405572959193e-07, 1.3827865075199952e-07, 2.300225965257141e-08, 7.14422512260171e-08, 2.851114544455413e-08, 2.982567437470607e-08, 8.936032713791064e-08, 6.22388370175031e-07, 6.478838798784636e-08, 1.3663023423760023e-07, 9.973181391842445e-08, 2.5761554667269593e-08, 4.130220077058766e-08, 3.9384463690339544e-08, 1.2158079698565416e-07, 4.302821707824478e-06, 1.8179063090428826e-06, 1.8520155435908237e-06, 1.6246107179540559e-06, 1.6448313544970006e-05, 1.0544916221988387e-05, 3.993061909568496e-06, 2.646479799750523e-07, 1.9193475964129902e-05, 4.803242745765601e-07, 1.696285067964709e-07, 4.550505764200352e-06, 4.235929372953251e-05, 4.443338639248395e-06, 5.104009687784128e-06, 1.3506396498996764e-05, 4.1758724478313525e-07, 4.494491463447048e-07, 3.156698369366495e-07, 1.0557599807725637e-06, 1.336463917311903e-08, 1.3893659556174498e-08, 6.770379457066156e-08, 1.4129696523923485e-07, 7.170518756538513e-08, 7.934466594861078e-08, 2.639154317307657e-08, 2.6134321373660896e-08, 7.196725881897237e-09, 2.1752363466021052e-08, 6.684639686227456e-08, 3.417795824134373e-08, 1.6228275967478112e-07, 4.107114648377319e-07, 6.472135396506928e-07, 2.951379372007068e-07, 5.653474133282543e-09, 4.830144462175667e-08, 8.887481861563629e-09, 3.7306168820805397e-08, 1.7784264727538357e-08, 4.641905082536368e-09, 3.413118676576232e-08, 1.937393818707278e-07, 1.2980176506971475e-06, 3.5641004814124244e-08, 2.149332445355867e-08, 3.055293689158134e-07, 1.5532516783878236e-07, 1.4520978766086046e-06, 3.488464628276233e-08, 3.825438398052938e-05, 4.5088432898410247e-07, 4.1766969616219285e-07, 6.770622462681786e-07, 1.4142248971893423e-07, 1.4235997696232516e-05, 6.293820433711517e-07, 4.762866865348769e-06, 9.024900577969674e-07, 9.058987870957935e-07, 1.5713684433649178e-06, 1.5720647184025438e-07, 1.818536503606083e-07, 7.193188622522939e-08, 1.1952824934269302e-06, 8.874837362782273e-07, 2.0870831463071227e-07, 9.906239029078279e-08, 7.793621747964607e-09, 1.0058498389753368e-07, 4.2059440374941914e-07, 1.843624630737395e-07, 1.6437947181202617e-07, 7.025352743994517e-08, 2.570448600636155e-07, 7.586877615040066e-08, 7.841313731660193e-07, 2.495309274763713e-07, 5.157681925993529e-08, 4.0674127177453556e-08, 7.531796519799627e-09, 4.797485431140558e-08, 1.7419973019627832e-08, 1.7958679165985814e-07, 1.2566392371127222e-08, 8.975440124459055e-08, 3.26965476915575e-08, 1.1208359751435637e-07, 3.906746215420753e-08, 4.6769045525252295e-08, 1.8523553535487736e-07, 1.4833052830454108e-07, 1.2279349448363064e-07, 1.0729105497375713e-06, 3.6538490011395197e-09, 1.6198403329781286e-07, 1.6190719875908144e-08, 1.2004933580556099e-07, 1.4800277448046018e-08, 4.02294837442696e-08, 2.15060893538066e-07, 1.1925696696835075e-07, 4.8982514044837444e-08, 7.608920071788816e-08, 2.3137479487900237e-08, 8.521050176568679e-08, 9.586213423062873e-08, 1.3351650807180704e-07, 3.021699157557123e-08, 4.423876376336011e-08, 2.610667060309879e-08, 2.3977091245797055e-07, 1.3192564551900432e-07, 1.6734931662654162e-08, 1.588336999702733e-07, 4.0643516285854275e-07, 8.753454494581092e-08, 8.366999395548191e-07, 3.437598650180007e-08, 7.847892646850596e-08, 8.526394701391382e-09, 9.601382799928615e-08, 5.258924034023948e-07, 1.3557448141909845e-07, 1.0307226716577134e-07, 1.0429813457335513e-08, 5.187714435805901e-08, 2.187001335585137e-08, 1.1791439824548888e-08, 2.98065643278278e-08, 4.338393466696289e-08, 2.9991046091026874e-08, 2.8507610494443725e-08, 3.058665143385042e-08, 6.441099031917474e-08, 1.5364101102477434e-08, 1.5973883549236234e-08, 2.5736850872704053e-08, 1.0903765712555469e-07, 3.2118737891551064e-08, 6.819742992547617e-09, 1.9251311300649832e-07, 5.8258109447706374e-08, 1.8765761922168167e-07, 4.0070790419122204e-07, 1.5791577823165426e-08, 1.950158434738114e-07, 1.0142063189277906e-08, 2.744815041921811e-08, 1.2843531571604672e-08, 3.7297493094001766e-08, 7.407496838141014e-08, 4.20607833007125e-08, 1.6924804668860816e-08, 1.459203531339881e-07, 4.344977000414474e-08, 1.7191403856031684e-07, 3.5817443233554513e-08, 8.440249388286247e-09, 4.194829728021432e-08, 2.514032360068086e-08, 2.8340199520471288e-08, 8.747196034164517e-08, 8.277125651545703e-09, 1.1676293709683705e-08, 1.4548514570833504e-07, 7.200282148289716e-09, 2.623600948936655e-06, 5.675736929333652e-07, 1.9483527466945816e-06, 6.752595282932816e-08, 8.168475318370838e-08, 1.0933046468153407e-07, 1.670913718498923e-07, 3.1387276777650186e-08, 2.973524537708272e-08, 5.752163900751839e-08, 5.850877471402782e-08, 3.2544622285968217e-07, 3.330221431951941e-08, 4.186786668469722e-07, 1.5085906568401697e-07, 2.3346819943981245e-07, 2.86402780602657e-07, 2.2940319865938363e-07, 1.8537603807544656e-07, 3.151798182443599e-07, 1.1075967449869495e-06, 1.5369782602192572e-07, 1.9237509718550427e-07, 1.64044664074936e-07, 2.900835340824415e-07, 1.246654903752642e-07, 5.802622027317739e-08, 5.186220519703966e-08, 6.0094205167615655e-09, 1.2333241272699524e-07, 1.3798474185477971e-07, 1.7370231830682314e-07, 5.617761189569137e-07, 5.1604470030497396e-08, 4.813277598714194e-08, 8.032698417537176e-08, 2.0645263703045202e-06, 5.638597713186755e-07, 8.794199857220519e-07, 3.4785980460583232e-06, 2.972389268052211e-07, 3.3904532870110415e-07, 9.469074058188198e-08, 3.754845678827223e-08, 1.5679037801419327e-07, 8.203105039683578e-08, 6.847962641387539e-09, 1.8251624211984563e-08, 6.050240841659615e-08, 3.956342808919544e-08, 1.0699947949888156e-07, 3.2566634899922065e-07, 3.5369430406717584e-07, 7.326295303755614e-08, 4.85765610847011e-07, 7.717713401689252e-07, 3.4567779749750116e-08, 3.246204585138912e-07, 3.1608601602783892e-06, 5.33099466792919e-08, 3.645687343123427e-07, 5.48158936908294e-07, 4.62306957160763e-08, 1.3466177506415988e-07, 4.3529482240955986e-08, 1.6404105451783835e-07, 2.463695381038633e-08, 5.958712634424046e-08, 9.493651020875404e-08, 5.523462576206839e-08, 5.7412357534758485e-08, 1.1850350347231142e-05, 5.8263944993086625e-06, 7.4208674050169066e-06, 9.127966222877149e-07, 2.0019581370434025e-06, 1.033498961078294e-06, 3.5146850763112525e-08, 2.058995278275688e-06, 3.5655509122989315e-07, 6.873234070781109e-08, 2.1935298022413008e-09, 5.560363547374436e-08, 3.3266996979364194e-07, 1.307369217329324e-07, 2.718762992515167e-08, 1.0462929189714032e-08, 7.466680358447775e-07, 6.923166040451179e-08, 1.6145664361033596e-08, 8.568521003837759e-09, 4.76221018175238e-09, 1.233977116044116e-07, 8.340628632197422e-09, 3.2649041248333788e-09, 5.0632489312363305e-09, 4.0704994930251814e-09, 1.2043538610839732e-08, 5.105608380517879e-09, 7.267142887457112e-09, 1.184516307262129e-07, 7.53557927168913e-08, 6.386964201965384e-08, 1.6212936770898523e-08, 2.610429419291904e-07, 6.979425393183192e-07, 6.647513117741255e-08, 7.717492849224072e-07, 6.651206945207377e-07, 3.324495310152997e-07, 3.707282019149716e-07, 3.99564243025452e-07, 6.411632114122767e-08, 7.107352217872176e-08, 1.6380016631956096e-07, 6.876800995314625e-08, 3.462474467141874e-07, 2.0256503319160402e-07, 6.19610148078209e-07, 2.6841073363925716e-08, 6.720335363752383e-07, 1.1348340649419697e-06, 1.8397931853542104e-06, 6.397251581802266e-07, 7.257533241045167e-08, 4.2213909523525217e-07, 3.9657925299252383e-07, 1.4037439655112394e-07, 3.249856774800719e-07, 1.5857655455420172e-07, 1.1122217102865761e-07, 7.391420808744442e-08, 3.42322238111592e-07, 5.39796154441774e-08, 8.517296379295658e-08, 4.061009803990601e-06, 1.4478755474556237e-05, 7.317032757470088e-09, 6.9484960008026064e-09, 4.468917325084476e-08, 9.23141172393116e-08, 5.411982328951126e-08, 2.2242811326123046e-07, 1.7609554703312824e-08, 2.0906279374344194e-08, 3.6797682678724186e-09, 6.177919686933819e-08, 1.7920288541972695e-07, 2.6279179721200308e-08, 2.6988200119149042e-08, 1.6432807115052128e-07, 1.2827612749788386e-07, 4.468908798571647e-08, 6.316552969565237e-08, 1.9461760203398626e-08, 2.087125849925542e-08, 2.2414580413965268e-08, 2.4765244077684656e-08, 6.785398465325443e-09, 2.4248794971981624e-08, 4.554979504689527e-09, 2.8977037658250993e-08, 2.0402325162649504e-08, 1.600950270130852e-07, 2.0199709638291097e-07, 1.611188515937556e-08, 5.964113825029926e-08, 4.098318573397819e-09, 3.9080127578472457e-08, 7.511338218080255e-09, 5.965624154669058e-07, 1.6478223585636442e-07, 1.4106989354445432e-08, 3.2855584919389e-08, 3.3387166364917675e-09, 1.220043444050134e-08, 4.624639160510924e-08, 6.842309385746148e-09, 1.74262879681919e-08, 4.6611329906909305e-08, 9.331947836699328e-08, 1.2306078644996887e-07, 1.2359445022980253e-08, 1.1173199254699284e-08, 2.7724862405875683e-08, 2.419210147763806e-07, 3.451186785241589e-07, 2.593766978975509e-08, 9.964568192799561e-08, 9.797809674694236e-09, 1.9085564417764544e-07, 3.972706252852731e-08, 2.6639204619982593e-08, 6.874148805735558e-09, 3.146993776681484e-08, 2.4086594407890516e-07, 1.3126927456141857e-07, 2.1254339799270383e-07, 2.050203384840188e-08, 3.694976058454813e-08, 6.563175816154398e-07, 2.560050127442537e-08, 2.6882981174480847e-08, 6.880636078676616e-07, 2.0092733166166e-07, 2.788039665801989e-08, 2.628409134786125e-08, 5.1678345158734373e-08, 1.8935413947929192e-07, 4.61852835087484e-07, 1.1086777718105623e-08, 1.4542604276357451e-07, 2.8737009216683873e-08, 6.105167926762078e-07, 1.2016463379893594e-08, 1.3944705301582871e-07, 2.093712758721722e-08, 4.3801410498645055e-08, 1.966320795077081e-08, 6.654448991838535e-09, 1.1149590584125235e-08, 6.424939158478082e-08, 6.971554888934861e-09, 3.260019587614238e-09, 1.4260189473702667e-08, 2.7895078247297533e-08, 8.11578289017234e-08, 2.5995715802196173e-08, 2.2855578762914774e-08, 1.055962854934478e-07, 8.145542551574181e-08, 3.7793686402665116e-08, 4.881891513264236e-08, 2.342062366267328e-08, 1.059935517133681e-08, 3.604105103249822e-08, 5.062430830093945e-08, 3.6804440384230475e-08, 1.501580193519203e-09, 1.4475033367489232e-06, 1.076210423889279e-06, 1.304991315009829e-07, 3.073601462233455e-08, 1.7184021317007137e-08, 2.0421090596300928e-08, 7.904992216367646e-09, 1.6902052379919041e-07, 1.2416506933732308e-08, 5.4758292122869534e-08, 2.6250422280327257e-08, 1.3261367115546818e-08, 6.29807459517906e-08, 1.270998595259698e-08, 2.0171681569536304e-07, 4.386637186826192e-08, 6.962349630157405e-08, 2.9565120485131047e-07, 7.925131626507209e-07, 2.0868920103112032e-07, 1.7341794489311724e-07, 4.2942417621816276e-08, 4.213406956665722e-09, 8.824785169281313e-08, 1.7341569957807224e-08, 7.321587247588468e-08, 1.7941774288487977e-08, 1.1245148101579616e-07, 4.242405395871174e-07, 8.259573469615589e-09, 1.1336403105133286e-07, 8.268798978861014e-08, 2.2186977588489754e-08, 1.9539720952366224e-08, 1.0675703876472653e-08, 3.288517547161973e-08, 2.4340963022950746e-08, 6.639137239972115e-08, 5.604687380866835e-09, 1.386604697728444e-08, 6.675873720496384e-08, 1.1355886009312144e-08, 3.132159633878473e-07, 3.12451788886392e-08, 1.502181845580708e-07, 1.3461754377885882e-08, 1.8882955998833495e-07, 4.645742279762999e-08, 4.6453880742092224e-08, 7.714453964524637e-09, 3.5857155467056145e-08, 7.60832108426257e-09, 4.221501370693659e-08, 4.3407251126836854e-09, 1.340157496088068e-08, 8.565600495558101e-08, 1.7045413969185574e-08, 5.4221903411644234e-08, 3.021912675649219e-08, 6.153376119755194e-08, 3.938857240370908e-09, 4.135628017820636e-08, 1.781920389021252e-08, 4.3105885083605244e-08, 3.903354972578654e-09, 7.663085455078544e-08, 1.1890405993142394e-08, 9.304217840622186e-09, 1.0968062014171664e-09, 1.0536767902635802e-08, 1.1516804221400889e-07, 8.134522886393825e-07, 5.952623993721318e-08, 2.806350174466843e-08, 1.2833099027886874e-08, 1.0605690192733164e-07, 7.872949936427176e-07, 2.7501393162765453e-08, 3.936289072470345e-09, 2.0519442145428002e-08, 7.394815870753746e-09, 3.598397313453461e-08, 2.5378517065632877e-08, 4.698972233541099e-08, 7.54952989012736e-09, 6.322805461422831e-07, 5.582006412652163e-09, 1.29640980617296e-07, 1.5874988434916304e-08, 3.3837810775594335e-08, 6.474512037613067e-09, 9.121148281110436e-08, 1.3918511676536127e-08, 8.230025549949005e-09, 2.7061290097663004e-08, 2.6095918315149902e-08, 5.722363471960534e-09, 6.963475698285038e-07, 4.685091781198025e-08, 9.590579885809802e-09, 2.099205858030473e-07, 3.082160660028421e-08, 3.563162565001221e-08, 7.326312925215461e-07, 2.1759731225756695e-06, 2.407518309155421e-07, 2.974515780351794e-07, 2.529018416908002e-08, 7.667950718825978e-09, 2.663289251358947e-07, 3.4358880185436647e-08, 2.3130198201215535e-08, 3.1239693498719134e-08, 2.8691621878351725e-07, 3.895845068768722e-08, 2.4184130253956937e-08, 1.1582445225144511e-08, 5.1545349322168477e-08, 2.034345492063494e-08, 8.201963197507212e-08, 1.164153573540716e-08, 5.496356720868789e-07, 1.1682151246361627e-08, 4.7576914852243135e-08, 1.6349824605299546e-08, 4.090862759653646e-08, 2.1271189609706198e-07, 1.6697286753242224e-07, 3.989708119433999e-08, 2.852450279533514e-06, 1.2500372292834072e-07, 2.4846613655427063e-07, 1.245429093188477e-08, 2.9700272463628608e-08, 4.250991558762962e-09, 1.61443480806156e-07, 2.6386018703306036e-07, 7.638056409575711e-09, 3.4455793773702226e-09, 7.273289526210647e-08, 1.7631434090503717e-08, 7.58661311550668e-09, 2.1547013062672704e-08, 1.2675349125856883e-07, 2.5637149292379036e-08, 3.500976220038865e-08, 6.472243541111311e-08, 8.387915251262257e-09, 3.069512288789156e-08, 7.520387867998579e-08, 1.5724964441687916e-07, 1.9634005354873807e-07, 1.2290831818972947e-07, 1.112118730439704e-09, 1.546895944670723e-08, 9.91701032404535e-09, 6.882473257974198e-07, 8.267616635748709e-08, 4.469531234008173e-08, 2.075201344098332e-08, 8.649378457903367e-08, 5.202766573120243e-08, 4.5564942041664835e-08, 2.0319955496006514e-08, 8.705182352741758e-09, 6.452066969586667e-08, 2.1777438519166026e-08, 1.030954166481024e-08, 3.211904342492744e-08, 2.3336936294526822e-07, 8.054096056753224e-09, 1.9623354319264763e-07, 1.2888089884199871e-07, 1.5392496166555247e-08, 1.401903038100727e-09, 5.696818305978013e-08, 6.080025372057207e-09, 1.0782793324892737e-08, 2.4260730313585555e-08, 1.9388659566743627e-08, 2.2970310453729326e-07, 1.9971754028347277e-08, 2.8477993296860404e-08, 5.2273552597625894e-08, 2.7392806600801123e-07, 9.857291161097237e-08, 3.12910977129377e-08, 4.151442212219081e-08, 5.251196366629074e-09, 1.580681100676884e-06, 8.547603442821128e-07, 1.068913135782168e-08, 1.0621830597301596e-06, 7.737313012512459e-08, 6.394216711669287e-08, 1.1698345758759388e-07, 1.0486609625104393e-07, 2.1161000063329993e-07, 1.53396815250062e-08, 5.094453570109181e-08, 1.4005379966874898e-08, 2.6282036102998063e-08, 8.778433624456738e-08, 7.772066545896905e-09, 4.228875383205377e-08, 3.3243779284930497e-07, 7.729244799747903e-08, 7.636901111496286e-10, 5.989500806435899e-08, 1.326090597331131e-07, 1.2853634245857393e-07, 8.844242671557367e-09, 1.0194374766570036e-07, 2.493779334145074e-07, 1.6547971881664125e-07, 1.1762754326127833e-08, 1.1496195639892903e-07, 2.9342709240154363e-07, 1.326124099421122e-08, 8.630262726683213e-08, 5.7394842656322e-08, 1.1094081031615133e-07, 2.2933713239581266e-07, 3.4706170026765903e-07, 1.4751107357824367e-07, 1.502495017291494e-08, 6.454319390059027e-08, 5.164533689594464e-08, 6.23741556182722e-08, 1.293601457064142e-07, 1.4052071506398534e-08, 5.386946000385251e-08, 2.0827554791935654e-08, 1.3040637902861363e-08, 1.0578981601838677e-07, 1.5079727688771527e-08, 8.92632726845477e-07, 4.6374381668101705e-08, 7.481006036869076e-07, 5.883147302654379e-09, 2.8707685117979054e-09, 8.381598490814213e-07, 7.341958596640552e-09, 1.4245998158912698e-08, 1.0926417104428765e-07, 1.1308178216040687e-07, 2.52339901862797e-07, 1.1782835684925885e-07, 4.6678056975224536e-08, 2.7959197179683315e-09, 3.4363861090014325e-08, 1.4674496640054713e-07, 3.5396915620822256e-08, 2.0581127557761647e-07, 7.18387909159901e-08, 2.7693943138729082e-08, 4.5493386835460115e-08, 1.9559182717898693e-08, 1.5359708172013598e-08, 1.2336623278486059e-08, 2.9570605519779747e-08, 2.877552560676122e-07, 9.051845495378075e-07, 2.3732602016934834e-07, 1.6521676471370483e-08, 1.5478875070584763e-08, 3.526786329643983e-08, 3.616410637619083e-08, 1.61590953950963e-08, 7.65007328595857e-08, 1.9661483108279754e-08, 4.917534823789538e-08, 1.1712612746350715e-07, 1.0889253054813253e-08, 1.494120169809321e-06, 1.018585660261806e-08, 3.7575969003000864e-08, 2.097097784314883e-08, 3.368558054717141e-08, 4.845588819080149e-09, 6.039624622644624e-07, 1.037331109898787e-08, 2.841650257323636e-07, 4.4990630954089283e-07, 3.463186004637464e-08, 7.720684180867465e-08, 1.471122175189521e-07, 1.1601575522490748e-07, 4.007488030310924e-07, 3.025649775167949e-08, 6.706784461130155e-08, 2.0128741340386114e-08, 1.5987744461654074e-09, 4.1919822280078733e-08, 1.3167154477855547e-08, 3.231814815762846e-08, 9.247659704669786e-08, 1.3075300842047e-07, 1.0574301256838226e-07, 3.762165334819656e-08, 1.0942246575496029e-07, 7.001474955359299e-08, 2.742706151082075e-08, 2.0766625752344225e-08, 4.5403403703403455e-08, 3.39040298058535e-08, 1.0469661759771043e-07, 2.8271578855765256e-08, 3.406226767310727e-07, 5.146206945028098e-07, 6.740708613506285e-07, 6.382248063374618e-09, 3.63878704945364e-08, 3.626059807970705e-08, 1.6065602892467723e-07, 3.639055989879125e-07, 6.232691696084203e-09, 4.805490050330263e-08, 3.372633727849461e-08, 6.328880317596486e-07, 6.480631498106959e-08, 2.1165197949812864e-07, 8.38779143919055e-08, 1.7589144363228115e-08, 2.729027670511641e-09, 2.144795097080987e-08, 7.861271456022223e-08, 2.0118186228046397e-08, 2.8407685093156942e-08, 2.4922530883486615e-07, 2.0156670998972004e-08, 2.6551649767725394e-08, 2.7848242822869906e-08, 6.907123761834555e-09, 1.880543720744754e-08, 1.3006903998302732e-08, 3.685918272822164e-07, 3.967941211158177e-07, 2.7592133022835696e-08, 2.5228947819755376e-08, 1.547002881352455e-07, 3.689306637966183e-08, 1.440177199718562e-09, 2.1504929392790473e-08, 5.068111263994979e-08, 5.081711407228795e-08, 1.171875219085905e-08, 5.409278358570191e-08, 7.138276600926474e-07, 2.5237213208129106e-07, 7.072044638789521e-08, 7.199763984999663e-08, 1.2525473103153217e-08, 3.4803417747752974e-07, 1.9591827538079087e-07, 1.2404700555634918e-07, 1.234617457157583e-07, 1.9201337408958352e-08, 1.9895249181445251e-07, 3.7876677794201896e-08, 1.0629785052174157e-08, 1.2437127772102485e-08, 2.1861892207653e-07, 2.6181456291851646e-07, 1.112900775979142e-07, 1.0776630432474121e-07, 6.380325157095967e-09, 3.895085143312826e-09, 1.5762756788717525e-07, 2.909027019271093e-09, 1.0381050685737137e-08, 2.8135211493918177e-08, 1.0778002490496874e-08, 1.3605974125141529e-08, 2.9236465692861202e-08, 1.9189795352758665e-07, 2.199506354827463e-07, 1.326399790002597e-08, 4.9004846403022384e-08, 2.980837132682268e-09, 8.926045680368588e-09, 1.0996975774446582e-08, 7.71560149104289e-09, 7.454491246505768e-09, 5.086162246925596e-08, 1.5129764108223753e-07, 1.1960075596562092e-08, 1.1323334270230134e-08, 9.391332156383214e-09, 9.585701832293125e-08, 1.905532798218701e-08, 1.8105303922766325e-08, 6.179227796110354e-08, 6.389401363549041e-08, 1.1853179771037503e-08, 9.37277544466042e-09, 1.2332148457971925e-07, 1.6522022860954166e-08, 1.246116454467483e-07, 4.196171854431441e-09, 3.996593278543514e-08, 1.2554556505506298e-08, 1.4302138140465104e-08, 6.631793780798034e-09, 5.964224669696705e-09, 5.556936244488497e-09, 1.4192455921602232e-07, 1.7613080771639034e-08, 3.380189639301534e-07, 7.85651934620546e-08, 2.966783085867064e-08, 2.8992105853831163e-06, 1.3787366697215475e-06, 5.313622430946907e-09, 2.512852859126724e-08, 8.406627216572815e-08, 4.492839167369311e-08, 5.408793057881667e-08, 2.4239175999696272e-08, 4.016805235096399e-07, 4.1083545454512205e-08, 5.4153481698904216e-08, 8.640767212853007e-09, 5.773256717134245e-08, 2.6443152023603034e-07, 8.953217047746875e-07, 2.7994001783326894e-08, 5.889480014786841e-09, 4.1788819515886644e-08, 2.8880645430717777e-08, 2.135752907861388e-08, 2.3024175277441827e-07, 8.786625471657317e-08, 2.0697297209437693e-09, 2.236410523437371e-08, 3.203276310870251e-09, 1.176874686592555e-08, 6.963571053120177e-08, 2.271932153519174e-08, 7.360382525689602e-09, 6.922528772435044e-09, 3.213871480056696e-08, 1.370577820125618e-07, 1.9815049157045905e-08, 1.0578956377571558e-08, 2.7049420481262132e-08, 2.9755937713815683e-09, 2.1773699288019088e-08, 1.09755387001087e-08, 1.991872444762066e-08, 2.3882098076910552e-08, 2.1357365653784655e-08, 6.109098560358461e-09, 1.1890497475519624e-08, 1.1459891702259029e-08, 3.73173456580389e-08, 1.572620256240498e-08, 3.404023374287135e-08, 3.6921580459647885e-08, 9.281765045443535e-08, 1.2323201303843234e-07, 4.2347593876002065e-08, 1.7423728237986325e-08, 5.8113389656000436e-08, 3.931436154402945e-08, 2.3690461148362374e-08, 1.792850135018398e-08, 1.440664210150544e-08, 7.019830494670032e-09, 6.041522482291839e-08, 4.867479930226182e-08, 1.0685319296044327e-08, 1.0051243393149889e-08, 4.2426261614991745e-08, 2.607815297039906e-08, 5.136670200300* Connection #0 to host localhost left intact 841e-09, 1.69729952315123e-09, 1.9131586981302462e-08, 2.111743526711507e-07, 1.337269672774255e-08, 2.0002481448955223e-08, 1.0454256482717028e-07, 2.8144228281234973e-08, 2.1344791889532644e-07, 2.1046110632028103e-08, 1.9114453664315079e-07, 3.957693550660224e-08, 2.931631826186276e-08, 1.105203111251285e-07, 4.84007678380749e-08, 5.583606110803885e-08, 1.2130111315400427e-07, 1.77621615193857e-08, 2.5610853882085394e-08, 1.203865309662433e-07, 4.674859610531712e-09, 1.5916098661250544e-08, 3.147594185293201e-08, 6.147686093527227e-08, 2.204641802450169e-08, 3.257763410147163e-07, 1.198914532096751e-07, 2.3818989802748547e-07, 1.4909986134625797e-08, 5.10168831624469e-08, 5.5142201915714395e-08, 2.288550327023131e-08, 5.714110073995471e-08, 5.185095801607531e-07, 4.977285783525076e-08, 1.1049896109227575e-08, 1.264099296349741e-07, 8.174881571676451e-08]]}","title":"Run a Prediction"},{"location":"modelserving/v1beta1/pmml/","text":"Deploy PMML model with InferenceService \u00b6 PMML, or predictive model markup language, is an XML format for describing data mining and statistical models, including inputs to the models, transformations used to prepare data for data mining, and the parameters that define the models themselves. In this example we show how you can serve the PMML format model on InferenceService . Create the InferenceService \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"pmml-demo\" spec : predictor : pmml : storageUri : gs://kfserving-examples/models/pmml Create the InferenceService with above yaml kubectl apply -f pmml.yaml Expected Output $ inferenceservice.serving.kserve.io/pmml-demo created Warning The pmmlserver is based on Py4J and that doesn't support multi-process mode, so we can't set spec.predictor.containerConcurrency . If you want to scale the PMMLServer to improve prediction performance, you should set the InferenceService's resources.limits.cpu to 1 and scale the replica size. Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=pmml-demo INPUT_PATH=@./pmml-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice pmml-demo -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * TCP_NODELAY set * Connected to localhost (::1) port 8081 (#0) > POST /v1/models/pmml-demo:predict HTTP/1.1 > Host: pmml-demo.default.example.com > User-Agent: curl/7.64.1 > Accept: */* > Content-Length: 45 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 45 out of 45 bytes < HTTP/1.1 200 OK < content-length: 39 < content-type: application/json; charset=UTF-8 < date: Sun, 18 Oct 2020 15:50:02 GMT < server: istio-envoy < x-envoy-upstream-service-time: 12 < * Connection #0 to host localhost left intact {\"predictions\": [{'Species': 'setosa', 'Probability_setosa': 1.0, 'Probability_versicolor': 0.0, 'Probability_virginica': 0.0, 'Node_Id': '2'}]}* Closing connection 0","title":"PMML"},{"location":"modelserving/v1beta1/pmml/#deploy-pmml-model-with-inferenceservice","text":"PMML, or predictive model markup language, is an XML format for describing data mining and statistical models, including inputs to the models, transformations used to prepare data for data mining, and the parameters that define the models themselves. In this example we show how you can serve the PMML format model on InferenceService .","title":"Deploy PMML model with InferenceService"},{"location":"modelserving/v1beta1/pmml/#create-the-inferenceservice","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"pmml-demo\" spec : predictor : pmml : storageUri : gs://kfserving-examples/models/pmml Create the InferenceService with above yaml kubectl apply -f pmml.yaml Expected Output $ inferenceservice.serving.kserve.io/pmml-demo created Warning The pmmlserver is based on Py4J and that doesn't support multi-process mode, so we can't set spec.predictor.containerConcurrency . If you want to scale the PMMLServer to improve prediction performance, you should set the InferenceService's resources.limits.cpu to 1 and scale the replica size.","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/pmml/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=pmml-demo INPUT_PATH=@./pmml-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice pmml-demo -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * TCP_NODELAY set * Connected to localhost (::1) port 8081 (#0) > POST /v1/models/pmml-demo:predict HTTP/1.1 > Host: pmml-demo.default.example.com > User-Agent: curl/7.64.1 > Accept: */* > Content-Length: 45 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 45 out of 45 bytes < HTTP/1.1 200 OK < content-length: 39 < content-type: application/json; charset=UTF-8 < date: Sun, 18 Oct 2020 15:50:02 GMT < server: istio-envoy < x-envoy-upstream-service-time: 12 < * Connection #0 to host localhost left intact {\"predictions\": [{'Species': 'setosa', 'Probability_setosa': 1.0, 'Probability_versicolor': 0.0, 'Probability_virginica': 0.0, 'Node_Id': '2'}]}* Closing connection 0","title":"Run a prediction"},{"location":"modelserving/v1beta1/sklearn/v2/","text":"Deploy sklearn-learn models with InferenceService \u00b6 This example walks you through how to deploy a scikit-learn model leveraging the v1beta1 version of the InferenceService CRD. Note that, by default the v1beta1 version will expose your model through an API compatible with the existing V1 Dataplane. However, this example will show you how to serve a model through an API compatible with the new V2 Dataplane . Training \u00b6 The first step will be to train a sample scikit-learn model. Note that this model will be then saved as model.joblib . from sklearn import svm from sklearn import datasets from joblib import dump iris = datasets . load_iris () X , y = iris . data , iris . target clf = svm . SVC ( gamma = 'scale' ) clf . fit ( X , y ) dump ( clf , 'model.joblib' ) Testing locally \u00b6 Once you've got your model serialised model.joblib , we can then use MLServer to spin up a local server. For more details on MLServer, feel free to check the SKLearn example doc . Note this step is optional and just meant for testing, feel free to jump straight to deploying with InferenceService . Pre-requisites \u00b6 Firstly, to use MLServer locally, you will first need to install the mlserver package in your local environment, as well as the SKLearn runtime. pip install mlserver mlserver-sklearn Model settings \u00b6 The next step will be providing some model settings so that MLServer knows: The inference runtime to serve your model (i.e. mlserver_sklearn.SKLearnModel ) The model's name and version These can be specified through environment variables or by creating a local model-settings.json file: { \"name\" : \"sklearn-iris\" , \"version\" : \"v1.0.0\" , \"implementation\" : \"mlserver_sklearn.SKLearnModel\" } Note that, when you deploy your model , KServe will already inject some sensible defaults so that it runs out-of-the-box without any further configuration. However, you can still override these defaults by providing a model-settings.json file similar to your local one. You can even provide a set of model-settings.json files to load multiple models . Serving our model locally \u00b6 With the mlserver package installed locally and a local model-settings.json file, you should now be ready to start our server as: mlserver start . Deploy with InferenceService \u00b6 Lastly, you will use KServe to deploy the trained model. For this, you will just need to use version v1beta1 of the InferenceService CRD and set the protocolVersion field to v2 . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"sklearn-irisv2\" spec : predictor : sklearn : protocolVersion : \"v2\" storageUri : \"gs://seldon-models/sklearn/iris\" Note that this makes the following assumptions: Your model weights (i.e. your model.joblib file) have already been uploaded to a \"model repository\" (GCS in this example) and can be accessed as gs://seldon-models/sklearn/iris . There is a K8s cluster available, accessible through kubectl . KServe has already been installed in your cluster . kubectl kubectl apply -f ./sklearn.yaml Testing deployed model \u00b6 You can now test your deployed model by sending a sample request. Note that this request needs to follow the V2 Dataplane protocol . You can see an example payload below: { \"inputs\" : [ { \"name\" : \"input-0\" , \"shape\" : [ 2 , 4 ], \"datatype\" : \"FP32\" , \"data\" : [ [ 6.8 , 2.8 , 4.8 , 1.4 ], [ 6.0 , 3.4 , 4.5 , 1.6 ] ] } ] } Now, assuming that your ingress can be accessed at ${INGRESS_HOST}:${INGRESS_PORT} or you can follow this instruction to find out your ingress IP and port. you can use curl to send the inference request as: SERVICE_HOSTNAME = $( kubectl get inferenceservice sklearn-irisv2 -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v \\ -H \"Host: ${ SERVICE_HOSTNAME } \" \\ -d @./iris-input.json \\ http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/sklearn-irisv2/infer Expected Output { \"id\" : \"823248cc-d770-4a51-9606-16803395569c\" , \"model_name\" : \"iris-classifier\" , \"model_version\" : \"v1.0.0\" , \"outputs\" : [ { \"data\" : [ 1 , 2 ], \"datatype\" : \"FP32\" , \"name\" : \"predict\" , \"parameters\" : null , \"shape\" : [ 2 ] } ] }","title":"scikit-learn"},{"location":"modelserving/v1beta1/sklearn/v2/#deploy-sklearn-learn-models-with-inferenceservice","text":"This example walks you through how to deploy a scikit-learn model leveraging the v1beta1 version of the InferenceService CRD. Note that, by default the v1beta1 version will expose your model through an API compatible with the existing V1 Dataplane. However, this example will show you how to serve a model through an API compatible with the new V2 Dataplane .","title":"Deploy sklearn-learn models with InferenceService"},{"location":"modelserving/v1beta1/sklearn/v2/#training","text":"The first step will be to train a sample scikit-learn model. Note that this model will be then saved as model.joblib . from sklearn import svm from sklearn import datasets from joblib import dump iris = datasets . load_iris () X , y = iris . data , iris . target clf = svm . SVC ( gamma = 'scale' ) clf . fit ( X , y ) dump ( clf , 'model.joblib' )","title":"Training"},{"location":"modelserving/v1beta1/sklearn/v2/#testing-locally","text":"Once you've got your model serialised model.joblib , we can then use MLServer to spin up a local server. For more details on MLServer, feel free to check the SKLearn example doc . Note this step is optional and just meant for testing, feel free to jump straight to deploying with InferenceService .","title":"Testing locally"},{"location":"modelserving/v1beta1/sklearn/v2/#pre-requisites","text":"Firstly, to use MLServer locally, you will first need to install the mlserver package in your local environment, as well as the SKLearn runtime. pip install mlserver mlserver-sklearn","title":"Pre-requisites"},{"location":"modelserving/v1beta1/sklearn/v2/#model-settings","text":"The next step will be providing some model settings so that MLServer knows: The inference runtime to serve your model (i.e. mlserver_sklearn.SKLearnModel ) The model's name and version These can be specified through environment variables or by creating a local model-settings.json file: { \"name\" : \"sklearn-iris\" , \"version\" : \"v1.0.0\" , \"implementation\" : \"mlserver_sklearn.SKLearnModel\" } Note that, when you deploy your model , KServe will already inject some sensible defaults so that it runs out-of-the-box without any further configuration. However, you can still override these defaults by providing a model-settings.json file similar to your local one. You can even provide a set of model-settings.json files to load multiple models .","title":"Model settings"},{"location":"modelserving/v1beta1/sklearn/v2/#serving-our-model-locally","text":"With the mlserver package installed locally and a local model-settings.json file, you should now be ready to start our server as: mlserver start .","title":"Serving our model locally"},{"location":"modelserving/v1beta1/sklearn/v2/#deploy-with-inferenceservice","text":"Lastly, you will use KServe to deploy the trained model. For this, you will just need to use version v1beta1 of the InferenceService CRD and set the protocolVersion field to v2 . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"sklearn-irisv2\" spec : predictor : sklearn : protocolVersion : \"v2\" storageUri : \"gs://seldon-models/sklearn/iris\" Note that this makes the following assumptions: Your model weights (i.e. your model.joblib file) have already been uploaded to a \"model repository\" (GCS in this example) and can be accessed as gs://seldon-models/sklearn/iris . There is a K8s cluster available, accessible through kubectl . KServe has already been installed in your cluster . kubectl kubectl apply -f ./sklearn.yaml","title":"Deploy with InferenceService"},{"location":"modelserving/v1beta1/sklearn/v2/#testing-deployed-model","text":"You can now test your deployed model by sending a sample request. Note that this request needs to follow the V2 Dataplane protocol . You can see an example payload below: { \"inputs\" : [ { \"name\" : \"input-0\" , \"shape\" : [ 2 , 4 ], \"datatype\" : \"FP32\" , \"data\" : [ [ 6.8 , 2.8 , 4.8 , 1.4 ], [ 6.0 , 3.4 , 4.5 , 1.6 ] ] } ] } Now, assuming that your ingress can be accessed at ${INGRESS_HOST}:${INGRESS_PORT} or you can follow this instruction to find out your ingress IP and port. you can use curl to send the inference request as: SERVICE_HOSTNAME = $( kubectl get inferenceservice sklearn-irisv2 -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v \\ -H \"Host: ${ SERVICE_HOSTNAME } \" \\ -d @./iris-input.json \\ http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/sklearn-irisv2/infer Expected Output { \"id\" : \"823248cc-d770-4a51-9606-16803395569c\" , \"model_name\" : \"iris-classifier\" , \"model_version\" : \"v1.0.0\" , \"outputs\" : [ { \"data\" : [ 1 , 2 ], \"datatype\" : \"FP32\" , \"name\" : \"predict\" , \"parameters\" : null , \"shape\" : [ 2 ] } ] }","title":"Testing deployed model"},{"location":"modelserving/v1beta1/spark/","text":"Predict on a Spark MLlib model PMML InferenceService \u00b6 Setup \u00b6 Install pyspark 3.0.x and pyspark2pmml pip install pyspark~ = 3 .0.0 pip install pyspark2pmml Get JPMML-SparkML jar Train a Spark MLlib model and export to PMML file \u00b6 Launch pyspark with --jars to specify the location of the JPMML-SparkML uber-JAR pyspark --jars ./jpmml-sparkml-executable-1.6.3.jar Fitting a Spark ML pipeline: from pyspark.ml import Pipeline from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml.feature import RFormula df = spark . read . csv ( \"Iris.csv\" , header = True , inferSchema = True ) formula = RFormula ( formula = \"Species ~ .\" ) classifier = DecisionTreeClassifier () pipeline = Pipeline ( stages = [ formula , classifier ]) pipelineModel = pipeline . fit ( df ) from pyspark2pmml import PMMLBuilder pmmlBuilder = PMMLBuilder ( sc , df , pipelineModel ) pmmlBuilder . buildFile ( \"DecisionTreeIris.pmml\" ) Upload the DecisionTreeIris.pmml to a GCS bucket, note that the PMMLServer expect model file name to be model.pmml gsutil cp ./DecisionTreeIris.pmml gs:// $BUCKET_NAME /sparkpmml/model.pmml Create the InferenceService with PMMLServer \u00b6 Create the InferenceService with pmml predictor and specify the storageUri with bucket location you uploaded to apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"spark-pmml\" spec : predictor : pmml : storageUri : gs://kfserving-examples/models/sparkpmml Apply the InferenceService custom resource kubectl apply -f spark_pmml.yaml Expected Output $ inferenceservice.serving.kserve.io/spark-pmml created Wait the InferenceService to be ready kubectl wait --for = condition = Ready inferenceservice spark-pmml inferenceservice.serving.kserve.io/spark-pmml condition met Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=spark-pmml INPUT_PATH=@./pmml-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice spark-pmml -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * Connected to spark-pmml.default.35.237.217.209.xip.io (35.237.217.209) port 80 (#0) > POST /v1/models/spark-pmml:predict HTTP/1.1 > Host: spark-pmml.default.35.237.217.209.xip.io > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 45 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 45 out of 45 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 39 < content-type: application/json; charset=UTF-8 < date: Sun, 07 Mar 2021 19:32:50 GMT < server: istio-envoy < x-envoy-upstream-service-time: 14 < * Connection #0 to host spark-pmml.default.35.237.217.209.xip.io left intact {\"predictions\": [[1.0, 0.0, 1.0, 0.0]]}","title":"Spark"},{"location":"modelserving/v1beta1/spark/#predict-on-a-spark-mllib-model-pmml-inferenceservice","text":"","title":"Predict on a Spark MLlib model PMML InferenceService"},{"location":"modelserving/v1beta1/spark/#setup","text":"Install pyspark 3.0.x and pyspark2pmml pip install pyspark~ = 3 .0.0 pip install pyspark2pmml Get JPMML-SparkML jar","title":"Setup"},{"location":"modelserving/v1beta1/spark/#train-a-spark-mllib-model-and-export-to-pmml-file","text":"Launch pyspark with --jars to specify the location of the JPMML-SparkML uber-JAR pyspark --jars ./jpmml-sparkml-executable-1.6.3.jar Fitting a Spark ML pipeline: from pyspark.ml import Pipeline from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml.feature import RFormula df = spark . read . csv ( \"Iris.csv\" , header = True , inferSchema = True ) formula = RFormula ( formula = \"Species ~ .\" ) classifier = DecisionTreeClassifier () pipeline = Pipeline ( stages = [ formula , classifier ]) pipelineModel = pipeline . fit ( df ) from pyspark2pmml import PMMLBuilder pmmlBuilder = PMMLBuilder ( sc , df , pipelineModel ) pmmlBuilder . buildFile ( \"DecisionTreeIris.pmml\" ) Upload the DecisionTreeIris.pmml to a GCS bucket, note that the PMMLServer expect model file name to be model.pmml gsutil cp ./DecisionTreeIris.pmml gs:// $BUCKET_NAME /sparkpmml/model.pmml","title":"Train a Spark MLlib model and export to PMML file"},{"location":"modelserving/v1beta1/spark/#create-the-inferenceservice-with-pmmlserver","text":"Create the InferenceService with pmml predictor and specify the storageUri with bucket location you uploaded to apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"spark-pmml\" spec : predictor : pmml : storageUri : gs://kfserving-examples/models/sparkpmml Apply the InferenceService custom resource kubectl apply -f spark_pmml.yaml Expected Output $ inferenceservice.serving.kserve.io/spark-pmml created Wait the InferenceService to be ready kubectl wait --for = condition = Ready inferenceservice spark-pmml inferenceservice.serving.kserve.io/spark-pmml condition met","title":"Create the InferenceService with PMMLServer"},{"location":"modelserving/v1beta1/spark/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=spark-pmml INPUT_PATH=@./pmml-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice spark-pmml -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * Connected to spark-pmml.default.35.237.217.209.xip.io (35.237.217.209) port 80 (#0) > POST /v1/models/spark-pmml:predict HTTP/1.1 > Host: spark-pmml.default.35.237.217.209.xip.io > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 45 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 45 out of 45 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 39 < content-type: application/json; charset=UTF-8 < date: Sun, 07 Mar 2021 19:32:50 GMT < server: istio-envoy < x-envoy-upstream-service-time: 14 < * Connection #0 to host spark-pmml.default.35.237.217.209.xip.io left intact {\"predictions\": [[1.0, 0.0, 1.0, 0.0]]}","title":"Run a prediction"},{"location":"modelserving/v1beta1/tensorflow/","text":"Deploy Tensorflow Model with InferenceService \u00b6 Create the HTTP InferenceService \u00b6 Create an InferenceService yaml which specifies the framework tensorflow and storageUri that is pointed to a saved tensorflow model , and name it as tensorflow.yaml . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"flower-sample\" spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Apply the tensorflow.yaml to create the InferenceService , by default it exposes a HTTP/REST endpoint. kubectl kubectl apply -f tensorflow.yaml Expected Output $ inferenceservice.serving.kserve.io/flower-sample created Wait for the InferenceService to be in ready state kubectl get isvc flower-sample NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE flower-sample http://flower-sample.default.example.com True 100 flower-sample-predictor-default-n9zs6 7m15s Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=flower-sample INPUT_PATH=@./input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * Connected to localhost (::1) port 8080 (#0) > POST /v1/models/tensorflow-sample:predict HTTP/1.1 > Host: tensorflow-sample.default.example.com > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 16201 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 16201 out of 16201 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 222 < content-type: application/json < date: Sun, 31 Jan 2021 01:01:50 GMT < x-envoy-upstream-service-time: 280 < server: istio-envoy < { \"predictions\": [ { \"scores\": [0.999114931, 9.20987877e-05, 0.000136786213, 0.000337257545, 0.000300532585, 1.84813616e-05], \"prediction\": 0, \"key\": \" 1\" } ] } Canary Rollout \u00b6 Canary rollout is a great way to control the risk of rolling out a new model by first moving a small percent of the traffic to it and then gradually increase the percentage. To run a canary rollout, you can apply the canary.yaml with the canaryTrafficPercent field specified. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"flower-example\" spec : predictor : canaryTrafficPercent : 20 tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" kubectl kubectl apply -f canary.yaml To verify if the traffic split percentage is applied correctly, you can run the following command: kubectl kubectl get isvc flower-sample NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE flower-sample http://flower-sample.default.example.com True 80 20 flower-sample-predictor-default-n9zs6 flower-sample-predictor-default-2kwtr 7m15s As you can see the traffic is split between the last rolled out revision and the current latest ready revision, KServe automatically tracks the last rolled out(stable) revision for you so you do not need to maintain both default and canary on the InferenceService as in v1alpha2. Create the gRPC InferenceService \u00b6 Create InferenceService which exposes the gRPC port and by default it listens on port 9000. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"flower-grpc\" spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" ports : - containerPort : 9000 name : h2c protocol : TCP Apply grpc.yaml to create the gRPC InferenceService. kubectl kubectl apply -f grpc.yaml Expected Output $ inferenceservice.serving.kserve.io/flower-grpc created Run a prediction \u00b6 We use a python gRPC client for the prediction, so you need to create a python virtual environment and install the tensorflow-serving-api . # The prediction script is written in TensorFlow 1.x pip install tensorflow-serving-api> = 1 .14.0,< 2 .0.0 Run prediction script MODEL_NAME = flower-grpc INPUT_PATH = ./input.json SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) python grpc_client.py --host $INGRESS_HOST --port $INGRESS_PORT --model $MODEL_NAME --hostname $SERVICE_HOSTNAME --input_path $INPUT_PATH Expected Output outputs { key: \"key\" value { dtype: DT_STRING tensor_shape { dim { size: 1 } } string_val: \" 1\" } } outputs { key: \"prediction\" value { dtype: DT_INT64 tensor_shape { dim { size: 1 } } int64_val: 0 } } outputs { key: \"scores\" value { dtype: DT_FLOAT tensor_shape { dim { size: 1 } dim { size: 6 } } float_val: 0.9991149306297302 float_val: 9.209887502947822e-05 float_val: 0.00013678647519554943 float_val: 0.0003372581850271672 float_val: 0.0003005331673193723 float_val: 1.848137799242977e-05 } } model_spec { name: \"flowers-sample\" version { value: 1 } signature_name: \"serving_default\" }","title":"Tensorflow"},{"location":"modelserving/v1beta1/tensorflow/#deploy-tensorflow-model-with-inferenceservice","text":"","title":"Deploy Tensorflow Model with InferenceService"},{"location":"modelserving/v1beta1/tensorflow/#create-the-http-inferenceservice","text":"Create an InferenceService yaml which specifies the framework tensorflow and storageUri that is pointed to a saved tensorflow model , and name it as tensorflow.yaml . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"flower-sample\" spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" Apply the tensorflow.yaml to create the InferenceService , by default it exposes a HTTP/REST endpoint. kubectl kubectl apply -f tensorflow.yaml Expected Output $ inferenceservice.serving.kserve.io/flower-sample created Wait for the InferenceService to be in ready state kubectl get isvc flower-sample NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE flower-sample http://flower-sample.default.example.com True 100 flower-sample-predictor-default-n9zs6 7m15s","title":"Create the HTTP InferenceService"},{"location":"modelserving/v1beta1/tensorflow/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME=flower-sample INPUT_PATH=@./input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH Expected Output * Connected to localhost (::1) port 8080 (#0) > POST /v1/models/tensorflow-sample:predict HTTP/1.1 > Host: tensorflow-sample.default.example.com > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 16201 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 16201 out of 16201 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 222 < content-type: application/json < date: Sun, 31 Jan 2021 01:01:50 GMT < x-envoy-upstream-service-time: 280 < server: istio-envoy < { \"predictions\": [ { \"scores\": [0.999114931, 9.20987877e-05, 0.000136786213, 0.000337257545, 0.000300532585, 1.84813616e-05], \"prediction\": 0, \"key\": \" 1\" } ] }","title":"Run a prediction"},{"location":"modelserving/v1beta1/tensorflow/#canary-rollout","text":"Canary rollout is a great way to control the risk of rolling out a new model by first moving a small percent of the traffic to it and then gradually increase the percentage. To run a canary rollout, you can apply the canary.yaml with the canaryTrafficPercent field specified. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"flower-example\" spec : predictor : canaryTrafficPercent : 20 tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers-2\" kubectl kubectl apply -f canary.yaml To verify if the traffic split percentage is applied correctly, you can run the following command: kubectl kubectl get isvc flower-sample NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE flower-sample http://flower-sample.default.example.com True 80 20 flower-sample-predictor-default-n9zs6 flower-sample-predictor-default-2kwtr 7m15s As you can see the traffic is split between the last rolled out revision and the current latest ready revision, KServe automatically tracks the last rolled out(stable) revision for you so you do not need to maintain both default and canary on the InferenceService as in v1alpha2.","title":"Canary Rollout"},{"location":"modelserving/v1beta1/tensorflow/#create-the-grpc-inferenceservice","text":"Create InferenceService which exposes the gRPC port and by default it listens on port 9000. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"flower-grpc\" spec : predictor : tensorflow : storageUri : \"gs://kfserving-samples/models/tensorflow/flowers\" ports : - containerPort : 9000 name : h2c protocol : TCP Apply grpc.yaml to create the gRPC InferenceService. kubectl kubectl apply -f grpc.yaml Expected Output $ inferenceservice.serving.kserve.io/flower-grpc created","title":"Create the gRPC InferenceService"},{"location":"modelserving/v1beta1/tensorflow/#run-a-prediction_1","text":"We use a python gRPC client for the prediction, so you need to create a python virtual environment and install the tensorflow-serving-api . # The prediction script is written in TensorFlow 1.x pip install tensorflow-serving-api> = 1 .14.0,< 2 .0.0 Run prediction script MODEL_NAME = flower-grpc INPUT_PATH = ./input.json SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) python grpc_client.py --host $INGRESS_HOST --port $INGRESS_PORT --model $MODEL_NAME --hostname $SERVICE_HOSTNAME --input_path $INPUT_PATH Expected Output outputs { key: \"key\" value { dtype: DT_STRING tensor_shape { dim { size: 1 } } string_val: \" 1\" } } outputs { key: \"prediction\" value { dtype: DT_INT64 tensor_shape { dim { size: 1 } } int64_val: 0 } } outputs { key: \"scores\" value { dtype: DT_FLOAT tensor_shape { dim { size: 1 } dim { size: 6 } } float_val: 0.9991149306297302 float_val: 9.209887502947822e-05 float_val: 0.00013678647519554943 float_val: 0.0003372581850271672 float_val: 0.0003005331673193723 float_val: 1.848137799242977e-05 } } model_spec { name: \"flowers-sample\" version { value: 1 } signature_name: \"serving_default\" }","title":"Run a prediction"},{"location":"modelserving/v1beta1/torchserve/","text":"Deploy PyTorch model with TorchServe InferenceService \u00b6 In this example, we use a trained pytorch mnist model to predict handwritten digits by running an inference service with TorchServe predictor. Creating model storage with model archive file \u00b6 TorchServe provides a utility to package all the model artifacts into a single Torchserve Model Archive Files (MAR) . You can store your model and dependent files on remote storage or local persistent volume, the mnist model and dependent files can be obtained from here . The KServe/TorchServe integration expects following model store layout. \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 config.properties \u251c\u2500\u2500 model-store \u2502 \u251c\u2500\u2500 densenet_161.mar \u2502 \u251c\u2500\u2500 mnist.mar Note For remote storage you can choose to start the example using the prebuilt mnist MAR file stored on KServe example GCS bucket gs://kfserving-examples/models/torchserve/image_classifier , you can also generate the MAR file with torch-model-archiver and create the model store on remote storage according to the above layout. torch-model-archiver --model-name mnist --version 1 .0 \\ --model-file model-archiver/model-store/mnist/mnist.py \\ --serialized-file model-archiver/model-store/mnist/mnist_cnn.pt \\ --handler model-archiver/model-store/mnist/mnist_handler.py \\ For PVC user please refer to model archive file generation for auto generation of MAR files from the model and dependent files. TorchServe with KServe envelope inference endpoints \u00b6 The KServe/TorchServe integration supports KServe v1 protocol and we are working on to support v2 protocol. API Verb Path Payload Predict POST /v1/models/ :predict Request:{\"instances\": []} Response:{\"predictions\": []} Explain POST /v1/models/ :explain Request:{\"instances\": []} Response:{\"predictions\": [], \"explainations\": []} Note The config.properties file includes the flag service_envelope=kfserving to enable the KServe inference protocol. The requests are converted from KServe inference request format to torchserve request format and sent to the inference_address configured via local socket. Sample requests for text and image classification Create the InferenceService \u00b6 For deploying the InferenceService on CPU apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier kubectl kubectl apply -f torchserve.yaml For deploying the InferenceService on GPU apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier resources : limits : memory : 4Gi nvidia.com/gpu : \"1\" kubectl kubectl apply -f gpu.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve created Inference \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torchserve -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) Use image converter to create input request for mnist. For other models please refer to input request curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist HTTP/1.1 > Host: torchserve.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 167 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Tue, 27 Oct 2020 08 :26:19 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517 < x-envoy-upstream-service-time: 6 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"predictions\" : [ \"2\" ]} Explanation \u00b6 Model interpretability is an important aspect which help to understand which of the input features were important for a particular classification. Captum is a model interpretability library, the KServe Explain Endpoint uses Captum's state-of-the-art algorithm, including integrated gradients to provide user with an easy way to understand which features are contributing to the model output. Your can refer to Captum Tutorial for more examples. curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/mnist:explain -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist:explain HTTP/1.1 > Host: torchserve.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 167 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Tue, 27 Oct 2020 08 :26:19 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517 < x-envoy-upstream-service-time: 6 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"explanations\" : [[[[ 0 .0005394675730469475, -0.0022280013123036043, -0.003416480100841055, -0.0051329881112415965, -0.009973864160829985, -0.004112560908882716, -0.009223458030656112, -0.0006676354577291628, -0.005249806664413386, -0.0009790519227372953, -0.0026914653993121195, -0.0069470097151383995, -0.00693530415962956, -0.005973878697847718, -0.00425042437288857, 0 .0032867281838150977, -0.004297780258633562, -0.005643196661192014, -0.00653025019738562, -0.0047062916121001185, -0.0018656628277792628, -0.0016757477204072532, -0.0010410417081844845, -0.0019093520822156726, -0.004451403461006374, -0.0008552767257773671, -0.0027638888169885267, -0.0 ] , [ 0 .006971297052106784, 0 .007316855222185687, 0 .012144494329150574, 0 .011477799383288441, 0 .006846725347670252, 0 .01149386176451476, 0 .0045351987881190655, 0 .007038361889638708, 0 .0035855377023272157, 0 .003031419502053957, -0.0008611575226775316, -0.0011085224745969223, -0.0050840743637658534, 0 .009855491784340777, 0 .007220680811043034, 0 .011374285598070253, 0 .007147725481709019, 0 .0037114580912849457, 0 .00030763245479291384, 0 .0018305492665953394, 0 .010106224395114147, 0 .012932881164284687, 0 .008862892007714321, 0 .0070960526615982435, -0.0015931137903787505, 0 .0036495747329455906, 0 .0002593849391051298, -0.0 ] , [ 0 .006467265785857396, -0.00041793201228071674, 0 .004900316089756856, 0 .002308395474823997, 0 .007859295399592283, 0 .003916404948969494, 0 .005630750246437249, 0 .0043712538044184375, 0 .006128530599133763, -0.009446321309831246, -0.014173645867037036, -0.0062988650915794565, -0.011473838941118539, -0.009049151947644047, -0.0007625645864610934, -0.013721416630061238, -0.0005580156670410108, 0 .0033404383756480784, -0.006693278798487951, -0.003705084551144756, 0 .005100375089529131, 5 .5276874714401074e-05, 0 .007221745280359063, -0.00573598303916232, -0.006836169033785967, 0 .0025401608627538936, 9 .303533912921196e-05, -0.0 ] , [ 0 .005914399808621816, 0 .00452643561023696, 0 .003968242261515448, 0 .010422786058967673, 0 .007728358107899074, 0 .01147115923288383, 0 .005683869479056691, 0 .011150670502307374, 0 .008742555292485278, 0 .0032882897575743754, 0 .014841138421861584, 0 .011741228362482451, 0 .0004296862879259221, -0.0035118140680654854, -0.006152254410078331, -0.004925121936901983, -2.3611205202801947e-06, 0 .029347073037039074, 0 .02901626308947743, 0 .023379353021343398, 0 .004027157620197582, -0.01677662249919171, -0.013497255736128979, 0 .006957482854214602, 0 .0018321766800746145, 0 .008277034396684563, 0 .002733405455464871, -0.0 ] , [ 0 .0049579739156640065, -0.002168016158233997, 0 .0020644317321723642, 0 .0020912464240293825, 0 .004719691119907336, 0 .007879231202446626, 0 .010594445898145937, 0 .006533067778982801, 0 .002290214592708113, -0.0036651114968251986, 0 .010753227423379443, 0 .006402706020466243, -0.047075193909339695, -0.08108259303568185, -0.07646875196692542, -0.1681834845371156, -0.1610307396135756, -0.12010309927453829, -0.016148831320070896, -0.009541525999486027, 0 .04575604594761406, 0 .031470966329886635, 0 .02452149438024385, 0 .016594078577569567, 0 .012213591301610382, -0.002230875840404426, 0 .0036704051254298374, -0.0 ] , [ 0 .006410107592414739, 0 .005578283890924384, 0 .001977103461731095, 0 .008935476507124939, 0 .0011305055729953436, 0 .0004946313900665659, -0.0040266029554395935, -0.004270765544167256, -0.010832150944943138, -0.01653511868336456, -0.011121302103373972, -0.42038514526905024, -0.22874576003118394, -0.16752936178907055, -0.17021699697722079, -0.09998584936787697, -0.09041117495322142, -0.10230248444795721, -0.15260897522094888, 0 .07770835838531896, -0.0813761125123066, 0 .027556910053932963, 0 .036305965104261866, 0 .03407793793894619, 0 .01212761779302579, 0 .006695133380685627, 0 .005331392748588556, -0.0 ] , [ 0 .008342680065996267, -0.00029249776150416367, 0 .002782130291086583, 0 .0027793744856745373, 0 .0020525102690845407, 0 .003679269934110004, 0 .009373846012918791, -0.0031751745946300403, -0.009042846256743316, 0 .0074141593032070775, -0.02796812516561052, -0.593171583786029, -0.4830164472795136, -0.353860128479443, -0.256482708704862, 0 .11515586314578445, 0 .12700563162828346, 0 .0022342450630152204, -0.24673707669992118, -0.012878340813781437, 0 .16866821780196756, 0 .009739033161051434, -0.000827843726513152, -0.0002137320694585577, -0.004179480126338929, 0 .008454049232317358, -0.002767934266266998, -0.0 ] , [ 0 .007070382982749552, 0 .005342127805750565, -0.000983984198542354, 0 .007910101170274493, 0 .001266267696096404, 0 .0038575136843053844, 0 .006941130321773131, -0.015195182020687892, -0.016954974010578504, -0.031186444096787943, -0.031754626467747966, 0 .038918845112017694, 0 .06248943950328597, 0 .07703301092601872, 0 .0438493628024275, -0.0482404449771698, -0.08718650815999045, -0.0014764704694506415, -0.07426336448916614, -0.10378029666564882, 0 .008572087846793842, -0.00017173413848283343, 0 .010058893270893113, 0 .0028410498666004377, 0 .002008290211806285, 0 .011905375389931099, 0 .006071375802943992, -0.0 ] , [ 0 .0076080165949142685, -0.0017127333725310495, 0 .00153128150106188, 0 .0033391793764531563, 0 .005373442509691564, 0 .007207746020295443, 0 .007422946703693544, -0.00699779191449194, 0 .002395328253696969, -0.011682618874195954, -0.012737004464649057, -0.05379966383523857, -0.07174960461749053, -0.03027341304050314, 0 .0019411862216381327, -0.0205575129473766, -0.04617091711614171, -0.017655308106959804, -0.009297162816368814, -0.03358572117988279, -0.1626068444778013, -0.015874364762085157, -0.0013736074085577258, -0.014763439328689378, 0 .00631805792697278, 0 .0021769414283267273, 0 .0023061635006792498, -0.0 ] , [ 0 .005569931813561535, 0 .004363218328087518, 0 .00025609463218383973, 0 .009577483244680675, 0 .007257755916229399, 0 .00976284778532342, -0.006388840235419147, -0.009017880790555707, -0.015308709334434867, -0.016743935775597355, -0.04372596546189275, -0.03523469356755156, -0.017257810114846107, 0 .011960489902313411, 0 .01529079831828911, -0.020076559119468443, -0.042792547669901516, -0.0029492027218867116, -0.011109560582516062, -0.12985858077848939, -0.2262858575494602, -0.003391725540087574, -0.03063368684328981, -0.01353486587575121, 0 .0011140822443932317, 0 .006583451102528798, 0 .005667533945285076, -0.0 ] , [ 0 .004056272267155598, -0.0006394041203204911, 0 .004664893926197093, 0 .010593032387298614, 0 .014750931538689989, 0 .015428721146282149, 0 .012167820222401367, 0 .017604752451202518, 0 .01038886849969188, 0 .020544326931163263, -0.0004206566917812794, -0.0037463581359232674, -0.0024656693040735075, 0 .0026061897697624353, -0.05186055271869177, -0.09158655048397382, 0 .022976389912563913, -0.19851635458461808, -0.11801281807622972, -0.29127727790584423, -0.017138655663803876, -0.04395515676468641, -0.019241432506341576, 0 .0011342298743447392, 0 .0030625771422964584, -0.0002867924892991192, -0.0017908808807543712, -0.0 ] , [ 0 .0030114260660488892, 0 .0020246448273580006, -0.003293361220376816, 0 .0036965043883218584, 0 .00013185761728146236, -0.004355610866966878, -0.006432601921104354, -0.004148701459814858, 0 .005974553907915845, -0.0001399233607281906, 0 .010392944122965082, 0 .015693249298693028, 0 .0459528427528407, -0.013921539948093455, -0.06615556518538708, 0 .02921438991320325, -0.16345220625101778, -0.002130491295590408, -0.11449749664916867, -0.030980255589300607, -0.04804122537359171, -0.05144994776295644, 0 .005122827412776085, 0 .006464862173908011, 0 .008624278272940246, 0 .0037316228508156427, 0 .0036947794337026706, -0.0 ] , [ 0 .0038173843228389405, -0.0017091931226819494, -0.0030871869816778068, 0 .002115642501535999, -0.006926441921580917, -0.003023077828426468, -0.014451359520861637, -0.0020793048380231397, -0.010948003939342523, -0.0014460716966395166, -0.01656990336897737, 0 .003052317148320358, -0.0026729564809943513, -0.06360067057346147, 0 .07780985635080599, -0.1436689936630281, -0.040817177623437874, -0.04373367754296477, -0.18337299150349698, 0 .025295182977407064, -0.03874921104331938, -0.002353901742617205, 0 .011772560401335033, 0 .012480994515707569, 0 .006498422579824301, 0 .00632320984076023, 0 .003407169765754805, -0.0 ] , [ 0 .00944355257990139, 0 .009242583578688485, 0 .005069860444386138, 0 .012666191449103024, 0 .00941789912565746, 0 .004720427012836104, 0 .007597687789204113, 0 .008679266528089945, 0 .00889322771021875, -0.0008577904940828809, 0 .0022973860384607604, 0 .025328230809207493, -0.09908781123080951, -0.07836626399832172, -0.1546141264726177, -0.2582207272050766, -0.2297524599578219, -0.29561835103416967, 0 .12048787956671528, -0.06279365699861471, -0.03832012404275233, 0 .022910264999199934, 0 .005803508497672737, -0.003858461926053348, 0 .0039451232171312765, 0 .003858476747495933, 0 .0013034515558609956, -0.0 ] , [ 0 .009725756015628606, -0.0004001101998876524, 0 .006490722835571152, 0 .00800808023631959, 0 .0065880711806331265, -0.0010264326176194034, -0.0018914305972878344, -0.008822522194658438, -0.016650520788128117, -0.03254382594389507, -0.014795713101569494, -0.05826499837818885, -0.05165369567511702, -0.13384277337594377, -0.22572641373340493, -0.21584739544668635, -0.2366836351939208, 0 .14937824076489659, -0.08127414932170171, -0.06720440139736879, -0.0038552732903526744, 0 .0107597891707803, -5.67453590118174e-05, 0 .0020161340511396244, -0.000783322694907436, -0.0006397207517995289, -0.005291639205010064, -0.0 ] , [ 0 .008627543242777584, 0 .007700097300051849, 0 .0020430960246806138, 0 .012949015733198586, 0 .008428709579953574, 0 .001358177022953576, 0 .00421863939925833, 0 .002657580000868709, -0.007339431957237175, 0 .02008439775442315, -0.0033717631758033114, -0.05176633249899187, -0.013790328758662772, -0.39102366157050594, -0.167341447585844, -0.04813367828213947, 0 .1367781582239039, -0.04672809260566293, -0.03237784669978756, 0 .03218068777925178, 0 .02415063765016493, -0.017849899351200002, -0.002975675228088795, -0.004819438014786686, 0 .005106898651831245, 0 .0024278620704227456, 6 .784303333368138e-05, -0.0 ] , [ 0 .009644258527009343, -0.001331907219439711, -0.0014639718434477777, 0 .008481926798958248, 0 .010278031715467508, 0 .003625808326891529, -0.01121188617599796, -0.0010634587872994379, -0.0002603820881968461, -0.017985648016990465, -0.06446652745470374, 0 .07726063173046191, -0.24739929795334742, -0.2701855018480216, -0.08888614776216278, 0 .1373325760136816, -0.02316068912438066, -0.042164834956711514, 0 .0009266091344106458, 0 .03141872420427644, 0 .011587728430225652, 0 .0004755143243520787, 0 .005860642609620605, 0 .008979633931394438, 0 .005061734169974005, 0 .003932710387086098, 0 .0015489986106803626, -0.0 ] , [ 0 .010998736164377534, 0 .009378969800902604, 0 .00030577045264713074, 0 .0159329353530375, 0 .014849508018911006, -0.0026513365659554225, 0 .002923303082126996, 0 .01917908707828847, -0.02338288107991566, -0.05706674679291175, 0 .009526265752669624, -0.19945255386401284, -0.10725519695909647, -0.3222906835083537, -0.03857038318412844, -0.013279804965996065, -0.046626023244262085, -0.029299060237210447, -0.043269580558906555, -0.03768510002290657, -0.02255977771908117, -0.02632588166863199, -0.014417349488098566, -0.003077271951572957, -0.0004973277708010661, 0 .0003475839139671271, -0.0014522783025903258, -0.0 ] , [ 0 .012215315671616316, -0.001693194176229889, 0 .011365785434529038, 0 .0036964574178487792, -0.010126738168635003, -0.025554378647710443, 0 .006538003839811914, -0.03181759044467965, -0.016424751042854728, 0 .06177539736110035, -0.43801735323216856, -0.29991040815937386, -0.2516019795363623, 0 .037789523540809, -0.010948746374759491, -0.0633901687126727, -0.005976006160777705, 0 .006035133605976937, -0.04961632526071937, -0.04142116972831476, -0.07558952727782252, -0.04165176179187153, -0.02021603856619006, -0.0027365663096057032, -0.011145473712733575, 0 .0003566937349350848, -0.00546472985268321, -0.0 ] , [ 0 .008009386447317503, 0 .006831207743885825, 0 .0051306149795546365, 0 .016239014770865052, 0 .020925441734273218, 0 .028344800173195076, -0.004805080609285047, -0.01880521614501033, -0.1272329010865855, -0.39835936819190537, -0.09113694760349819, -0.04061591094832608, -0.12677021961235907, 0 .015567707226741051, -0.005615051546243333, -0.06454044862001587, 0 .0195457674752272, -0.04219686517155871, -0.08060569979524296, 0 .027234494361702787, -0.009152881336047056, -0.030865118003992217, -0.005770311060090559, 0 .002905833371986098, 5 .606663556872091e-05, 0 .003209538083839772, -0.0018588810743365345, -0.0 ] , [ 0 .007587008852984699, -0.0021213639853557625, 0 .0007709558092903736, 0 .013883256128746423, 0 .017328713012428214, 0 .03645357525636198, -0.04043993335238427, 0 .05730125171252314, -0.2563293727512057, -0.11438826083879326, 0 .02662382809034687, 0 .03525271352483709, 0 .04745678120172762, 0 .0336360484090392, -0.002916635707204059, -0.17950855098650784, -0.44161773297052964, -0.4512180227831197, -0.4940283106297913, -0.1970108671285798, 0 .04344323143078066, -0.012005120444897523, 0 .00987576109166055, -0.0018336757466252476, 0 .0004913959502151706, -0.0005409724034216215, -0.005039223900868212, -0.0 ] , [ 0 .00637876531169957, 0 .005189469227685454, 0 .0007676355246000376, 0 .018378100865097655, 0 .015739815031394887, -0.035524983116512455, 0 .03781006978038308, 0 .28859052096740495, 0 .0726464110153121, -0.026768468497420147, 0 .06278766200288134, 0 .17897045813699355, -0.13780371920803108, -0.14176458123649577, -0.1733103177731656, -0.3106508869296763, 0 .04788355140275794, 0 .04235327890285105, -0.031266625292514394, -0.016263819217960652, -0.031388328800811355, -0.01791363975905968, -0.012025067979443894, 0 .008335083985905805, -0.0014386677797296231, 0 .0055376544652972854, 0 .002241522815466253, -0.0 ] , [ 0 .007455256326741617, -0.0009475207572210404, 0 .0020288385162615286, 0 .015399640135796092, 0 .021133843188103074, -0.019846405097622234, -0.003162485751163173, -0.14199005055318842, -0.044200898667146035, -0.013395459413208084, 0 .11019680479230103, -0.014057216041764874, -0.12553853334447865, -0.05992513534766256, 0 .06467942189539834, 0 .08866056095907732, -0.1451321508061849, -0.07382491447758655, -0.046961739981080476, 0 .0008943713493160624, 0 .03231044103656507, 0 .00036034241706501196, -0.011387669277619417, -0.00014602449257226195, -0.0021863729003374116, 0 .0018817840156005856, 0 .0037909804578166286, -0.0 ] , [ 0 .006511855618626698, 0 .006236866054439829, -0.001440571166157676, 0 .012795776609942026, 0 .011530545030403624, 0 .03495489377257363, 0 .04792403136095304, 0 .049378583599065225, 0 .03296101702085617, -0.0005351385876652296, 0 .017744115897640366, 0 .0011656622496764954, 0 .0232845869823761, -0.0561191397060232, -0.02854070511118366, -0.028614174047247348, -0.007763531086362863, 0 .01823079560098924, 0 .021961392405283622, -0.009666681805706179, 0 .009547046884328725, -0.008729943263791338, 0 .006408909680578429, 0 .009794327096359952, -0.0025825219195515304, 0 .007063559189211571, 0 .007867244119267047, -0.0 ] , [ 0 .007936663546039311, -0.00010710180170593153, 0 .002716512705673228, 0 .0038633557307721487, -0.0014877316616940372, -0.0004788143065635909, 0 .012508842248031202, 0 .0045381104608414645, -0.010650910516128294, -0.013785341529644855, -0.034287643221318206, -0.022152707546335495, -0.047056481347685974, -0.032166744564720455, -0.021551611335278546, -0.002174962503376043, 0 .024344287130424306, 0 .015579272560525105, 0 .010958169741952194, -0.010607232913436921, -0.005548369726118836, -0.0014630046444242706, 0 .013144180105016433, 0 .0031349366359021916, 0 .0010984887428255974, 0 .005426941473328394, 0 .006566511860044785, -0.0 ] , [ 0 .0005529184874606495, 0 .00026139355020588705, -0.002887623443531047, 0 .0013988462990850632, 0 .00203365139495493, -0.007276926701775218, -0.004010419939595932, 0 .017521952161185662, 0 .0006996977433557911, 0 .02083134683611201, 0 .013690533534289498, -0.005466724359976675, -0.008857712321334327, 0 .017408578822635818, 0 .0076439343049154425, 0 .0017861314923539985, 0 .007465865707523924, 0 .008034420825988495, 0 .003976298558337994, 0 .00411970637898539, -0.004572592545819698, 0 .0029563907011979935, -0.0006382227820088148, 0 .0015153753877889707, -0.0052626601797995595, 0 .0025664706985019416, 0 .005161751034260073, -0.0 ] , [ 0 .0009424280561998445, -0.0012942360298110595, 0 .0011900868416523343, 0 .000984424113178899, 0 .0020988269382781564, -0.005870080062890889, -0.004950484744457169, 0 .003117643454332697, -0.002509563565777083, 0 .005831604884101081, 0 .009531085216183116, 0 .010030206821909806, 0 .005858190171099734, 4 .9344529936340524e-05, -0.004027895832421331, 0 .0025436439920587606, 0 .00531153867563076, 0 .00495942692369508, 0 .009215148318606382, 0 .00010011928* Connection #0 to host a64b698726695486693928d4bd795ffa-152408018.us-west-2.elb.amazonaws.com left intact 317543458 , 0 .0060051362999805355, -0.0008195376963202741, 0 .0041728603512658224, -0.0017597169567888774, -0.0010577007775543158, 0 .00046033327178068433, -0.0007674196306044449, -0.0 ] , [ -0.0, -0.0, 0 .0013386963856532302, 0 .00035183178922260837, 0 .0030610334903526204, 8 .951834979315781e-05, 0 .0023676793550483524, -0.0002900551076915047, -0.00207019445286608, -7.61697478482574e-05, 0 .0012150086715244216, 0 .009831239281792168, 0 .003479667642621962, 0 .0070584324334114525, 0 .004161851261339585, 0 .0026146296354490665, -9.194746959222099e-05, 0 .0013583866966571571, 0 .0016821551239318913, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0 ]]]]} Autoscaling \u00b6 One of the main serverless inference features is to automatically scale the replicas of an InferenceService matching the incoming workload. KServe by default enables Knative Pod Autoscaler which watches traffic flow and scales up and down based on the configured metrics. Autoscaling Example Canary Rollout \u00b6 Canary rollout is a deployment strategy when you release a new version of model to a small percent of the production traffic. Canary Deployment Monitoring \u00b6 Expose metrics and setup grafana dashboards","title":"PyTorch"},{"location":"modelserving/v1beta1/torchserve/#deploy-pytorch-model-with-torchserve-inferenceservice","text":"In this example, we use a trained pytorch mnist model to predict handwritten digits by running an inference service with TorchServe predictor.","title":"Deploy PyTorch model with TorchServe InferenceService"},{"location":"modelserving/v1beta1/torchserve/#creating-model-storage-with-model-archive-file","text":"TorchServe provides a utility to package all the model artifacts into a single Torchserve Model Archive Files (MAR) . You can store your model and dependent files on remote storage or local persistent volume, the mnist model and dependent files can be obtained from here . The KServe/TorchServe integration expects following model store layout. \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 config.properties \u251c\u2500\u2500 model-store \u2502 \u251c\u2500\u2500 densenet_161.mar \u2502 \u251c\u2500\u2500 mnist.mar Note For remote storage you can choose to start the example using the prebuilt mnist MAR file stored on KServe example GCS bucket gs://kfserving-examples/models/torchserve/image_classifier , you can also generate the MAR file with torch-model-archiver and create the model store on remote storage according to the above layout. torch-model-archiver --model-name mnist --version 1 .0 \\ --model-file model-archiver/model-store/mnist/mnist.py \\ --serialized-file model-archiver/model-store/mnist/mnist_cnn.pt \\ --handler model-archiver/model-store/mnist/mnist_handler.py \\ For PVC user please refer to model archive file generation for auto generation of MAR files from the model and dependent files.","title":"Creating model storage with model archive file"},{"location":"modelserving/v1beta1/torchserve/#torchserve-with-kserve-envelope-inference-endpoints","text":"The KServe/TorchServe integration supports KServe v1 protocol and we are working on to support v2 protocol. API Verb Path Payload Predict POST /v1/models/ :predict Request:{\"instances\": []} Response:{\"predictions\": []} Explain POST /v1/models/ :explain Request:{\"instances\": []} Response:{\"predictions\": [], \"explainations\": []} Note The config.properties file includes the flag service_envelope=kfserving to enable the KServe inference protocol. The requests are converted from KServe inference request format to torchserve request format and sent to the inference_address configured via local socket. Sample requests for text and image classification","title":"TorchServe with KServe envelope inference endpoints"},{"location":"modelserving/v1beta1/torchserve/#create-the-inferenceservice","text":"For deploying the InferenceService on CPU apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier kubectl kubectl apply -f torchserve.yaml For deploying the InferenceService on GPU apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier resources : limits : memory : 4Gi nvidia.com/gpu : \"1\" kubectl kubectl apply -f gpu.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/torchserve/#inference","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torchserve -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) Use image converter to create input request for mnist. For other models please refer to input request curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist HTTP/1.1 > Host: torchserve.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 167 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Tue, 27 Oct 2020 08 :26:19 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517 < x-envoy-upstream-service-time: 6 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"predictions\" : [ \"2\" ]}","title":"Inference"},{"location":"modelserving/v1beta1/torchserve/#explanation","text":"Model interpretability is an important aspect which help to understand which of the input features were important for a particular classification. Captum is a model interpretability library, the KServe Explain Endpoint uses Captum's state-of-the-art algorithm, including integrated gradients to provide user with an easy way to understand which features are contributing to the model output. Your can refer to Captum Tutorial for more examples. curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/mnist:explain -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist:explain HTTP/1.1 > Host: torchserve.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 167 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Tue, 27 Oct 2020 08 :26:19 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517 < x-envoy-upstream-service-time: 6 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"explanations\" : [[[[ 0 .0005394675730469475, -0.0022280013123036043, -0.003416480100841055, -0.0051329881112415965, -0.009973864160829985, -0.004112560908882716, -0.009223458030656112, -0.0006676354577291628, -0.005249806664413386, -0.0009790519227372953, -0.0026914653993121195, -0.0069470097151383995, -0.00693530415962956, -0.005973878697847718, -0.00425042437288857, 0 .0032867281838150977, -0.004297780258633562, -0.005643196661192014, -0.00653025019738562, -0.0047062916121001185, -0.0018656628277792628, -0.0016757477204072532, -0.0010410417081844845, -0.0019093520822156726, -0.004451403461006374, -0.0008552767257773671, -0.0027638888169885267, -0.0 ] , [ 0 .006971297052106784, 0 .007316855222185687, 0 .012144494329150574, 0 .011477799383288441, 0 .006846725347670252, 0 .01149386176451476, 0 .0045351987881190655, 0 .007038361889638708, 0 .0035855377023272157, 0 .003031419502053957, -0.0008611575226775316, -0.0011085224745969223, -0.0050840743637658534, 0 .009855491784340777, 0 .007220680811043034, 0 .011374285598070253, 0 .007147725481709019, 0 .0037114580912849457, 0 .00030763245479291384, 0 .0018305492665953394, 0 .010106224395114147, 0 .012932881164284687, 0 .008862892007714321, 0 .0070960526615982435, -0.0015931137903787505, 0 .0036495747329455906, 0 .0002593849391051298, -0.0 ] , [ 0 .006467265785857396, -0.00041793201228071674, 0 .004900316089756856, 0 .002308395474823997, 0 .007859295399592283, 0 .003916404948969494, 0 .005630750246437249, 0 .0043712538044184375, 0 .006128530599133763, -0.009446321309831246, -0.014173645867037036, -0.0062988650915794565, -0.011473838941118539, -0.009049151947644047, -0.0007625645864610934, -0.013721416630061238, -0.0005580156670410108, 0 .0033404383756480784, -0.006693278798487951, -0.003705084551144756, 0 .005100375089529131, 5 .5276874714401074e-05, 0 .007221745280359063, -0.00573598303916232, -0.006836169033785967, 0 .0025401608627538936, 9 .303533912921196e-05, -0.0 ] , [ 0 .005914399808621816, 0 .00452643561023696, 0 .003968242261515448, 0 .010422786058967673, 0 .007728358107899074, 0 .01147115923288383, 0 .005683869479056691, 0 .011150670502307374, 0 .008742555292485278, 0 .0032882897575743754, 0 .014841138421861584, 0 .011741228362482451, 0 .0004296862879259221, -0.0035118140680654854, -0.006152254410078331, -0.004925121936901983, -2.3611205202801947e-06, 0 .029347073037039074, 0 .02901626308947743, 0 .023379353021343398, 0 .004027157620197582, -0.01677662249919171, -0.013497255736128979, 0 .006957482854214602, 0 .0018321766800746145, 0 .008277034396684563, 0 .002733405455464871, -0.0 ] , [ 0 .0049579739156640065, -0.002168016158233997, 0 .0020644317321723642, 0 .0020912464240293825, 0 .004719691119907336, 0 .007879231202446626, 0 .010594445898145937, 0 .006533067778982801, 0 .002290214592708113, -0.0036651114968251986, 0 .010753227423379443, 0 .006402706020466243, -0.047075193909339695, -0.08108259303568185, -0.07646875196692542, -0.1681834845371156, -0.1610307396135756, -0.12010309927453829, -0.016148831320070896, -0.009541525999486027, 0 .04575604594761406, 0 .031470966329886635, 0 .02452149438024385, 0 .016594078577569567, 0 .012213591301610382, -0.002230875840404426, 0 .0036704051254298374, -0.0 ] , [ 0 .006410107592414739, 0 .005578283890924384, 0 .001977103461731095, 0 .008935476507124939, 0 .0011305055729953436, 0 .0004946313900665659, -0.0040266029554395935, -0.004270765544167256, -0.010832150944943138, -0.01653511868336456, -0.011121302103373972, -0.42038514526905024, -0.22874576003118394, -0.16752936178907055, -0.17021699697722079, -0.09998584936787697, -0.09041117495322142, -0.10230248444795721, -0.15260897522094888, 0 .07770835838531896, -0.0813761125123066, 0 .027556910053932963, 0 .036305965104261866, 0 .03407793793894619, 0 .01212761779302579, 0 .006695133380685627, 0 .005331392748588556, -0.0 ] , [ 0 .008342680065996267, -0.00029249776150416367, 0 .002782130291086583, 0 .0027793744856745373, 0 .0020525102690845407, 0 .003679269934110004, 0 .009373846012918791, -0.0031751745946300403, -0.009042846256743316, 0 .0074141593032070775, -0.02796812516561052, -0.593171583786029, -0.4830164472795136, -0.353860128479443, -0.256482708704862, 0 .11515586314578445, 0 .12700563162828346, 0 .0022342450630152204, -0.24673707669992118, -0.012878340813781437, 0 .16866821780196756, 0 .009739033161051434, -0.000827843726513152, -0.0002137320694585577, -0.004179480126338929, 0 .008454049232317358, -0.002767934266266998, -0.0 ] , [ 0 .007070382982749552, 0 .005342127805750565, -0.000983984198542354, 0 .007910101170274493, 0 .001266267696096404, 0 .0038575136843053844, 0 .006941130321773131, -0.015195182020687892, -0.016954974010578504, -0.031186444096787943, -0.031754626467747966, 0 .038918845112017694, 0 .06248943950328597, 0 .07703301092601872, 0 .0438493628024275, -0.0482404449771698, -0.08718650815999045, -0.0014764704694506415, -0.07426336448916614, -0.10378029666564882, 0 .008572087846793842, -0.00017173413848283343, 0 .010058893270893113, 0 .0028410498666004377, 0 .002008290211806285, 0 .011905375389931099, 0 .006071375802943992, -0.0 ] , [ 0 .0076080165949142685, -0.0017127333725310495, 0 .00153128150106188, 0 .0033391793764531563, 0 .005373442509691564, 0 .007207746020295443, 0 .007422946703693544, -0.00699779191449194, 0 .002395328253696969, -0.011682618874195954, -0.012737004464649057, -0.05379966383523857, -0.07174960461749053, -0.03027341304050314, 0 .0019411862216381327, -0.0205575129473766, -0.04617091711614171, -0.017655308106959804, -0.009297162816368814, -0.03358572117988279, -0.1626068444778013, -0.015874364762085157, -0.0013736074085577258, -0.014763439328689378, 0 .00631805792697278, 0 .0021769414283267273, 0 .0023061635006792498, -0.0 ] , [ 0 .005569931813561535, 0 .004363218328087518, 0 .00025609463218383973, 0 .009577483244680675, 0 .007257755916229399, 0 .00976284778532342, -0.006388840235419147, -0.009017880790555707, -0.015308709334434867, -0.016743935775597355, -0.04372596546189275, -0.03523469356755156, -0.017257810114846107, 0 .011960489902313411, 0 .01529079831828911, -0.020076559119468443, -0.042792547669901516, -0.0029492027218867116, -0.011109560582516062, -0.12985858077848939, -0.2262858575494602, -0.003391725540087574, -0.03063368684328981, -0.01353486587575121, 0 .0011140822443932317, 0 .006583451102528798, 0 .005667533945285076, -0.0 ] , [ 0 .004056272267155598, -0.0006394041203204911, 0 .004664893926197093, 0 .010593032387298614, 0 .014750931538689989, 0 .015428721146282149, 0 .012167820222401367, 0 .017604752451202518, 0 .01038886849969188, 0 .020544326931163263, -0.0004206566917812794, -0.0037463581359232674, -0.0024656693040735075, 0 .0026061897697624353, -0.05186055271869177, -0.09158655048397382, 0 .022976389912563913, -0.19851635458461808, -0.11801281807622972, -0.29127727790584423, -0.017138655663803876, -0.04395515676468641, -0.019241432506341576, 0 .0011342298743447392, 0 .0030625771422964584, -0.0002867924892991192, -0.0017908808807543712, -0.0 ] , [ 0 .0030114260660488892, 0 .0020246448273580006, -0.003293361220376816, 0 .0036965043883218584, 0 .00013185761728146236, -0.004355610866966878, -0.006432601921104354, -0.004148701459814858, 0 .005974553907915845, -0.0001399233607281906, 0 .010392944122965082, 0 .015693249298693028, 0 .0459528427528407, -0.013921539948093455, -0.06615556518538708, 0 .02921438991320325, -0.16345220625101778, -0.002130491295590408, -0.11449749664916867, -0.030980255589300607, -0.04804122537359171, -0.05144994776295644, 0 .005122827412776085, 0 .006464862173908011, 0 .008624278272940246, 0 .0037316228508156427, 0 .0036947794337026706, -0.0 ] , [ 0 .0038173843228389405, -0.0017091931226819494, -0.0030871869816778068, 0 .002115642501535999, -0.006926441921580917, -0.003023077828426468, -0.014451359520861637, -0.0020793048380231397, -0.010948003939342523, -0.0014460716966395166, -0.01656990336897737, 0 .003052317148320358, -0.0026729564809943513, -0.06360067057346147, 0 .07780985635080599, -0.1436689936630281, -0.040817177623437874, -0.04373367754296477, -0.18337299150349698, 0 .025295182977407064, -0.03874921104331938, -0.002353901742617205, 0 .011772560401335033, 0 .012480994515707569, 0 .006498422579824301, 0 .00632320984076023, 0 .003407169765754805, -0.0 ] , [ 0 .00944355257990139, 0 .009242583578688485, 0 .005069860444386138, 0 .012666191449103024, 0 .00941789912565746, 0 .004720427012836104, 0 .007597687789204113, 0 .008679266528089945, 0 .00889322771021875, -0.0008577904940828809, 0 .0022973860384607604, 0 .025328230809207493, -0.09908781123080951, -0.07836626399832172, -0.1546141264726177, -0.2582207272050766, -0.2297524599578219, -0.29561835103416967, 0 .12048787956671528, -0.06279365699861471, -0.03832012404275233, 0 .022910264999199934, 0 .005803508497672737, -0.003858461926053348, 0 .0039451232171312765, 0 .003858476747495933, 0 .0013034515558609956, -0.0 ] , [ 0 .009725756015628606, -0.0004001101998876524, 0 .006490722835571152, 0 .00800808023631959, 0 .0065880711806331265, -0.0010264326176194034, -0.0018914305972878344, -0.008822522194658438, -0.016650520788128117, -0.03254382594389507, -0.014795713101569494, -0.05826499837818885, -0.05165369567511702, -0.13384277337594377, -0.22572641373340493, -0.21584739544668635, -0.2366836351939208, 0 .14937824076489659, -0.08127414932170171, -0.06720440139736879, -0.0038552732903526744, 0 .0107597891707803, -5.67453590118174e-05, 0 .0020161340511396244, -0.000783322694907436, -0.0006397207517995289, -0.005291639205010064, -0.0 ] , [ 0 .008627543242777584, 0 .007700097300051849, 0 .0020430960246806138, 0 .012949015733198586, 0 .008428709579953574, 0 .001358177022953576, 0 .00421863939925833, 0 .002657580000868709, -0.007339431957237175, 0 .02008439775442315, -0.0033717631758033114, -0.05176633249899187, -0.013790328758662772, -0.39102366157050594, -0.167341447585844, -0.04813367828213947, 0 .1367781582239039, -0.04672809260566293, -0.03237784669978756, 0 .03218068777925178, 0 .02415063765016493, -0.017849899351200002, -0.002975675228088795, -0.004819438014786686, 0 .005106898651831245, 0 .0024278620704227456, 6 .784303333368138e-05, -0.0 ] , [ 0 .009644258527009343, -0.001331907219439711, -0.0014639718434477777, 0 .008481926798958248, 0 .010278031715467508, 0 .003625808326891529, -0.01121188617599796, -0.0010634587872994379, -0.0002603820881968461, -0.017985648016990465, -0.06446652745470374, 0 .07726063173046191, -0.24739929795334742, -0.2701855018480216, -0.08888614776216278, 0 .1373325760136816, -0.02316068912438066, -0.042164834956711514, 0 .0009266091344106458, 0 .03141872420427644, 0 .011587728430225652, 0 .0004755143243520787, 0 .005860642609620605, 0 .008979633931394438, 0 .005061734169974005, 0 .003932710387086098, 0 .0015489986106803626, -0.0 ] , [ 0 .010998736164377534, 0 .009378969800902604, 0 .00030577045264713074, 0 .0159329353530375, 0 .014849508018911006, -0.0026513365659554225, 0 .002923303082126996, 0 .01917908707828847, -0.02338288107991566, -0.05706674679291175, 0 .009526265752669624, -0.19945255386401284, -0.10725519695909647, -0.3222906835083537, -0.03857038318412844, -0.013279804965996065, -0.046626023244262085, -0.029299060237210447, -0.043269580558906555, -0.03768510002290657, -0.02255977771908117, -0.02632588166863199, -0.014417349488098566, -0.003077271951572957, -0.0004973277708010661, 0 .0003475839139671271, -0.0014522783025903258, -0.0 ] , [ 0 .012215315671616316, -0.001693194176229889, 0 .011365785434529038, 0 .0036964574178487792, -0.010126738168635003, -0.025554378647710443, 0 .006538003839811914, -0.03181759044467965, -0.016424751042854728, 0 .06177539736110035, -0.43801735323216856, -0.29991040815937386, -0.2516019795363623, 0 .037789523540809, -0.010948746374759491, -0.0633901687126727, -0.005976006160777705, 0 .006035133605976937, -0.04961632526071937, -0.04142116972831476, -0.07558952727782252, -0.04165176179187153, -0.02021603856619006, -0.0027365663096057032, -0.011145473712733575, 0 .0003566937349350848, -0.00546472985268321, -0.0 ] , [ 0 .008009386447317503, 0 .006831207743885825, 0 .0051306149795546365, 0 .016239014770865052, 0 .020925441734273218, 0 .028344800173195076, -0.004805080609285047, -0.01880521614501033, -0.1272329010865855, -0.39835936819190537, -0.09113694760349819, -0.04061591094832608, -0.12677021961235907, 0 .015567707226741051, -0.005615051546243333, -0.06454044862001587, 0 .0195457674752272, -0.04219686517155871, -0.08060569979524296, 0 .027234494361702787, -0.009152881336047056, -0.030865118003992217, -0.005770311060090559, 0 .002905833371986098, 5 .606663556872091e-05, 0 .003209538083839772, -0.0018588810743365345, -0.0 ] , [ 0 .007587008852984699, -0.0021213639853557625, 0 .0007709558092903736, 0 .013883256128746423, 0 .017328713012428214, 0 .03645357525636198, -0.04043993335238427, 0 .05730125171252314, -0.2563293727512057, -0.11438826083879326, 0 .02662382809034687, 0 .03525271352483709, 0 .04745678120172762, 0 .0336360484090392, -0.002916635707204059, -0.17950855098650784, -0.44161773297052964, -0.4512180227831197, -0.4940283106297913, -0.1970108671285798, 0 .04344323143078066, -0.012005120444897523, 0 .00987576109166055, -0.0018336757466252476, 0 .0004913959502151706, -0.0005409724034216215, -0.005039223900868212, -0.0 ] , [ 0 .00637876531169957, 0 .005189469227685454, 0 .0007676355246000376, 0 .018378100865097655, 0 .015739815031394887, -0.035524983116512455, 0 .03781006978038308, 0 .28859052096740495, 0 .0726464110153121, -0.026768468497420147, 0 .06278766200288134, 0 .17897045813699355, -0.13780371920803108, -0.14176458123649577, -0.1733103177731656, -0.3106508869296763, 0 .04788355140275794, 0 .04235327890285105, -0.031266625292514394, -0.016263819217960652, -0.031388328800811355, -0.01791363975905968, -0.012025067979443894, 0 .008335083985905805, -0.0014386677797296231, 0 .0055376544652972854, 0 .002241522815466253, -0.0 ] , [ 0 .007455256326741617, -0.0009475207572210404, 0 .0020288385162615286, 0 .015399640135796092, 0 .021133843188103074, -0.019846405097622234, -0.003162485751163173, -0.14199005055318842, -0.044200898667146035, -0.013395459413208084, 0 .11019680479230103, -0.014057216041764874, -0.12553853334447865, -0.05992513534766256, 0 .06467942189539834, 0 .08866056095907732, -0.1451321508061849, -0.07382491447758655, -0.046961739981080476, 0 .0008943713493160624, 0 .03231044103656507, 0 .00036034241706501196, -0.011387669277619417, -0.00014602449257226195, -0.0021863729003374116, 0 .0018817840156005856, 0 .0037909804578166286, -0.0 ] , [ 0 .006511855618626698, 0 .006236866054439829, -0.001440571166157676, 0 .012795776609942026, 0 .011530545030403624, 0 .03495489377257363, 0 .04792403136095304, 0 .049378583599065225, 0 .03296101702085617, -0.0005351385876652296, 0 .017744115897640366, 0 .0011656622496764954, 0 .0232845869823761, -0.0561191397060232, -0.02854070511118366, -0.028614174047247348, -0.007763531086362863, 0 .01823079560098924, 0 .021961392405283622, -0.009666681805706179, 0 .009547046884328725, -0.008729943263791338, 0 .006408909680578429, 0 .009794327096359952, -0.0025825219195515304, 0 .007063559189211571, 0 .007867244119267047, -0.0 ] , [ 0 .007936663546039311, -0.00010710180170593153, 0 .002716512705673228, 0 .0038633557307721487, -0.0014877316616940372, -0.0004788143065635909, 0 .012508842248031202, 0 .0045381104608414645, -0.010650910516128294, -0.013785341529644855, -0.034287643221318206, -0.022152707546335495, -0.047056481347685974, -0.032166744564720455, -0.021551611335278546, -0.002174962503376043, 0 .024344287130424306, 0 .015579272560525105, 0 .010958169741952194, -0.010607232913436921, -0.005548369726118836, -0.0014630046444242706, 0 .013144180105016433, 0 .0031349366359021916, 0 .0010984887428255974, 0 .005426941473328394, 0 .006566511860044785, -0.0 ] , [ 0 .0005529184874606495, 0 .00026139355020588705, -0.002887623443531047, 0 .0013988462990850632, 0 .00203365139495493, -0.007276926701775218, -0.004010419939595932, 0 .017521952161185662, 0 .0006996977433557911, 0 .02083134683611201, 0 .013690533534289498, -0.005466724359976675, -0.008857712321334327, 0 .017408578822635818, 0 .0076439343049154425, 0 .0017861314923539985, 0 .007465865707523924, 0 .008034420825988495, 0 .003976298558337994, 0 .00411970637898539, -0.004572592545819698, 0 .0029563907011979935, -0.0006382227820088148, 0 .0015153753877889707, -0.0052626601797995595, 0 .0025664706985019416, 0 .005161751034260073, -0.0 ] , [ 0 .0009424280561998445, -0.0012942360298110595, 0 .0011900868416523343, 0 .000984424113178899, 0 .0020988269382781564, -0.005870080062890889, -0.004950484744457169, 0 .003117643454332697, -0.002509563565777083, 0 .005831604884101081, 0 .009531085216183116, 0 .010030206821909806, 0 .005858190171099734, 4 .9344529936340524e-05, -0.004027895832421331, 0 .0025436439920587606, 0 .00531153867563076, 0 .00495942692369508, 0 .009215148318606382, 0 .00010011928* Connection #0 to host a64b698726695486693928d4bd795ffa-152408018.us-west-2.elb.amazonaws.com left intact 317543458 , 0 .0060051362999805355, -0.0008195376963202741, 0 .0041728603512658224, -0.0017597169567888774, -0.0010577007775543158, 0 .00046033327178068433, -0.0007674196306044449, -0.0 ] , [ -0.0, -0.0, 0 .0013386963856532302, 0 .00035183178922260837, 0 .0030610334903526204, 8 .951834979315781e-05, 0 .0023676793550483524, -0.0002900551076915047, -0.00207019445286608, -7.61697478482574e-05, 0 .0012150086715244216, 0 .009831239281792168, 0 .003479667642621962, 0 .0070584324334114525, 0 .004161851261339585, 0 .0026146296354490665, -9.194746959222099e-05, 0 .0013583866966571571, 0 .0016821551239318913, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0 ]]]]}","title":"Explanation"},{"location":"modelserving/v1beta1/torchserve/#autoscaling","text":"One of the main serverless inference features is to automatically scale the replicas of an InferenceService matching the incoming workload. KServe by default enables Knative Pod Autoscaler which watches traffic flow and scales up and down based on the configured metrics. Autoscaling Example","title":"Autoscaling"},{"location":"modelserving/v1beta1/torchserve/#canary-rollout","text":"Canary rollout is a deployment strategy when you release a new version of model to a small percent of the production traffic. Canary Deployment","title":"Canary Rollout"},{"location":"modelserving/v1beta1/torchserve/#monitoring","text":"Expose metrics and setup grafana dashboards","title":"Monitoring"},{"location":"modelserving/v1beta1/torchserve/autoscaling/","text":"Autoscaling \u00b6 KServe supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes\u2019 Horizontal Pod Autoscaler (HPA). The features and limitations of each of these Autoscalers are listed below. IMPORTANT: If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install HPA extension after you install Knative Serving. Knative Pod Autoscaler (KPA) - Part of the Knative Serving core and enabled by default once Knative Serving is installed. - Supports scale to zero functionality. - Does not support CPU-based autoscaling. Horizontal Pod Autoscaler (HPA) - Not part of the Knative Serving core, and must be enabled after Knative Serving installation. - Does not support scale to zero functionality. - Supports CPU-based autoscaling. Create InferenceService with concurrency target \u00b6 Soft limit \u00b6 You can configure InferenceService with annotation autoscaling.knative.dev/target for a soft limit. The soft limit is a targeted limit rather than a strictly enforced bound, particularly if there is a sudden burst of requests, this value can be exceeded. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" annotations : autoscaling.knative.dev/target : \"10\" spec : predictor : pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier\" Hard limit \u00b6 You can also configure InferenceService with field containerConcurrency for a hard limit. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : containerConcurrency : 10 pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier\" Create the InferenceService \u00b6 kubectl apply -f torchserve.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve created Run inference with concurrent requests \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT Install hey load generator go get -u github.com/rakyll/hey Send concurrent inference requests MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torchserve -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) ./hey -m POST -z 30s -D ./mnist.json -host ${ SERVICE_HOSTNAME } http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict Check the pods that are scaled up \u00b6 hey by default generates 50 requests concurrently, so you can see that the InferenceService scales to 5 pods as the container concurrency target is 10. kubectl get pods -n kserve-test NAME READY STATUS RESTARTS AGE torchserve-predictor-default-cj2d8-deployment-69444c9c74-67qwb 2 /2 Terminating 0 103s torchserve-predictor-default-cj2d8-deployment-69444c9c74-nnxk8 2 /2 Terminating 0 95s torchserve-predictor-default-cj2d8-deployment-69444c9c74-rq8jq 2 /2 Running 0 50m torchserve-predictor-default-cj2d8-deployment-69444c9c74-tsrwr 2 /2 Running 0 113s torchserve-predictor-default-cj2d8-deployment-69444c9c74-vvpjl 2 /2 Running 0 109s torchserve-predictor-default-cj2d8-deployment-69444c9c74-xvn7t 2 /2 Terminating 0 103s","title":"Autoscaling"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#autoscaling","text":"KServe supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes\u2019 Horizontal Pod Autoscaler (HPA). The features and limitations of each of these Autoscalers are listed below. IMPORTANT: If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install HPA extension after you install Knative Serving. Knative Pod Autoscaler (KPA) - Part of the Knative Serving core and enabled by default once Knative Serving is installed. - Supports scale to zero functionality. - Does not support CPU-based autoscaling. Horizontal Pod Autoscaler (HPA) - Not part of the Knative Serving core, and must be enabled after Knative Serving installation. - Does not support scale to zero functionality. - Supports CPU-based autoscaling.","title":"Autoscaling"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#create-inferenceservice-with-concurrency-target","text":"","title":"Create InferenceService with concurrency target"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#soft-limit","text":"You can configure InferenceService with annotation autoscaling.knative.dev/target for a soft limit. The soft limit is a targeted limit rather than a strictly enforced bound, particularly if there is a sudden burst of requests, this value can be exceeded. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" annotations : autoscaling.knative.dev/target : \"10\" spec : predictor : pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier\"","title":"Soft limit"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#hard-limit","text":"You can also configure InferenceService with field containerConcurrency for a hard limit. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : containerConcurrency : 10 pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier\"","title":"Hard limit"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#create-the-inferenceservice","text":"kubectl apply -f torchserve.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#run-inference-with-concurrent-requests","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT Install hey load generator go get -u github.com/rakyll/hey Send concurrent inference requests MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torchserve -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) ./hey -m POST -z 30s -D ./mnist.json -host ${ SERVICE_HOSTNAME } http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict","title":"Run inference with concurrent requests"},{"location":"modelserving/v1beta1/torchserve/autoscaling/#check-the-pods-that-are-scaled-up","text":"hey by default generates 50 requests concurrently, so you can see that the InferenceService scales to 5 pods as the container concurrency target is 10. kubectl get pods -n kserve-test NAME READY STATUS RESTARTS AGE torchserve-predictor-default-cj2d8-deployment-69444c9c74-67qwb 2 /2 Terminating 0 103s torchserve-predictor-default-cj2d8-deployment-69444c9c74-nnxk8 2 /2 Terminating 0 95s torchserve-predictor-default-cj2d8-deployment-69444c9c74-rq8jq 2 /2 Running 0 50m torchserve-predictor-default-cj2d8-deployment-69444c9c74-tsrwr 2 /2 Running 0 113s torchserve-predictor-default-cj2d8-deployment-69444c9c74-vvpjl 2 /2 Running 0 109s torchserve-predictor-default-cj2d8-deployment-69444c9c74-xvn7t 2 /2 Terminating 0 103s","title":"Check the pods that are scaled up"},{"location":"modelserving/v1beta1/torchserve/bert/","text":"TorchServe example with Huggingface bert model \u00b6 In this example we will show how to serve Huggingface Transformers with TorchServe on KServe. Model archive file creation \u00b6 Clone pytorch/serve repository, navigate to examples/Huggingface_Transformers and follow the steps for creating the MAR file including serialized model and other dependent files. TorchServe supports both eager model and torchscript and here we save as the pretrained model. torch-model-archiver --model-name BERTSeqClassification --version 1 .0 \\ --serialized-file Transformer_model/pytorch_model.bin \\ --handler ./Transformer_handler_generalized.py \\ --extra-files \"Transformer_model/config.json,./setup_config.json,./Seq_classification_artifacts/index_to_name.json\" Create the InferenceService \u00b6 Apply the CRD kubectl apply -f bert.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve-bert created Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = torchserve-bert SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -n <namespace> -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/BERTSeqClassification:predict -d ./sample_text.txt Expected Output * Trying 44 .239.20.204... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 44 .239.20.204 ) port 80 ( #0) > PUT /v1/models/BERTSeqClassification:predict HTTP/1.1 > Host: torchserve-bert.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 79 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 8 < date: Wed, 04 Nov 2020 10 :54:49 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: 4b54d3ac-185f-444c-b344-b8a785fdeb50 < x-envoy-upstream-service-time: 2085 < server: istio-envoy < * Connection #0 to host torchserve-bert.kserve-test.example.com left intact Accepted Captum Explanations \u00b6 In order to understand the word importances and attributions when we make an explanation Request, we use Captum Insights for the Hugginface Transformers pre-trained model. MODEL_NAME = torchserve-bert SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -n <namespace> -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/BERTSeqClassification:explaine -d ./sample_text.txt Expected output * Trying ::1:8080... * Connected to localhost ( ::1 ) port 8080 ( #0) > POST /v1/models/BERTSeqClassification:explain HTTP/1.1 > Host: torchserve-bert.default.example.com > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 84 > Content-Type: application/x-www-form-urlencoded >Handling connection for 8080 * upload completely sent off: 84 out of 84 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 292 < content-type: application/json ; charset = UTF-8 < date: Sun, 27 Dec 2020 05 :53:52 GMT < server: istio-envoy < x-envoy-upstream-service-time: 5769 < * Connection #0 to host localhost left intact { \"explanations\" : [{ \"importances\" : [ 0 .0, -0.6324463574494716, -0.033115653530477414, 0 .2681695752722339, -0.29124745608778546, 0 .5422589681903883, -0.3848768219546909, 0 .0 ] , \"words\" : [ \"[CLS]\" , \"bloomberg\" , \"has\" , \"reported\" , \"on\" , \"the\" , \"economy\" , \"[SEP]\" ] , \"delta\" : -0.0007350619859377225 }]}","title":"TorchServe example with Huggingface bert model"},{"location":"modelserving/v1beta1/torchserve/bert/#torchserve-example-with-huggingface-bert-model","text":"In this example we will show how to serve Huggingface Transformers with TorchServe on KServe.","title":"TorchServe example with Huggingface bert model"},{"location":"modelserving/v1beta1/torchserve/bert/#model-archive-file-creation","text":"Clone pytorch/serve repository, navigate to examples/Huggingface_Transformers and follow the steps for creating the MAR file including serialized model and other dependent files. TorchServe supports both eager model and torchscript and here we save as the pretrained model. torch-model-archiver --model-name BERTSeqClassification --version 1 .0 \\ --serialized-file Transformer_model/pytorch_model.bin \\ --handler ./Transformer_handler_generalized.py \\ --extra-files \"Transformer_model/config.json,./setup_config.json,./Seq_classification_artifacts/index_to_name.json\"","title":"Model archive file creation"},{"location":"modelserving/v1beta1/torchserve/bert/#create-the-inferenceservice","text":"Apply the CRD kubectl apply -f bert.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve-bert created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/torchserve/bert/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = torchserve-bert SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -n <namespace> -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/BERTSeqClassification:predict -d ./sample_text.txt Expected Output * Trying 44 .239.20.204... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 44 .239.20.204 ) port 80 ( #0) > PUT /v1/models/BERTSeqClassification:predict HTTP/1.1 > Host: torchserve-bert.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 79 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 8 < date: Wed, 04 Nov 2020 10 :54:49 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: 4b54d3ac-185f-444c-b344-b8a785fdeb50 < x-envoy-upstream-service-time: 2085 < server: istio-envoy < * Connection #0 to host torchserve-bert.kserve-test.example.com left intact Accepted","title":"Run a prediction"},{"location":"modelserving/v1beta1/torchserve/bert/#captum-explanations","text":"In order to understand the word importances and attributions when we make an explanation Request, we use Captum Insights for the Hugginface Transformers pre-trained model. MODEL_NAME = torchserve-bert SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -n <namespace> -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/BERTSeqClassification:explaine -d ./sample_text.txt Expected output * Trying ::1:8080... * Connected to localhost ( ::1 ) port 8080 ( #0) > POST /v1/models/BERTSeqClassification:explain HTTP/1.1 > Host: torchserve-bert.default.example.com > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 84 > Content-Type: application/x-www-form-urlencoded >Handling connection for 8080 * upload completely sent off: 84 out of 84 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 292 < content-type: application/json ; charset = UTF-8 < date: Sun, 27 Dec 2020 05 :53:52 GMT < server: istio-envoy < x-envoy-upstream-service-time: 5769 < * Connection #0 to host localhost left intact { \"explanations\" : [{ \"importances\" : [ 0 .0, -0.6324463574494716, -0.033115653530477414, 0 .2681695752722339, -0.29124745608778546, 0 .5422589681903883, -0.3848768219546909, 0 .0 ] , \"words\" : [ \"[CLS]\" , \"bloomberg\" , \"has\" , \"reported\" , \"on\" , \"the\" , \"economy\" , \"[SEP]\" ] , \"delta\" : -0.0007350619859377225 }]}","title":"Captum Explanations"},{"location":"modelserving/v1beta1/torchserve/canary/","text":"Canary Rollout \u00b6 Create InferenceService with default model \u00b6 apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier\" Apply the InferenceService kubectl apply -f torchserve.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve created Create InferenceService with canary model \u00b6 Change the storageUri for the new model version and apply the InferenceService apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : canaryTrafficPercent : 20 pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier/v2\" Apply the InferenceService kubectl apply -f canary.yaml You should now see two revisions created kubectl get revisions -l serving.kserve.io/inferenceservice = torchserve NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON torchserve-predictor-default-9lttm torchserve-predictor-default torchserve-predictor-default-9lttm 1 True torchserve-predictor-default-kxp96 torchserve-predictor-default torchserve-predictor-default-kxp96 2 True Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torchserve -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist:predict HTTP/1.1 > Host: torchserve.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 167 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Tue, 27 Oct 2020 08 :26:19 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517 < x-envoy-upstream-service-time: 6 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"predictions\" : [ \"2\" ]} Check the traffic split between the two revisions \u00b6 kubectl get pods -l serving.kserve.io/inferenceservice = torchserve NAME READY STATUS RESTARTS AGE torchserve-predictor-default-9lttm-deployment-7dd5cff4cb-tmmlc 2 /2 Running 0 21m torchserve-predictor-default-kxp96-deployment-5d949864df-bmzfk 2 /2 Running 0 20m Check the traffic split kubectl get ksvc torchserve-predictor-default -oyaml status: address: url: http://torchserve-predictor-default.default.svc.cluster.local traffic: - latestRevision: true percent: 20 revisionName: torchserve-predictor-default-kxp96 tag: latest url: http://latest-torchserve-predictor-default.default.example.com - latestRevision: false percent: 80 revisionName: torchserve-predictor-default-9lttm tag: prev url: http://prev-torchserve-predictor-default.default.example.com url: http://torchserve-predictor-default.default.example.com","title":"Canary Rollout"},{"location":"modelserving/v1beta1/torchserve/canary/#canary-rollout","text":"","title":"Canary Rollout"},{"location":"modelserving/v1beta1/torchserve/canary/#create-inferenceservice-with-default-model","text":"apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier\" Apply the InferenceService kubectl apply -f torchserve.yaml Expected Output $inferenceservice .serving.kserve.io/torchserve created","title":"Create InferenceService with default model"},{"location":"modelserving/v1beta1/torchserve/canary/#create-inferenceservice-with-canary-model","text":"Change the storageUri for the new model version and apply the InferenceService apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"torchserve\" spec : predictor : canaryTrafficPercent : 20 pytorch : storageUri : \"gs://kfserving-examples/models/torchserve/image_classifier/v2\" Apply the InferenceService kubectl apply -f canary.yaml You should now see two revisions created kubectl get revisions -l serving.kserve.io/inferenceservice = torchserve NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON torchserve-predictor-default-9lttm torchserve-predictor-default torchserve-predictor-default-9lttm 1 True torchserve-predictor-default-kxp96 torchserve-predictor-default torchserve-predictor-default-kxp96 2 True","title":"Create InferenceService with canary model"},{"location":"modelserving/v1beta1/torchserve/canary/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torchserve -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist:predict HTTP/1.1 > Host: torchserve.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 167 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Tue, 27 Oct 2020 08 :26:19 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: b10cfc9f-cd0f-4cda-9c6c-194c2cdaa517 < x-envoy-upstream-service-time: 6 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"predictions\" : [ \"2\" ]}","title":"Run a prediction"},{"location":"modelserving/v1beta1/torchserve/canary/#check-the-traffic-split-between-the-two-revisions","text":"kubectl get pods -l serving.kserve.io/inferenceservice = torchserve NAME READY STATUS RESTARTS AGE torchserve-predictor-default-9lttm-deployment-7dd5cff4cb-tmmlc 2 /2 Running 0 21m torchserve-predictor-default-kxp96-deployment-5d949864df-bmzfk 2 /2 Running 0 20m Check the traffic split kubectl get ksvc torchserve-predictor-default -oyaml status: address: url: http://torchserve-predictor-default.default.svc.cluster.local traffic: - latestRevision: true percent: 20 revisionName: torchserve-predictor-default-kxp96 tag: latest url: http://latest-torchserve-predictor-default.default.example.com - latestRevision: false percent: 80 revisionName: torchserve-predictor-default-9lttm tag: prev url: http://prev-torchserve-predictor-default.default.example.com url: http://torchserve-predictor-default.default.example.com","title":"Check the traffic split between the two revisions"},{"location":"modelserving/v1beta1/torchserve/imgconv/","text":"Convert image to byteArray \u00b6 The python script converts image to bytesarray Steps: Check python packages are installed Run below command This will write a file input.json python img2bytearray.py 0 .png","title":"Convert image to byteArray"},{"location":"modelserving/v1beta1/torchserve/imgconv/#convert-image-to-bytearray","text":"The python script converts image to bytesarray Steps: Check python packages are installed Run below command This will write a file input.json python img2bytearray.py 0 .png","title":"Convert image to byteArray"},{"location":"modelserving/v1beta1/torchserve/metrics/","text":"Metrics \u00b6 This adds prometheus and granfana to the cluster with some default metrics. Setup \u00b6 Your ~/.kube/config should point to a cluster with KServe installed . Your cluster's Istio Ingress gateway must be network accessible . Open the Istio Dashboard via the Grafana UI and Prometheus UI \u00b6 Note: Make sure to enable prometheus and grafana while installing istio. Access prometheus and grafana \u00b6 Grafana and Prometheus can be accessed from the below links # Grafana istioctl dashboard grafana # Prometheus istioctl dashboard prometheus Deployment yaml \u00b6 Enable prometheus scraping by adding annotations to deployment yaml. Here our torchserve's metrics port is 8082. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : \"torch-metrics\" annotations : prometheus.io/scrape : 'true' prometheus.io/port : '8082' spec : predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier Create the InferenceService \u00b6 Apply the CRD kubectl apply -f metrics.yaml Expected Output $inferenceservice .serving.kserve.io/torch-metrics created Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT Inference \u00b6 MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torch-metrics <namespace> -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist:predict HTTP/1.1 > Host: torch-metrics.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 272 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Fri, 23 Oct 2020 13 :01:09 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: 8881f2b9-462e-4e2d-972f-90b4eb083e53 < x-envoy-upstream-service-time: 5018 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"predictions\" : [ \"2\" ]} Adding data source \u00b6 Prometheus graph view \u00b6 Navigate to prometheus page Add a query in the prometheus page Grafana dashboard \u00b6 Navigate to grafana page Add a dashboard from the top left + symbol Click add query and enter the query Add Prometheus data source to Grafana to visualize metrics. Link: Add datasource For Exposing grafana and prometheus under istio ingress refer. Remotely accessing telemetry addons Apply below deployment apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : grafana-gateway namespace : istio-system spec : selector : istio : ingressgateway servers : - port : number : 80 name : http-grafana protocol : HTTP hosts : - \"grafana.example.com\" --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : grafana-vs namespace : istio-system spec : hosts : - \"grafana.example.com\" gateways : - grafana-gateway http : - route : - destination : host : grafana port : number : 3000 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : grafana namespace : istio-system spec : host : grafana trafficPolicy : tls : mode : DISABLE --- All request with hostname grafana.example.com redirects to grafana.","title":"Metrics"},{"location":"modelserving/v1beta1/torchserve/metrics/#metrics","text":"This adds prometheus and granfana to the cluster with some default metrics.","title":"Metrics"},{"location":"modelserving/v1beta1/torchserve/metrics/#setup","text":"Your ~/.kube/config should point to a cluster with KServe installed . Your cluster's Istio Ingress gateway must be network accessible .","title":"Setup"},{"location":"modelserving/v1beta1/torchserve/metrics/#open-the-istio-dashboard-via-the-grafana-ui-and-prometheus-ui","text":"Note: Make sure to enable prometheus and grafana while installing istio.","title":"Open the Istio Dashboard via the Grafana UI and Prometheus UI"},{"location":"modelserving/v1beta1/torchserve/metrics/#access-prometheus-and-grafana","text":"Grafana and Prometheus can be accessed from the below links # Grafana istioctl dashboard grafana # Prometheus istioctl dashboard prometheus","title":"Access prometheus and grafana"},{"location":"modelserving/v1beta1/torchserve/metrics/#deployment-yaml","text":"Enable prometheus scraping by adding annotations to deployment yaml. Here our torchserve's metrics port is 8082. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : \"torch-metrics\" annotations : prometheus.io/scrape : 'true' prometheus.io/port : '8082' spec : predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier","title":"Deployment yaml"},{"location":"modelserving/v1beta1/torchserve/metrics/#create-the-inferenceservice","text":"Apply the CRD kubectl apply -f metrics.yaml Expected Output $inferenceservice .serving.kserve.io/torch-metrics created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/torchserve/metrics/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT","title":"Run a prediction"},{"location":"modelserving/v1beta1/torchserve/metrics/#inference","text":"MODEL_NAME = mnist SERVICE_HOSTNAME = $( kubectl get inferenceservice torch-metrics <namespace> -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict -d @./mnist.json Expected Output * Trying 52 .89.19.61... * Connected to a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com ( 52 .89.19.61 ) port 80 ( #0) > PUT /v1/models/mnist:predict HTTP/1.1 > Host: torch-metrics.kserve-test.example.com > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 272 > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < cache-control: no-cache ; no-store, must-revalidate, private < content-length: 1 < date: Fri, 23 Oct 2020 13 :01:09 GMT < expires: Thu, 01 Jan 1970 00 :00:00 UTC < pragma: no-cache < x-request-id: 8881f2b9-462e-4e2d-972f-90b4eb083e53 < x-envoy-upstream-service-time: 5018 < server: istio-envoy < * Connection #0 to host a881f5a8c676a41edbccdb0a394a80d6-2069247558.us-west-2.elb.amazonaws.com left intact { \"predictions\" : [ \"2\" ]}","title":"Inference"},{"location":"modelserving/v1beta1/torchserve/metrics/#adding-data-source","text":"","title":"Adding data source"},{"location":"modelserving/v1beta1/torchserve/metrics/#prometheus-graph-view","text":"Navigate to prometheus page Add a query in the prometheus page","title":"Prometheus graph view"},{"location":"modelserving/v1beta1/torchserve/metrics/#grafana-dashboard","text":"Navigate to grafana page Add a dashboard from the top left + symbol Click add query and enter the query Add Prometheus data source to Grafana to visualize metrics. Link: Add datasource For Exposing grafana and prometheus under istio ingress refer. Remotely accessing telemetry addons Apply below deployment apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : grafana-gateway namespace : istio-system spec : selector : istio : ingressgateway servers : - port : number : 80 name : http-grafana protocol : HTTP hosts : - \"grafana.example.com\" --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : grafana-vs namespace : istio-system spec : hosts : - \"grafana.example.com\" gateways : - grafana-gateway http : - route : - destination : host : grafana port : number : 3000 --- apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : grafana namespace : istio-system spec : host : grafana trafficPolicy : tls : mode : DISABLE --- All request with hostname grafana.example.com redirects to grafana.","title":"Grafana dashboard"},{"location":"modelserving/v1beta1/torchserve/model-archiver/","text":"Generate model archiver files for torchserve \u00b6 Setup \u00b6 Your ~/.kube/config should point to a cluster with KServe installed . Your cluster's Istio Ingress gateway must be network accessible . 1. Create PV and PVC \u00b6 Create a Persistent volume and volume claim. This document uses amazonEBS PV. For AWS EFS storage you can refer to AWS EFS storage 1.1 Create PV \u00b6 Edit volume id in pv.yaml file kubectl apply -f pv.yaml Expected Output persistentvolume/model-pv-volume created 1.2 Create PVC \u00b6 kubectl apply -f pvc.yaml Expected Output persistentvolumeclaim/model-pv-claim created 2 Create model store files layout and copy to PV \u00b6 We create a pod with the PV attached to copy the model files and config.properties for generating model archive file. 2.1 Create pod for copying model store files to PV \u00b6 kubectl apply -f pvpod.yaml Expected Output pod/model-store-pod created 2.2 Create model store file layout on PV \u00b6 2.2.1 Create properties.json file \u00b6 This file has model-name, version, model-file name, serialized-file name, extra-files, handlers, workers etc. of the models. [ { \"model-name\" : \"mnist\" , \"version\" : \"1.0\" , \"model-file\" : \"\" , \"serialized-file\" : \"mnist_cnn.pt\" , \"extra-files\" : \"\" , \"handler\" : \"mnist_handler.py\" , \"min-workers\" : 1 , \"max-workers\" : 3 , \"batch-size\" : 1 , \"max-batch-delay\" : 100 , \"response-timeout\" : 120 , \"requirements\" : \"\" }, { \"model-name\" : \"densenet_161\" , \"version\" : \"1.0\" , \"model-file\" : \"\" , \"serialized-file\" : \"densenet161-8d451a50.pth\" , \"extra-files\" : \"index_to_name.json\" , \"handler\" : \"image_classifier\" , \"min-workers\" : 1 , \"max-workers\" : 3 , \"batch-size\" : 1 , \"max-batch-delay\" : 100 , \"response-timeout\" : 120 , \"requirements\" : \"\" } ] 2.2.2 Copy model and its dependent Files \u00b6 Copy all the model and dependent files to the PV in the structure given below. An empty config folder, a model-store folder containing model name as folder name. Within that model folder, the files required to build the marfile. \u251c\u2500\u2500 config \u251c\u2500\u2500 model-store \u2502 \u251c\u2500\u2500 densenet_161 \u2502 \u2502 \u251c\u2500\u2500 densenet161-8d451a50.pth \u2502 \u2502 \u251c\u2500\u2500 index_to_name.json \u2502 \u2502 \u2514\u2500\u2500 model.py \u2502 \u251c\u2500\u2500 mnist \u2502 \u2502 \u251c\u2500\u2500 mnist_cnn.pt \u2502 \u2502 \u251c\u2500\u2500 mnist_handler.py \u2502 \u2502 \u2514\u2500\u2500 mnist.py \u2502 \u2514\u2500\u2500 properties.json 2.2.3 Create folders for model-store and config in PV \u00b6 kubectl exec -it model-store-pod -c model-store -n kserve-test -- mkdir /pv/model-store/ kubectl exec -it model-store-pod -c model-store -n kserve-test -- mkdir /pv/config/ 2.3 Copy model files and config.properties to the PV \u00b6 kubectl cp model-store/* model-store-pod:/pv/model-store/ -c model-store -n kserve-test kubectl cp config.properties model-store-pod:/pv/config/ -c model-store -n kserve-test 2.4 Delete pv pod \u00b6 Since amazon EBS provide only ReadWriteOnce mode, we have to unbind the PV for use of model archiver. kubectl delete pod model-store-pod -n kserve-test 3 Generate model archive file and server configuration file \u00b6 3.1 Create model archive pod and run model archive file generation script \u00b6 kubectl apply -f model-archiver.yaml -n kserve-test 3.2 Check the output and delete model archive pod \u00b6 Verify mar files and config.properties kubectl exec -it margen-pod -n kserve-test -- ls -lR /home/model-server/model-store kubectl exec -it margen-pod -n kserve-test -- cat /home/model-server/config/config.properties 3.3 Delete model archiver \u00b6 kubectl delete -f model-archiver.yaml -n kserve-test","title":"Generate model archiver files for torchserve"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#generate-model-archiver-files-for-torchserve","text":"","title":"Generate model archiver files for torchserve"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#setup","text":"Your ~/.kube/config should point to a cluster with KServe installed . Your cluster's Istio Ingress gateway must be network accessible .","title":"Setup"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#1-create-pv-and-pvc","text":"Create a Persistent volume and volume claim. This document uses amazonEBS PV. For AWS EFS storage you can refer to AWS EFS storage","title":"1. Create PV and PVC"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#11-create-pv","text":"Edit volume id in pv.yaml file kubectl apply -f pv.yaml Expected Output persistentvolume/model-pv-volume created","title":"1.1 Create PV"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#12-create-pvc","text":"kubectl apply -f pvc.yaml Expected Output persistentvolumeclaim/model-pv-claim created","title":"1.2 Create PVC"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#2-create-model-store-files-layout-and-copy-to-pv","text":"We create a pod with the PV attached to copy the model files and config.properties for generating model archive file.","title":"2 Create model store files layout and copy to PV"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#21-create-pod-for-copying-model-store-files-to-pv","text":"kubectl apply -f pvpod.yaml Expected Output pod/model-store-pod created","title":"2.1 Create pod for copying model store files to PV"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#22-create-model-store-file-layout-on-pv","text":"","title":"2.2 Create model store file layout on PV"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#221-create-propertiesjson-file","text":"This file has model-name, version, model-file name, serialized-file name, extra-files, handlers, workers etc. of the models. [ { \"model-name\" : \"mnist\" , \"version\" : \"1.0\" , \"model-file\" : \"\" , \"serialized-file\" : \"mnist_cnn.pt\" , \"extra-files\" : \"\" , \"handler\" : \"mnist_handler.py\" , \"min-workers\" : 1 , \"max-workers\" : 3 , \"batch-size\" : 1 , \"max-batch-delay\" : 100 , \"response-timeout\" : 120 , \"requirements\" : \"\" }, { \"model-name\" : \"densenet_161\" , \"version\" : \"1.0\" , \"model-file\" : \"\" , \"serialized-file\" : \"densenet161-8d451a50.pth\" , \"extra-files\" : \"index_to_name.json\" , \"handler\" : \"image_classifier\" , \"min-workers\" : 1 , \"max-workers\" : 3 , \"batch-size\" : 1 , \"max-batch-delay\" : 100 , \"response-timeout\" : 120 , \"requirements\" : \"\" } ]","title":"2.2.1 Create properties.json file"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#222-copy-model-and-its-dependent-files","text":"Copy all the model and dependent files to the PV in the structure given below. An empty config folder, a model-store folder containing model name as folder name. Within that model folder, the files required to build the marfile. \u251c\u2500\u2500 config \u251c\u2500\u2500 model-store \u2502 \u251c\u2500\u2500 densenet_161 \u2502 \u2502 \u251c\u2500\u2500 densenet161-8d451a50.pth \u2502 \u2502 \u251c\u2500\u2500 index_to_name.json \u2502 \u2502 \u2514\u2500\u2500 model.py \u2502 \u251c\u2500\u2500 mnist \u2502 \u2502 \u251c\u2500\u2500 mnist_cnn.pt \u2502 \u2502 \u251c\u2500\u2500 mnist_handler.py \u2502 \u2502 \u2514\u2500\u2500 mnist.py \u2502 \u2514\u2500\u2500 properties.json","title":"2.2.2 Copy model and its dependent Files"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#223-create-folders-for-model-store-and-config-in-pv","text":"kubectl exec -it model-store-pod -c model-store -n kserve-test -- mkdir /pv/model-store/ kubectl exec -it model-store-pod -c model-store -n kserve-test -- mkdir /pv/config/","title":"2.2.3 Create folders for model-store and config in PV"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#23-copy-model-files-and-configproperties-to-the-pv","text":"kubectl cp model-store/* model-store-pod:/pv/model-store/ -c model-store -n kserve-test kubectl cp config.properties model-store-pod:/pv/config/ -c model-store -n kserve-test","title":"2.3 Copy model files and config.properties to the PV"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#24-delete-pv-pod","text":"Since amazon EBS provide only ReadWriteOnce mode, we have to unbind the PV for use of model archiver. kubectl delete pod model-store-pod -n kserve-test","title":"2.4 Delete pv pod"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#3-generate-model-archive-file-and-server-configuration-file","text":"","title":"3 Generate model archive file and server configuration file"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#31-create-model-archive-pod-and-run-model-archive-file-generation-script","text":"kubectl apply -f model-archiver.yaml -n kserve-test","title":"3.1 Create model archive pod and run model archive file generation script"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#32-check-the-output-and-delete-model-archive-pod","text":"Verify mar files and config.properties kubectl exec -it margen-pod -n kserve-test -- ls -lR /home/model-server/model-store kubectl exec -it margen-pod -n kserve-test -- cat /home/model-server/config/config.properties","title":"3.2 Check the output and delete model archive pod"},{"location":"modelserving/v1beta1/torchserve/model-archiver/#33-delete-model-archiver","text":"kubectl delete -f model-archiver.yaml -n kserve-test","title":"3.3 Delete model archiver"},{"location":"modelserving/v1beta1/torchserve/model-archiver/model-archiver-image/","text":"Model archiver for torchserve \u00b6 Steps: Modify config in entrypoint for default config (optional) Build docker image Push docker image to repo docker build --file Dockerfile -t margen:latest . docker tag margen:latest { username } /margen:latest docker push { username } /margen:latest","title":"Model archiver for torchserve"},{"location":"modelserving/v1beta1/torchserve/model-archiver/model-archiver-image/#model-archiver-for-torchserve","text":"Steps: Modify config in entrypoint for default config (optional) Build docker image Push docker image to repo docker build --file Dockerfile -t margen:latest . docker tag margen:latest { username } /margen:latest docker push { username } /margen:latest","title":"Model archiver for torchserve"},{"location":"modelserving/v1beta1/torchserve/model-archiver/model-store/","text":"Model archiver for torchserve \u00b6 Place all the file required to grenerate marfile in the model folder \u00b6","title":"Model archiver for torchserve"},{"location":"modelserving/v1beta1/torchserve/model-archiver/model-store/#model-archiver-for-torchserve","text":"","title":"Model archiver for torchserve"},{"location":"modelserving/v1beta1/torchserve/model-archiver/model-store/#place-all-the-file-required-to-grenerate-marfile-in-the-model-folder","text":"","title":"Place all the file required to grenerate marfile in the model folder"},{"location":"modelserving/v1beta1/transformer/feast/","text":"Predict on an InferenceService with transformer using Feast online feature store \u00b6 Transformer is an InferenceService component which does pre/post processing alongside with model inference. In this example, instead of typical input transformation of raw data to tensors, we demonstrate a use case of online feature augmentation as part of preprocessing. We use a Feast Transformer to gather online features, run inference with a SKLearn predictor, and leave post processing as pass-through. Setup \u00b6 Your ~/.kube/config should point to a cluster with KServe installed . Your cluster's Istio Ingress gateway must be network accessible . Your Feast online store is populated with driver data , instructions available here , and network accessible. Build Transformer image \u00b6 KServe.KFModel base class mainly defines three handlers preprocess , predict and postprocess , these handlers are executed in sequence, the output of the preprocess is passed to predict as the input, when predictor_host is passed the predict handler by default makes a HTTP call to the predictor url and gets back a response which then passes to postproces handler. KServe automatically fills in the predictor_host for Transformer and handle the call to the Predictor , for gRPC predictor currently you would need to overwrite the predict handler to make the gRPC call. To implement a Transformer you can derive from the base KFModel class and then overwrite the preprocess and postprocess handler to have your own customized transformation logic. Extend KFModel and implement pre/post processing functions \u00b6 We created a class, DriverTransformer, which extends KFModel for this driver ranking example. It takes additional arguments for the transformer to interact with Feast: * feast_serving_url: The Feast serving URL, in the form of <host_name_or_ip:port> * entity_ids: The entity IDs for which to retrieve features from the Feast feature store * feature_refs: The feature references for the features to be retrieved Please see the code example here Build Transformer docker image \u00b6 docker build -t { username } /driver-transformer:latest -f driver_transformer.Dockerfile . docker push { username } /driver-transformer:latest Create the InferenceService \u00b6 Please use the YAML file and update the feast_serving_url argument to create the InferenceService , which includes a Feast Transformer and a SKLearn Predictor. In the Feast Transformer image we packaged the driver transformer class so KServe knows to use the preprocess implementation to augment inputs with online features before making model inference requests. Then the InferenceService uses SKLearn to serve the driver ranking model , which is trained with Feast offline features, available in a gcs bucket specified under storageUri . Apply the CRD kubectl apply -f driver_transformer.yaml Expected Output $ inferenceservice.serving.kserve.io/driver-transformer created Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT SERVICE_NAME=sklearn-driver-transformer MODEL_NAME=sklearn-driver-transformer INPUT_PATH=@./driver-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -d $INPUT_PATH http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict Expected Output > POST /v1/models/sklearn-driver-transformer:predict HTTP/1.1 > Host: sklearn-driver-transformer.default.example.com > User-Agent: curl/7.58.0 > Accept: */* > Content-Length: 57 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 57 out of 57 bytes < HTTP/1.1 200 OK < content-length: 117 < content-type: application/json; charset=UTF-8 < date: Thu, 27 May 2021 00:34:21 GMT < server: istio-envoy < x-envoy-upstream-service-time: 47 < * Connection #0 to host 1.2.3.4 left intact {\"predictions\": [1.8440737040128852, 1.7381656744054226, 3.6771303027855993, 2.241143189554492, 0.06753551272342406]}","title":"Feast"},{"location":"modelserving/v1beta1/transformer/feast/#predict-on-an-inferenceservice-with-transformer-using-feast-online-feature-store","text":"Transformer is an InferenceService component which does pre/post processing alongside with model inference. In this example, instead of typical input transformation of raw data to tensors, we demonstrate a use case of online feature augmentation as part of preprocessing. We use a Feast Transformer to gather online features, run inference with a SKLearn predictor, and leave post processing as pass-through.","title":"Predict on an InferenceService with transformer using Feast online feature store"},{"location":"modelserving/v1beta1/transformer/feast/#setup","text":"Your ~/.kube/config should point to a cluster with KServe installed . Your cluster's Istio Ingress gateway must be network accessible . Your Feast online store is populated with driver data , instructions available here , and network accessible.","title":"Setup"},{"location":"modelserving/v1beta1/transformer/feast/#build-transformer-image","text":"KServe.KFModel base class mainly defines three handlers preprocess , predict and postprocess , these handlers are executed in sequence, the output of the preprocess is passed to predict as the input, when predictor_host is passed the predict handler by default makes a HTTP call to the predictor url and gets back a response which then passes to postproces handler. KServe automatically fills in the predictor_host for Transformer and handle the call to the Predictor , for gRPC predictor currently you would need to overwrite the predict handler to make the gRPC call. To implement a Transformer you can derive from the base KFModel class and then overwrite the preprocess and postprocess handler to have your own customized transformation logic.","title":"Build Transformer image"},{"location":"modelserving/v1beta1/transformer/feast/#extend-kfmodel-and-implement-prepost-processing-functions","text":"We created a class, DriverTransformer, which extends KFModel for this driver ranking example. It takes additional arguments for the transformer to interact with Feast: * feast_serving_url: The Feast serving URL, in the form of <host_name_or_ip:port> * entity_ids: The entity IDs for which to retrieve features from the Feast feature store * feature_refs: The feature references for the features to be retrieved Please see the code example here","title":"Extend KFModel and implement pre/post processing functions"},{"location":"modelserving/v1beta1/transformer/feast/#build-transformer-docker-image","text":"docker build -t { username } /driver-transformer:latest -f driver_transformer.Dockerfile . docker push { username } /driver-transformer:latest","title":"Build Transformer docker image"},{"location":"modelserving/v1beta1/transformer/feast/#create-the-inferenceservice","text":"Please use the YAML file and update the feast_serving_url argument to create the InferenceService , which includes a Feast Transformer and a SKLearn Predictor. In the Feast Transformer image we packaged the driver transformer class so KServe knows to use the preprocess implementation to augment inputs with online features before making model inference requests. Then the InferenceService uses SKLearn to serve the driver ranking model , which is trained with Feast offline features, available in a gcs bucket specified under storageUri . Apply the CRD kubectl apply -f driver_transformer.yaml Expected Output $ inferenceservice.serving.kserve.io/driver-transformer created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/transformer/feast/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT SERVICE_NAME=sklearn-driver-transformer MODEL_NAME=sklearn-driver-transformer INPUT_PATH=@./driver-input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -d $INPUT_PATH http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict Expected Output > POST /v1/models/sklearn-driver-transformer:predict HTTP/1.1 > Host: sklearn-driver-transformer.default.example.com > User-Agent: curl/7.58.0 > Accept: */* > Content-Length: 57 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 57 out of 57 bytes < HTTP/1.1 200 OK < content-length: 117 < content-type: application/json; charset=UTF-8 < date: Thu, 27 May 2021 00:34:21 GMT < server: istio-envoy < x-envoy-upstream-service-time: 47 < * Connection #0 to host 1.2.3.4 left intact {\"predictions\": [1.8440737040128852, 1.7381656744054226, 3.6771303027855993, 2.241143189554492, 0.06753551272342406]}","title":"Run a prediction"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/","text":"Deploy Transformer with InferenceService \u00b6 Transformer is an InferenceService component which does pre/post processing alongside with model inference. It usually takes raw input and transforms them to the input tensors model server expects. In this example we demonstrate an example of running inference with Transformer and TorchServe predictor. Build Transformer image \u00b6 KServe.KFModel base class mainly defines three handlers preprocess , predict and postprocess , these handlers are executed in sequence, the output of the preprocess is passed to predict as the input, when predictor_host is passed the predict handler by default makes a HTTP call to the predictor url and gets back a response which then passes to postproces handler. KServe automatically fills in the predictor_host for Transformer and handle the call to the Predictor , for gRPC predictor currently you would need to overwrite the predict handler to make the gRPC call. To implement a Transformer you can derive from the base KFModel class and then overwrite the preprocess and postprocess handler to have your own customized transformation logic. Extend KFModel and implement pre/post processing functions \u00b6 import kserve from typing import List , Dict from PIL import Image import torchvision.transforms as transforms import logging import io import numpy as np import base64 logging . basicConfig ( level = kserve . constants . KSERVE_LOGLEVEL ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) def image_transform ( instance ): byte_array = base64 . b64decode ( instance [ 'image_bytes' ][ 'b64' ]) image = Image . open ( io . BytesIO ( byte_array )) a = np . asarray ( image ) im = Image . fromarray ( a ) res = transform ( im ) logging . info ( res ) return res . tolist () class ImageTransformer ( kserve . KFModel ): def __init__ ( self , name : str , predictor_host : str ): super () . __init__ ( name ) self . predictor_host = predictor_host def preprocess ( self , inputs : Dict ) -> Dict : return { 'instances' : [ image_transform ( instance ) for instance in inputs [ 'instances' ]]} def postprocess ( self , inputs : Dict ) -> Dict : return inputs Please see the code example here Build Transformer docker image \u00b6 docker build -t { username } /image-transformer:latest -f transformer.Dockerfile . docker push { username } /image-transformer:latest Create the InferenceService \u00b6 Please use the YAML file to create the InferenceService , which includes a Transformer and a PyTorch Predictor. By default InferenceService uses TorchServe to serve the PyTorch models and the models are loaded from a model repository in KServe example gcs bucket according to TorchServe model repository layout. The model repository contains a mnist model but you can store more than one models there. In the Transformer image you can create a tranformer class for all the models in the repository if they can share the same transformer or maintain a map from model name to transformer classes so KServe knows to use the transformer for the corresponding model. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torchserve-transformer spec : transformer : containers : - image : kfserving/torchserve-image-transformer:latest name : kserve-container env : - name : STORAGE_URI value : gs://kfserving-examples/models/torchserve/image_classifier predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier Note STORAGE_URI environment variable is a build-in env to inject the storage initializer for custom container just like StorageURI field for prepackaged predictors and the downloaded artifacts are stored under /mnt/models . Apply the CRD kubectl apply -f transformer.yaml Expected Output $ inferenceservice.serving.kserve.io/torchserve-transformer created Run a prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT SERVICE_NAME=torchserve-transformer MODEL_NAME=mnist INPUT_PATH=@./input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -d $INPUT_PATH http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict Expected Output > POST /v1/models/mnist:predict HTTP/1.1 > Host: torchserve-transformer.default.example.com > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 401 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 401 out of 401 bytes Handling connection for 8080 * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 20 < content-type: application/json; charset=UTF-8 < date: Tue, 12 Jan 2021 09:52:30 GMT < server: istio-envoy < x-envoy-upstream-service-time: 83 < * Connection #0 to host localhost left intact {\"predictions\": [2]}","title":"How to write a custom transformer"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/#deploy-transformer-with-inferenceservice","text":"Transformer is an InferenceService component which does pre/post processing alongside with model inference. It usually takes raw input and transforms them to the input tensors model server expects. In this example we demonstrate an example of running inference with Transformer and TorchServe predictor.","title":"Deploy Transformer with InferenceService"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/#build-transformer-image","text":"KServe.KFModel base class mainly defines three handlers preprocess , predict and postprocess , these handlers are executed in sequence, the output of the preprocess is passed to predict as the input, when predictor_host is passed the predict handler by default makes a HTTP call to the predictor url and gets back a response which then passes to postproces handler. KServe automatically fills in the predictor_host for Transformer and handle the call to the Predictor , for gRPC predictor currently you would need to overwrite the predict handler to make the gRPC call. To implement a Transformer you can derive from the base KFModel class and then overwrite the preprocess and postprocess handler to have your own customized transformation logic.","title":"Build Transformer image"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/#extend-kfmodel-and-implement-prepost-processing-functions","text":"import kserve from typing import List , Dict from PIL import Image import torchvision.transforms as transforms import logging import io import numpy as np import base64 logging . basicConfig ( level = kserve . constants . KSERVE_LOGLEVEL ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) def image_transform ( instance ): byte_array = base64 . b64decode ( instance [ 'image_bytes' ][ 'b64' ]) image = Image . open ( io . BytesIO ( byte_array )) a = np . asarray ( image ) im = Image . fromarray ( a ) res = transform ( im ) logging . info ( res ) return res . tolist () class ImageTransformer ( kserve . KFModel ): def __init__ ( self , name : str , predictor_host : str ): super () . __init__ ( name ) self . predictor_host = predictor_host def preprocess ( self , inputs : Dict ) -> Dict : return { 'instances' : [ image_transform ( instance ) for instance in inputs [ 'instances' ]]} def postprocess ( self , inputs : Dict ) -> Dict : return inputs Please see the code example here","title":"Extend KFModel and implement pre/post processing functions"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/#build-transformer-docker-image","text":"docker build -t { username } /image-transformer:latest -f transformer.Dockerfile . docker push { username } /image-transformer:latest","title":"Build Transformer docker image"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/#create-the-inferenceservice","text":"Please use the YAML file to create the InferenceService , which includes a Transformer and a PyTorch Predictor. By default InferenceService uses TorchServe to serve the PyTorch models and the models are loaded from a model repository in KServe example gcs bucket according to TorchServe model repository layout. The model repository contains a mnist model but you can store more than one models there. In the Transformer image you can create a tranformer class for all the models in the repository if they can share the same transformer or maintain a map from model name to transformer classes so KServe knows to use the transformer for the corresponding model. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torchserve-transformer spec : transformer : containers : - image : kfserving/torchserve-image-transformer:latest name : kserve-container env : - name : STORAGE_URI value : gs://kfserving-examples/models/torchserve/image_classifier predictor : pytorch : storageUri : gs://kfserving-examples/models/torchserve/image_classifier Note STORAGE_URI environment variable is a build-in env to inject the storage initializer for custom container just like StorageURI field for prepackaged predictors and the downloaded artifacts are stored under /mnt/models . Apply the CRD kubectl apply -f transformer.yaml Expected Output $ inferenceservice.serving.kserve.io/torchserve-transformer created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/transformer/torchserve_image_transformer/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT SERVICE_NAME=torchserve-transformer MODEL_NAME=mnist INPUT_PATH=@./input.json SERVICE_HOSTNAME=$(kubectl get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -H \"Host: ${SERVICE_HOSTNAME}\" -d $INPUT_PATH http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict Expected Output > POST /v1/models/mnist:predict HTTP/1.1 > Host: torchserve-transformer.default.example.com > User-Agent: curl/7.73.0 > Accept: */* > Content-Length: 401 > Content-Type: application/x-www-form-urlencoded > * upload completely sent off: 401 out of 401 bytes Handling connection for 8080 * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 20 < content-type: application/json; charset=UTF-8 < date: Tue, 12 Jan 2021 09:52:30 GMT < server: istio-envoy < x-envoy-upstream-service-time: 83 < * Connection #0 to host localhost left intact {\"predictions\": [2]}","title":"Run a prediction"},{"location":"modelserving/v1beta1/triton/bert/","text":"QA Inference with BERT model using Triton Inference Server \u00b6 Bidirectional Embedding Representations from Transformers (BERT), is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. This example demonstrates - Inference on Question Answering (QA) task with BERT Base/Large model - The use of fine-tuned NVIDIA BERT models - Deploy Transformer for preprocess using BERT tokenizer - Deploy BERT model on Triton Inference Server - Inference with V2 KServe protocol We can run inference on a fine-tuned BERT model for tasks like Question Answering. Here we use a BERT model fine-tuned on a SQuaD 2.0 Dataset which contains 100,000+ question-answer pairs on 500+ articles combined with over 50,000 new, unanswerable questions. Setup \u00b6 Your ~/.kube/config should point to a cluster with KFServing 0.5 installed . Your cluster's Istio Ingress gateway must be network accessible . Skip tag resolution for nvcr.io which requires auth to resolve triton inference server image digest kubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving Increase progress deadline since pulling triton image and big bert model may longer than default timeout for 120s, this setting requires knative 0.15.0+ kubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving Extend KFServer and Implement pre/postprocess and predict \u00b6 The preprocess handler converts the paragraph and the question to BERT input using BERT tokenizer The predict handler calls Triton Inference Server using PYTHON REST API The postprocess handler converts raw prediction to the answer with the probability class BertTransformer ( kserve . KFModel ): def __init__ ( self , name : str , predictor_host : str ): super () . __init__ ( name ) self . short_paragraph_text = \"The Apollo program was the third United States human spaceflight program. First conceived as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was dedicated to President John F. Kennedy's national goal of landing a man on the Moon. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972 followed by the Apollo-Soyuz Test Project a joint Earth orbit mission with the Soviet Union in 1975.\" self . predictor_host = predictor_host self . tokenizer = tokenization . FullTokenizer ( vocab_file = \"/mnt/models/vocab.txt\" , do_lower_case = True ) self . model_name = \"bert_tf_v2_large_fp16_128_v2\" self . triton_client = None def preprocess ( self , inputs : Dict ) -> Dict : self . doc_tokens = data_processing . convert_doc_tokens ( self . short_paragraph_text ) self . features = data_processing . convert_examples_to_features ( self . doc_tokens , inputs [ \"instances\" ][ 0 ], self . tokenizer , 128 , 128 , 64 ) return self . features def predict ( self , features : Dict ) -> Dict : if not self . triton_client : self . triton_client = httpclient . InferenceServerClient ( url = self . predictor_host , verbose = True ) unique_ids = np . zeros ([ 1 , 1 ], dtype = np . int32 ) segment_ids = features [ \"segment_ids\" ] . reshape ( 1 , 128 ) input_ids = features [ \"input_ids\" ] . reshape ( 1 , 128 ) input_mask = features [ \"input_mask\" ] . reshape ( 1 , 128 ) inputs = [] inputs . append ( httpclient . InferInput ( 'unique_ids' , [ 1 , 1 ], \"INT32\" )) inputs . append ( httpclient . InferInput ( 'segment_ids' , [ 1 , 128 ], \"INT32\" )) inputs . append ( httpclient . InferInput ( 'input_ids' , [ 1 , 128 ], \"INT32\" )) inputs . append ( httpclient . InferInput ( 'input_mask' , [ 1 , 128 ], \"INT32\" )) inputs [ 0 ] . set_data_from_numpy ( unique_ids ) inputs [ 1 ] . set_data_from_numpy ( segment_ids ) inputs [ 2 ] . set_data_from_numpy ( input_ids ) inputs [ 3 ] . set_data_from_numpy ( input_mask ) outputs = [] outputs . append ( httpclient . InferRequestedOutput ( 'start_logits' , binary_data = False )) outputs . append ( httpclient . InferRequestedOutput ( 'end_logits' , binary_data = False )) result = self . triton_client . infer ( self . model_name , inputs , outputs = outputs ) return result . get_response () def postprocess ( self , result : Dict ) -> Dict : end_logits = result [ 'outputs' ][ 0 ][ 'data' ] start_logits = result [ 'outputs' ][ 1 ][ 'data' ] n_best_size = 20 # The maximum length of an answer that can be generated. This is needed # because the start and end predictions are not conditioned on one another max_answer_length = 30 ( prediction , nbest_json , scores_diff_json ) = \\ data_processing . get_predictions ( self . doc_tokens , self . features , start_logits , end_logits , n_best_size , max_answer_length ) return { \"predictions\" : prediction , \"prob\" : nbest_json [ 0 ][ 'probability' ] * 100.0 } Build the KServe Transformer image with above code cd bert_tokenizer_v2 docker build -t $USER /bert_transformer-v2:latest . --rm Or you can use the prebuild image kfserving/bert-transformer-v2:latest Create the InferenceService \u00b6 Add above custom KServe Transformer image and Triton Predictor to the InferenceService spec apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"bert-v2\" spec : transformer : containers : - name : kserve-container image : kfserving/bert-transformer-v2:latest command : - \"python\" - \"-m\" - \"bert_transformer_v2\" env : - name : STORAGE_URI value : \"gs://kfserving-samples/models/triton/bert-transformer\" predictor : triton : runtimeVersion : 20.10-py3 resources : limits : cpu : \"1\" memory : 8Gi requests : cpu : \"1\" memory : 8Gi storageUri : \"gs://kfserving-examples/models/triton/bert\" Apply the InferenceService yaml. kubectl apply -f bert_v1beta1.yaml Expected Output inferenceservice.serving.kserve.io/bert-v2 created Check the InferenceService \u00b6 kubectl get inferenceservice bert-v2 NAME URL READY AGE bert-v2 http://bert-v2.default.35.229.120.99.xip.io True 71s you will see both transformer and predictor are created and in ready state kubectl get revision -l serving.kserve.io/inferenceservice=bert-v2 NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON bert-v2-predictor-default-plhgs bert-v2-predictor-default bert-v2-predictor-default-plhgs 1 True bert-v2-transformer-default-sd6nc bert-v2-transformer-default bert-v2-transformer-default-sd6nc 1 True Run a Prediction \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT Send a question request with following input, the transformer expects sending a list of instances or inputs and preprocess then converts the inputs to expected tensor sending to Triton Inference Server . { \"instances\" : [ \"What President is credited with the original notion of putting Americans in space?\" ] } MODEL_NAME = bert-v2 INPUT_PATH = @./input.json SERVICE_HOSTNAME = $( kubectl get inferenceservices bert-v2 -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" -d $INPUT_PATH http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ $MODEL_NAME :predict Expected output {\"predictions\": \"John F. Kennedy\", \"prob\": 77.91848979818604}","title":"QA Inference with BERT model using Triton Inference Server"},{"location":"modelserving/v1beta1/triton/bert/#qa-inference-with-bert-model-using-triton-inference-server","text":"Bidirectional Embedding Representations from Transformers (BERT), is a method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. This example demonstrates - Inference on Question Answering (QA) task with BERT Base/Large model - The use of fine-tuned NVIDIA BERT models - Deploy Transformer for preprocess using BERT tokenizer - Deploy BERT model on Triton Inference Server - Inference with V2 KServe protocol We can run inference on a fine-tuned BERT model for tasks like Question Answering. Here we use a BERT model fine-tuned on a SQuaD 2.0 Dataset which contains 100,000+ question-answer pairs on 500+ articles combined with over 50,000 new, unanswerable questions.","title":"QA Inference with BERT model using Triton Inference Server"},{"location":"modelserving/v1beta1/triton/bert/#setup","text":"Your ~/.kube/config should point to a cluster with KFServing 0.5 installed . Your cluster's Istio Ingress gateway must be network accessible . Skip tag resolution for nvcr.io which requires auth to resolve triton inference server image digest kubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving Increase progress deadline since pulling triton image and big bert model may longer than default timeout for 120s, this setting requires knative 0.15.0+ kubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving","title":"Setup"},{"location":"modelserving/v1beta1/triton/bert/#extend-kfserver-and-implement-prepostprocess-and-predict","text":"The preprocess handler converts the paragraph and the question to BERT input using BERT tokenizer The predict handler calls Triton Inference Server using PYTHON REST API The postprocess handler converts raw prediction to the answer with the probability class BertTransformer ( kserve . KFModel ): def __init__ ( self , name : str , predictor_host : str ): super () . __init__ ( name ) self . short_paragraph_text = \"The Apollo program was the third United States human spaceflight program. First conceived as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was dedicated to President John F. Kennedy's national goal of landing a man on the Moon. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972 followed by the Apollo-Soyuz Test Project a joint Earth orbit mission with the Soviet Union in 1975.\" self . predictor_host = predictor_host self . tokenizer = tokenization . FullTokenizer ( vocab_file = \"/mnt/models/vocab.txt\" , do_lower_case = True ) self . model_name = \"bert_tf_v2_large_fp16_128_v2\" self . triton_client = None def preprocess ( self , inputs : Dict ) -> Dict : self . doc_tokens = data_processing . convert_doc_tokens ( self . short_paragraph_text ) self . features = data_processing . convert_examples_to_features ( self . doc_tokens , inputs [ \"instances\" ][ 0 ], self . tokenizer , 128 , 128 , 64 ) return self . features def predict ( self , features : Dict ) -> Dict : if not self . triton_client : self . triton_client = httpclient . InferenceServerClient ( url = self . predictor_host , verbose = True ) unique_ids = np . zeros ([ 1 , 1 ], dtype = np . int32 ) segment_ids = features [ \"segment_ids\" ] . reshape ( 1 , 128 ) input_ids = features [ \"input_ids\" ] . reshape ( 1 , 128 ) input_mask = features [ \"input_mask\" ] . reshape ( 1 , 128 ) inputs = [] inputs . append ( httpclient . InferInput ( 'unique_ids' , [ 1 , 1 ], \"INT32\" )) inputs . append ( httpclient . InferInput ( 'segment_ids' , [ 1 , 128 ], \"INT32\" )) inputs . append ( httpclient . InferInput ( 'input_ids' , [ 1 , 128 ], \"INT32\" )) inputs . append ( httpclient . InferInput ( 'input_mask' , [ 1 , 128 ], \"INT32\" )) inputs [ 0 ] . set_data_from_numpy ( unique_ids ) inputs [ 1 ] . set_data_from_numpy ( segment_ids ) inputs [ 2 ] . set_data_from_numpy ( input_ids ) inputs [ 3 ] . set_data_from_numpy ( input_mask ) outputs = [] outputs . append ( httpclient . InferRequestedOutput ( 'start_logits' , binary_data = False )) outputs . append ( httpclient . InferRequestedOutput ( 'end_logits' , binary_data = False )) result = self . triton_client . infer ( self . model_name , inputs , outputs = outputs ) return result . get_response () def postprocess ( self , result : Dict ) -> Dict : end_logits = result [ 'outputs' ][ 0 ][ 'data' ] start_logits = result [ 'outputs' ][ 1 ][ 'data' ] n_best_size = 20 # The maximum length of an answer that can be generated. This is needed # because the start and end predictions are not conditioned on one another max_answer_length = 30 ( prediction , nbest_json , scores_diff_json ) = \\ data_processing . get_predictions ( self . doc_tokens , self . features , start_logits , end_logits , n_best_size , max_answer_length ) return { \"predictions\" : prediction , \"prob\" : nbest_json [ 0 ][ 'probability' ] * 100.0 } Build the KServe Transformer image with above code cd bert_tokenizer_v2 docker build -t $USER /bert_transformer-v2:latest . --rm Or you can use the prebuild image kfserving/bert-transformer-v2:latest","title":"Extend KFServer and Implement pre/postprocess and predict"},{"location":"modelserving/v1beta1/triton/bert/#create-the-inferenceservice","text":"Add above custom KServe Transformer image and Triton Predictor to the InferenceService spec apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"bert-v2\" spec : transformer : containers : - name : kserve-container image : kfserving/bert-transformer-v2:latest command : - \"python\" - \"-m\" - \"bert_transformer_v2\" env : - name : STORAGE_URI value : \"gs://kfserving-samples/models/triton/bert-transformer\" predictor : triton : runtimeVersion : 20.10-py3 resources : limits : cpu : \"1\" memory : 8Gi requests : cpu : \"1\" memory : 8Gi storageUri : \"gs://kfserving-examples/models/triton/bert\" Apply the InferenceService yaml. kubectl apply -f bert_v1beta1.yaml Expected Output inferenceservice.serving.kserve.io/bert-v2 created","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/triton/bert/#check-the-inferenceservice","text":"kubectl get inferenceservice bert-v2 NAME URL READY AGE bert-v2 http://bert-v2.default.35.229.120.99.xip.io True 71s you will see both transformer and predictor are created and in ready state kubectl get revision -l serving.kserve.io/inferenceservice=bert-v2 NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON bert-v2-predictor-default-plhgs bert-v2-predictor-default bert-v2-predictor-default-plhgs 1 True bert-v2-transformer-default-sd6nc bert-v2-transformer-default bert-v2-transformer-default-sd6nc 1 True","title":"Check the InferenceService"},{"location":"modelserving/v1beta1/triton/bert/#run-a-prediction","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT Send a question request with following input, the transformer expects sending a list of instances or inputs and preprocess then converts the inputs to expected tensor sending to Triton Inference Server . { \"instances\" : [ \"What President is credited with the original notion of putting Americans in space?\" ] } MODEL_NAME = bert-v2 INPUT_PATH = @./input.json SERVICE_HOSTNAME = $( kubectl get inferenceservices bert-v2 -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -H \"Host: ${ SERVICE_HOSTNAME } \" -d $INPUT_PATH http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ $MODEL_NAME :predict Expected output {\"predictions\": \"John F. Kennedy\", \"prob\": 77.91848979818604}","title":"Run a Prediction"},{"location":"modelserving/v1beta1/triton/torchscript/","text":"Predict on a Triton InferenceService with TorchScript model \u00b6 While Python is a suitable and preferred language for many scenarios requiring dynamism and ease of iteration, there are equally many situations where precisely these properties of Python are unfavorable. One environment in which the latter often applies is production \u2013 the land of low latencies and strict deployment requirements. For production scenarios, C++ is very often the language of choice, The following example will outline the path PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++ like Triton Inference Server, with no dependency on Python. Setup \u00b6 Your ~/.kube/config should point to a cluster with KFServing 0.5 installed . Your cluster's Istio Ingress gateway must be network accessible . Skip tag resolution for nvcr.io which requires auth to resolve triton inference server image digest kubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving Increase progress deadline since pulling triton image and big bert model may longer than default timeout for 120s, this setting requires knative 0.15.0+ kubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving Train a Pytorch Model \u00b6 Train the cifar pytorch model . Export as Torchscript Model \u00b6 A PyTorch model\u2019s journey from Python to C++ is enabled by Torch Script , a representation of a PyTorch model that can be understood, compiled and serialized by the Torch Script compiler. If you are starting out from an existing PyTorch model written in the vanilla eager API, you must first convert your model to Torch Script. Convert the above model via Tracing and serialize the script module to a file import torch # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. example = torch . rand ( 1 , 3 , 32 , 32 ) traced_script_module = torch . jit . trace ( net , example ) traced_script_module . save ( \"model.pt\" ) Store your trained model on GCS in a Model Repository \u00b6 Once the model is exported as Torchscript model file, the next step is to upload the model to a GCS bucket. Triton supports loading multiple models so it expects a model repository which follows a required layout in the bucket. <model-repository-path>/ <model-name>/ [config.pbtxt] [<output-labels-file> ...] <version>/ <model-definition-file> <version>/ <model-definition-file> ... <model-name>/ [config.pbtxt] [<output-labels-file> ...] <version>/ <model-definition-file> <version>/ <model-definition-file> For example in your model repository bucket gs://kfserving-examples/models/torchscript , the layout can be torchscript/ cifar/ config.pbtxt 1/ model.pt The config.pbtxt defines a model configuration that provides the required and optional information for the model. A minimal model configuration must specify name, platform, max_batch_size, input, and output. Due to the absence of names for inputs and outputs in a TorchScript model, the name attribute of both the inputs and outputs in the configuration must follow a specific naming convention i.e. \u201c __ \u201d. Where can be any string and refers to the position of the corresponding input/output. This means if there are two inputs and two outputs they must be named as: INPUT__0 , INPUT__1 and OUTPUT__0 , OUTPUT__1 such that INPUT__0 refers to first input and INPUT__1 refers to the second input, etc. name: \"cifar\" platform: \"pytorch_libtorch\" max_batch_size: 1 input [ { name: \"INPUT__0\" data_type: TYPE_FP32 dims: [3,32,32] } ] output [ { name: \"OUTPUT__0\" data_type: TYPE_FP32 dims: [10] } ] instance_group [ { count: 1 kind: KIND_CPU } ] To schedule the model on GPU you would need to change the instance_group with GPU kind instance_group [ { count: 1 kind: KIND_GPU } ] Inference with HTTP endpoint \u00b6 Create the InferenceService \u00b6 Create the inference service yaml with the above specified model repository uri. kubectl apply -f torchscript.yaml apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torchscript-cifar10 spec : predictor : triton : storageUri : gs://kfserving-examples/models/torchscript runtimeVersion : 20.10-py3 env : - name : OMP_NUM_THREADS value : \"1\" Setting OMP_NUM_THREADS env is critical for performance : OMP_NUM_THREADS is commonly used in numpy, PyTorch, and Tensorflow to perform multi-threaded linear algebra. We want one thread per worker instead of many threads per worker to avoid contention. Expected Output and check the readiness of the InferenceService $ inferenceservice.serving.kserve.io/torchscript-cifar10 created kubectl get inferenceservices torchscript-demo Run a prediction with curl \u00b6 The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT The latest Triton Inference Server already switched to use KServe prediction V2 protocol , so the input request needs to follow the V2 schema with the specified data type, shape. MODEL_NAME = cifar10 INPUT_PATH = @./input.json SERVICE_HOSTNAME = $( kubectl get inferenceservice torchscript-cifar10 -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -X -H \"Host: ${ SERVICE_HOSTNAME } \" POST https:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/ $MODEL_NAME /infer -d $INPUT_PATH expected output * Connected to torchscript-cifar.default.svc.cluster.local ( 10 .51.242.87 ) port 80 ( #0) > POST /v2/models/cifar10/infer HTTP/1.1 > Host: torchscript-cifar.default.svc.cluster.local > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 110765 > Content-Type: application/x-www-form-urlencoded > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < content-length: 315 < content-type: application/json < date: Sun, 11 Oct 2020 21 :26:51 GMT < x-envoy-upstream-service-time: 8 < server: istio-envoy < * Connection #0 to host torchscript-cifar.default.svc.cluster.local left intact { \"model_name\" : \"cifar10\" , \"model_version\" : \"1\" , \"outputs\" : [{ \"name\" : \"OUTPUT__0\" , \"datatype\" : \"FP32\" , \"shape\" : [ 1 ,10 ] , \"data\" : [ -2.0964810848236086,-0.13700756430625916,-0.5095657706260681,2.795621395111084,-0.5605481863021851,1.9934231042861939,1.1288187503814698,-1.4043136835098267,0.6004879474639893,-2.1237082481384279 ]}]} Inference with gRPC endpoint \u00b6 Create the InferenceService \u00b6 Create the inference service yaml and expose the gRPC port, currently only one port is allowed to expose either HTTP or gRPC port and by default HTTP port is exposed. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torchscript-cifar10 spec : predictor : triton : storageUri : gs://kfserving-examples/models/torchscript runtimeVersion : 20.10-py3 ports : - containerPort : 9000 name : h2c protocol : TCP env : - name : OMP_NUM_THREADS value : \"1\" Apply the gRPC InferenceService yaml and then you can call the model with tritonclient python library after InferenceService is ready. kubectl apply -f torchscript_grpc.yaml Run a performance test \u00b6 QPS rate --rate can be changed in the perf.yaml . kubectl create -f perf.yaml Requests [total, rate, throughput] 6000, 100.02, 100.01 Duration [total, attack, wait] 59.995s, 59.99s, 4.961ms Latencies [min, mean, 50, 90, 95, 99, max] 4.222ms, 5.7ms, 5.548ms, 6.384ms, 6.743ms, 9.286ms, 25.85ms Bytes In [total, mean] 1890000, 315.00 Bytes Out [total, mean] 665874000, 110979.00 Success [ratio] 100.00% Status Codes [code:count] 200:6000 Error Set: Add Transformer to the InferenceService \u00b6 Triton Inference Server expects tensors as input data, often times a pre-processing step is required before making the prediction call when the user is sending in request with raw input format. Transformer component can be specified on InferenceService spec for user implemented pre/post processing code. User is responsible to create a python class which extends from KServe KFModel base class which implements preprocess handler to transform raw input format to tensor format according to V2 prediction protocol, postprocess handle is to convert raw prediction response to a more user friendly response. Implement pre/post processing functions \u00b6 import kserve from typing import List , Dict from PIL import Image import torchvision.transforms as transforms import logging import io import numpy as np import base64 logging . basicConfig ( level = kserve . constants . KSERVE_LOGLEVEL ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) def image_transform ( instance ): byte_array = base64 . b64decode ( instance [ 'image_bytes' ][ 'b64' ]) image = Image . open ( io . BytesIO ( byte_array )) a = np . asarray ( image ) im = Image . fromarray ( a ) res = transform ( im ) logging . info ( res ) return res . tolist () class ImageTransformer ( kserve . KFModel ): def __init__ ( self , name : str , predictor_host : str ): super () . __init__ ( name ) self . predictor_host = predictor_host def preprocess ( self , inputs : Dict ) -> Dict : return { 'inputs' : [ { 'name' : 'INPUT__0' , 'shape' : [ 1 , 3 , 32 , 32 ], 'datatype' : \"FP32\" , 'data' : [ image_transform ( instance ) for instance in inputs [ 'instances' ]] } ] } def postprocess ( self , results : Dict ) -> Dict : # Here we reshape the data because triton always returns the flatten 1D array as json if not explicitly requesting binary # since we are not using the triton python client library which takes care of the reshape it is up to user to reshape the returned tensor. return { output [ \"name\" ] : np . array ( output [ \"data\" ]) . reshape ( output [ \"shape\" ]) for output in results [ \"outputs\" ]} Build Transformer docker image \u00b6 docker build -t $DOCKER_USER/image-transformer-v2:latest -f transformer.Dockerfile . --rm Create the InferenceService with Transformer \u00b6 Please use the YAML file to create the InferenceService, which adds the image transformer component with the docker image built from above. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torch-transfomer spec : predictor : triton : storageUri : gs://kfserving-examples/models/torchscript runtimeVersion : 20.10-py3 env : - name : OMP_NUM_THREADS value : \"1\" transformer : containers : - image : kfserving/image-transformer-v2:latest name : kserve-container command : - \"python\" - \"-m\" - \"image_transformer_v2\" args : - --model_name - cifar10 - --protocol - v2 kubectl apply -f torch_transformer.yaml Expected Output $ inferenceservice.serving.kserve.io/torch-transfomer created Run a prediction from curl \u00b6 The transformer does not enforce a specific schema like predictor but the general recommendation is to send in as a list of object(dict): \"instances\": <value>|<list-of-objects> { \"instances\" : [ { \"image\" : { \"b64\" : \"aW1hZ2UgYnl0ZXM=\" }, \"caption\" : \"seaside\" }, { \"image\" : { \"b64\" : \"YXdlc29tZSBpbWFnZSBieXRlcw==\" }, \"caption\" : \"mountains\" } ] } SERVICE_NAME=torch-transfomer MODEL_NAME=cifar10 INPUT_PATH=@./image.json SERVICE_HOSTNAME=$(kubectl get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -X POST -H \"Host: ${SERVICE_HOSTNAME}\" https://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH You should see an output similar to the one below: > POST /v2/models/cifar:predict HTTP/2 > user-agent: curl/7.71.1 > accept: */* > content-length: 3422 > content-type: application/x-www-form-urlencoded > * We are completely uploaded and fine * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)! < HTTP/2 200 < content-length: 338 < content-type: application/json; charset=UTF-8 < date: Thu, 08 Oct 2020 13:15:14 GMT < server: istio-envoy < x-envoy-upstream-service-time: 52 < {\"model_name\": \"cifar\", \"model_version\": \"1\", \"outputs\": [{\"name\": \"OUTPUT__0\", \"datatype\": \"FP32\", \"shape\": [1, 10], \"data\": [-0.7299326062202454, -2.186835289001465, -0.029627874493598938, 2.3753483295440674, -0.3476247489452362, 1.3253062963485718, 0.5721136927604675, 0.049311548471450806, -0.3691796362400055, -1.0804035663604736]}]}","title":"Predict on a Triton InferenceService with TorchScript model"},{"location":"modelserving/v1beta1/triton/torchscript/#predict-on-a-triton-inferenceservice-with-torchscript-model","text":"While Python is a suitable and preferred language for many scenarios requiring dynamism and ease of iteration, there are equally many situations where precisely these properties of Python are unfavorable. One environment in which the latter often applies is production \u2013 the land of low latencies and strict deployment requirements. For production scenarios, C++ is very often the language of choice, The following example will outline the path PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++ like Triton Inference Server, with no dependency on Python.","title":"Predict on a Triton InferenceService with TorchScript model"},{"location":"modelserving/v1beta1/triton/torchscript/#setup","text":"Your ~/.kube/config should point to a cluster with KFServing 0.5 installed . Your cluster's Istio Ingress gateway must be network accessible . Skip tag resolution for nvcr.io which requires auth to resolve triton inference server image digest kubectl patch cm config-deployment --patch '{\"data\":{\"registriesSkippingTagResolving\":\"nvcr.io\"}}' -n knative-serving Increase progress deadline since pulling triton image and big bert model may longer than default timeout for 120s, this setting requires knative 0.15.0+ kubectl patch cm config-deployment --patch '{\"data\":{\"progressDeadline\": \"600s\"}}' -n knative-serving","title":"Setup"},{"location":"modelserving/v1beta1/triton/torchscript/#train-a-pytorch-model","text":"Train the cifar pytorch model .","title":"Train a Pytorch Model"},{"location":"modelserving/v1beta1/triton/torchscript/#export-as-torchscript-model","text":"A PyTorch model\u2019s journey from Python to C++ is enabled by Torch Script , a representation of a PyTorch model that can be understood, compiled and serialized by the Torch Script compiler. If you are starting out from an existing PyTorch model written in the vanilla eager API, you must first convert your model to Torch Script. Convert the above model via Tracing and serialize the script module to a file import torch # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing. example = torch . rand ( 1 , 3 , 32 , 32 ) traced_script_module = torch . jit . trace ( net , example ) traced_script_module . save ( \"model.pt\" )","title":"Export as Torchscript Model"},{"location":"modelserving/v1beta1/triton/torchscript/#store-your-trained-model-on-gcs-in-a-model-repository","text":"Once the model is exported as Torchscript model file, the next step is to upload the model to a GCS bucket. Triton supports loading multiple models so it expects a model repository which follows a required layout in the bucket. <model-repository-path>/ <model-name>/ [config.pbtxt] [<output-labels-file> ...] <version>/ <model-definition-file> <version>/ <model-definition-file> ... <model-name>/ [config.pbtxt] [<output-labels-file> ...] <version>/ <model-definition-file> <version>/ <model-definition-file> For example in your model repository bucket gs://kfserving-examples/models/torchscript , the layout can be torchscript/ cifar/ config.pbtxt 1/ model.pt The config.pbtxt defines a model configuration that provides the required and optional information for the model. A minimal model configuration must specify name, platform, max_batch_size, input, and output. Due to the absence of names for inputs and outputs in a TorchScript model, the name attribute of both the inputs and outputs in the configuration must follow a specific naming convention i.e. \u201c __ \u201d. Where can be any string and refers to the position of the corresponding input/output. This means if there are two inputs and two outputs they must be named as: INPUT__0 , INPUT__1 and OUTPUT__0 , OUTPUT__1 such that INPUT__0 refers to first input and INPUT__1 refers to the second input, etc. name: \"cifar\" platform: \"pytorch_libtorch\" max_batch_size: 1 input [ { name: \"INPUT__0\" data_type: TYPE_FP32 dims: [3,32,32] } ] output [ { name: \"OUTPUT__0\" data_type: TYPE_FP32 dims: [10] } ] instance_group [ { count: 1 kind: KIND_CPU } ] To schedule the model on GPU you would need to change the instance_group with GPU kind instance_group [ { count: 1 kind: KIND_GPU } ]","title":"Store your trained model on GCS in a Model Repository"},{"location":"modelserving/v1beta1/triton/torchscript/#inference-with-http-endpoint","text":"","title":"Inference with HTTP endpoint"},{"location":"modelserving/v1beta1/triton/torchscript/#create-the-inferenceservice","text":"Create the inference service yaml with the above specified model repository uri. kubectl apply -f torchscript.yaml apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torchscript-cifar10 spec : predictor : triton : storageUri : gs://kfserving-examples/models/torchscript runtimeVersion : 20.10-py3 env : - name : OMP_NUM_THREADS value : \"1\" Setting OMP_NUM_THREADS env is critical for performance : OMP_NUM_THREADS is commonly used in numpy, PyTorch, and Tensorflow to perform multi-threaded linear algebra. We want one thread per worker instead of many threads per worker to avoid contention. Expected Output and check the readiness of the InferenceService $ inferenceservice.serving.kserve.io/torchscript-cifar10 created kubectl get inferenceservices torchscript-demo","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/triton/torchscript/#run-a-prediction-with-curl","text":"The first step is to determine the ingress IP and ports and set INGRESS_HOST and INGRESS_PORT The latest Triton Inference Server already switched to use KServe prediction V2 protocol , so the input request needs to follow the V2 schema with the specified data type, shape. MODEL_NAME = cifar10 INPUT_PATH = @./input.json SERVICE_HOSTNAME = $( kubectl get inferenceservice torchscript-cifar10 -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v -X -H \"Host: ${ SERVICE_HOSTNAME } \" POST https:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/ $MODEL_NAME /infer -d $INPUT_PATH expected output * Connected to torchscript-cifar.default.svc.cluster.local ( 10 .51.242.87 ) port 80 ( #0) > POST /v2/models/cifar10/infer HTTP/1.1 > Host: torchscript-cifar.default.svc.cluster.local > User-Agent: curl/7.47.0 > Accept: */* > Content-Length: 110765 > Content-Type: application/x-www-form-urlencoded > Expect: 100 -continue > < HTTP/1.1 100 Continue * We are completely uploaded and fine < HTTP/1.1 200 OK < content-length: 315 < content-type: application/json < date: Sun, 11 Oct 2020 21 :26:51 GMT < x-envoy-upstream-service-time: 8 < server: istio-envoy < * Connection #0 to host torchscript-cifar.default.svc.cluster.local left intact { \"model_name\" : \"cifar10\" , \"model_version\" : \"1\" , \"outputs\" : [{ \"name\" : \"OUTPUT__0\" , \"datatype\" : \"FP32\" , \"shape\" : [ 1 ,10 ] , \"data\" : [ -2.0964810848236086,-0.13700756430625916,-0.5095657706260681,2.795621395111084,-0.5605481863021851,1.9934231042861939,1.1288187503814698,-1.4043136835098267,0.6004879474639893,-2.1237082481384279 ]}]}","title":"Run a prediction with curl"},{"location":"modelserving/v1beta1/triton/torchscript/#inference-with-grpc-endpoint","text":"","title":"Inference with gRPC endpoint"},{"location":"modelserving/v1beta1/triton/torchscript/#create-the-inferenceservice_1","text":"Create the inference service yaml and expose the gRPC port, currently only one port is allowed to expose either HTTP or gRPC port and by default HTTP port is exposed. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torchscript-cifar10 spec : predictor : triton : storageUri : gs://kfserving-examples/models/torchscript runtimeVersion : 20.10-py3 ports : - containerPort : 9000 name : h2c protocol : TCP env : - name : OMP_NUM_THREADS value : \"1\" Apply the gRPC InferenceService yaml and then you can call the model with tritonclient python library after InferenceService is ready. kubectl apply -f torchscript_grpc.yaml","title":"Create the InferenceService"},{"location":"modelserving/v1beta1/triton/torchscript/#run-a-performance-test","text":"QPS rate --rate can be changed in the perf.yaml . kubectl create -f perf.yaml Requests [total, rate, throughput] 6000, 100.02, 100.01 Duration [total, attack, wait] 59.995s, 59.99s, 4.961ms Latencies [min, mean, 50, 90, 95, 99, max] 4.222ms, 5.7ms, 5.548ms, 6.384ms, 6.743ms, 9.286ms, 25.85ms Bytes In [total, mean] 1890000, 315.00 Bytes Out [total, mean] 665874000, 110979.00 Success [ratio] 100.00% Status Codes [code:count] 200:6000 Error Set:","title":"Run a performance test"},{"location":"modelserving/v1beta1/triton/torchscript/#add-transformer-to-the-inferenceservice","text":"Triton Inference Server expects tensors as input data, often times a pre-processing step is required before making the prediction call when the user is sending in request with raw input format. Transformer component can be specified on InferenceService spec for user implemented pre/post processing code. User is responsible to create a python class which extends from KServe KFModel base class which implements preprocess handler to transform raw input format to tensor format according to V2 prediction protocol, postprocess handle is to convert raw prediction response to a more user friendly response.","title":"Add Transformer to the InferenceService"},{"location":"modelserving/v1beta1/triton/torchscript/#implement-prepost-processing-functions","text":"import kserve from typing import List , Dict from PIL import Image import torchvision.transforms as transforms import logging import io import numpy as np import base64 logging . basicConfig ( level = kserve . constants . KSERVE_LOGLEVEL ) transform = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.5 , 0.5 , 0.5 ), ( 0.5 , 0.5 , 0.5 ))]) def image_transform ( instance ): byte_array = base64 . b64decode ( instance [ 'image_bytes' ][ 'b64' ]) image = Image . open ( io . BytesIO ( byte_array )) a = np . asarray ( image ) im = Image . fromarray ( a ) res = transform ( im ) logging . info ( res ) return res . tolist () class ImageTransformer ( kserve . KFModel ): def __init__ ( self , name : str , predictor_host : str ): super () . __init__ ( name ) self . predictor_host = predictor_host def preprocess ( self , inputs : Dict ) -> Dict : return { 'inputs' : [ { 'name' : 'INPUT__0' , 'shape' : [ 1 , 3 , 32 , 32 ], 'datatype' : \"FP32\" , 'data' : [ image_transform ( instance ) for instance in inputs [ 'instances' ]] } ] } def postprocess ( self , results : Dict ) -> Dict : # Here we reshape the data because triton always returns the flatten 1D array as json if not explicitly requesting binary # since we are not using the triton python client library which takes care of the reshape it is up to user to reshape the returned tensor. return { output [ \"name\" ] : np . array ( output [ \"data\" ]) . reshape ( output [ \"shape\" ]) for output in results [ \"outputs\" ]}","title":"Implement pre/post processing functions"},{"location":"modelserving/v1beta1/triton/torchscript/#build-transformer-docker-image","text":"docker build -t $DOCKER_USER/image-transformer-v2:latest -f transformer.Dockerfile . --rm","title":"Build Transformer docker image"},{"location":"modelserving/v1beta1/triton/torchscript/#create-the-inferenceservice-with-transformer","text":"Please use the YAML file to create the InferenceService, which adds the image transformer component with the docker image built from above. apiVersion : serving.kserve.io/v1beta1 kind : InferenceService metadata : name : torch-transfomer spec : predictor : triton : storageUri : gs://kfserving-examples/models/torchscript runtimeVersion : 20.10-py3 env : - name : OMP_NUM_THREADS value : \"1\" transformer : containers : - image : kfserving/image-transformer-v2:latest name : kserve-container command : - \"python\" - \"-m\" - \"image_transformer_v2\" args : - --model_name - cifar10 - --protocol - v2 kubectl apply -f torch_transformer.yaml Expected Output $ inferenceservice.serving.kserve.io/torch-transfomer created","title":"Create the InferenceService with Transformer"},{"location":"modelserving/v1beta1/triton/torchscript/#run-a-prediction-from-curl","text":"The transformer does not enforce a specific schema like predictor but the general recommendation is to send in as a list of object(dict): \"instances\": <value>|<list-of-objects> { \"instances\" : [ { \"image\" : { \"b64\" : \"aW1hZ2UgYnl0ZXM=\" }, \"caption\" : \"seaside\" }, { \"image\" : { \"b64\" : \"YXdlc29tZSBpbWFnZSBieXRlcw==\" }, \"caption\" : \"mountains\" } ] } SERVICE_NAME=torch-transfomer MODEL_NAME=cifar10 INPUT_PATH=@./image.json SERVICE_HOSTNAME=$(kubectl get inferenceservice $SERVICE_NAME -o jsonpath='{.status.url}' | cut -d \"/\" -f 3) curl -v -X POST -H \"Host: ${SERVICE_HOSTNAME}\" https://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/$MODEL_NAME:predict -d $INPUT_PATH You should see an output similar to the one below: > POST /v2/models/cifar:predict HTTP/2 > user-agent: curl/7.71.1 > accept: */* > content-length: 3422 > content-type: application/x-www-form-urlencoded > * We are completely uploaded and fine * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4): * old SSL session ID is stale, removing * Connection state changed (MAX_CONCURRENT_STREAMS == 4294967295)! < HTTP/2 200 < content-length: 338 < content-type: application/json; charset=UTF-8 < date: Thu, 08 Oct 2020 13:15:14 GMT < server: istio-envoy < x-envoy-upstream-service-time: 52 < {\"model_name\": \"cifar\", \"model_version\": \"1\", \"outputs\": [{\"name\": \"OUTPUT__0\", \"datatype\": \"FP32\", \"shape\": [1, 10], \"data\": [-0.7299326062202454, -2.186835289001465, -0.029627874493598938, 2.3753483295440674, -0.3476247489452362, 1.3253062963485718, 0.5721136927604675, 0.049311548471450806, -0.3691796362400055, -1.0804035663604736]}]}","title":"Run a prediction from curl"},{"location":"modelserving/v1beta1/xgboost/","text":"Deploying XGBoost models \u00b6 This example walks you through how to deploy a xgboost model leveraging the v1beta1 version of the InferenceService CRD. Note that, by default the v1beta1 version will expose your model through an API compatible with the existing V1 Dataplane. However, this example will show you how to serve a model through an API compatible with the new V2 Dataplane . Training \u00b6 The first step will be to train a sample xgboost model. We will save this model as model.bst . import xgboost as xgb from sklearn.datasets import load_iris import os model_dir = \".\" BST_FILE = \"model.bst\" iris = load_iris () y = iris [ 'target' ] X = iris [ 'data' ] dtrain = xgb . DMatrix ( X , label = y ) param = { 'max_depth' : 6 , 'eta' : 0.1 , 'silent' : 1 , 'nthread' : 4 , 'num_class' : 10 , 'objective' : 'multi:softmax' } xgb_model = xgb . train ( params = param , dtrain = dtrain ) model_file = os . path . join (( model_dir ), BST_FILE ) xgb_model . save_model ( model_file ) Testing locally \u00b6 Once we've got our model.bst model serialised, we can then use MLServer to spin up a local server. For more details on MLServer, feel free to check the XGBoost example in their docs . Note that this step is optional and just meant for testing. Feel free to jump straight to deploying your trained model . Pre-requisites \u00b6 Firstly, to use MLServer locally, you will first need to install the mlserver package in your local environment as well as the XGBoost runtime. pip install mlserver mlserver-xgboost Model settings \u00b6 The next step will be providing some model settings so that MLServer knows: The inference runtime that we want our model to use (i.e. mlserver_xgboost.XGBoostModel ) Our model's name and version These can be specified through environment variables or by creating a local model-settings.json file: { \"name\" : \"xgboost-iris\" , \"version\" : \"v1.0.0\" , \"implementation\" : \"mlserver_xgboost.XGBoostModel\" } Note that, when we deploy our model , KServe will already inject some sensible defaults so that it runs out-of-the-box without any further configuration. However, you can still override these defaults by providing a model-settings.json file similar to your local one. You can even provide a set of model-settings.json files to load multiple models . Serving our model locally \u00b6 With the mlserver package installed locally and a local model-settings.json file, we should now be ready to start our server as: mlserver start . Deployment \u00b6 Lastly, we will use KServe to deploy our trained model. For this, we will just need to use version v1beta1 of the InferenceService CRD and set the the protocolVersion field to v2 . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"xgboost-iris\" spec : predictor : xgboost : protocolVersion : \"v2\" storageUri : \"gs://kfserving-samples/models/xgboost/iris\" Note that this makes the following assumptions: Your model weights (i.e. your model.bst file) have already been uploaded to a \"model repository\" (GCS in this example) and can be accessed as gs://kfserving-samples/models/xgboost/iris . There is a K8s cluster available, accessible through kubectl . KServe has already been installed in your cluster . Assuming that we've got a cluster accessible through kubectl with KServe already installed, we can deploy our model as: kubectl apply -f ./xgboost.yaml Testing deployed model \u00b6 We can now test our deployed model by sending a sample request. Note that this request needs to follow the V2 Dataplane protocol . You can see an example payload below: { \"inputs\" : [ { \"name\" : \"input-0\" , \"shape\" : [ 2 , 4 ], \"datatype\" : \"FP32\" , \"data\" : [ [ 6.8 , 2.8 , 4.8 , 1.4 ], [ 6.0 , 3.4 , 4.5 , 1.6 ] ] } ] } Now, assuming that our ingress can be accessed at ${INGRESS_HOST}:${INGRESS_PORT} , we can use curl to send our inference request as: You can follow these instructions to find out your ingress IP and port. SERVICE_HOSTNAME = $( kubectl get inferenceservice xgboost-iris -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v \\ -H \"Host: ${ SERVICE_HOSTNAME } \" \\ -d @./iris-input.json \\ http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/xgboost-iris/infer The output will be something similar to: { \"id\" : \"4e546709-0887-490a-abd6-00cbc4c26cf4\" , \"model_name\" : \"xgboost-iris\" , \"model_version\" : \"v1.0.0\" , \"outputs\" : [ { \"data\" : [ 1.0 , 1.0 ], \"datatype\" : \"FP32\" , \"name\" : \"predict\" , \"parameters\" : null , \"shape\" : [ 2 ] } ] }","title":"XGBoost"},{"location":"modelserving/v1beta1/xgboost/#deploying-xgboost-models","text":"This example walks you through how to deploy a xgboost model leveraging the v1beta1 version of the InferenceService CRD. Note that, by default the v1beta1 version will expose your model through an API compatible with the existing V1 Dataplane. However, this example will show you how to serve a model through an API compatible with the new V2 Dataplane .","title":"Deploying XGBoost models"},{"location":"modelserving/v1beta1/xgboost/#training","text":"The first step will be to train a sample xgboost model. We will save this model as model.bst . import xgboost as xgb from sklearn.datasets import load_iris import os model_dir = \".\" BST_FILE = \"model.bst\" iris = load_iris () y = iris [ 'target' ] X = iris [ 'data' ] dtrain = xgb . DMatrix ( X , label = y ) param = { 'max_depth' : 6 , 'eta' : 0.1 , 'silent' : 1 , 'nthread' : 4 , 'num_class' : 10 , 'objective' : 'multi:softmax' } xgb_model = xgb . train ( params = param , dtrain = dtrain ) model_file = os . path . join (( model_dir ), BST_FILE ) xgb_model . save_model ( model_file )","title":"Training"},{"location":"modelserving/v1beta1/xgboost/#testing-locally","text":"Once we've got our model.bst model serialised, we can then use MLServer to spin up a local server. For more details on MLServer, feel free to check the XGBoost example in their docs . Note that this step is optional and just meant for testing. Feel free to jump straight to deploying your trained model .","title":"Testing locally"},{"location":"modelserving/v1beta1/xgboost/#pre-requisites","text":"Firstly, to use MLServer locally, you will first need to install the mlserver package in your local environment as well as the XGBoost runtime. pip install mlserver mlserver-xgboost","title":"Pre-requisites"},{"location":"modelserving/v1beta1/xgboost/#model-settings","text":"The next step will be providing some model settings so that MLServer knows: The inference runtime that we want our model to use (i.e. mlserver_xgboost.XGBoostModel ) Our model's name and version These can be specified through environment variables or by creating a local model-settings.json file: { \"name\" : \"xgboost-iris\" , \"version\" : \"v1.0.0\" , \"implementation\" : \"mlserver_xgboost.XGBoostModel\" } Note that, when we deploy our model , KServe will already inject some sensible defaults so that it runs out-of-the-box without any further configuration. However, you can still override these defaults by providing a model-settings.json file similar to your local one. You can even provide a set of model-settings.json files to load multiple models .","title":"Model settings"},{"location":"modelserving/v1beta1/xgboost/#serving-our-model-locally","text":"With the mlserver package installed locally and a local model-settings.json file, we should now be ready to start our server as: mlserver start .","title":"Serving our model locally"},{"location":"modelserving/v1beta1/xgboost/#deployment","text":"Lastly, we will use KServe to deploy our trained model. For this, we will just need to use version v1beta1 of the InferenceService CRD and set the the protocolVersion field to v2 . apiVersion : \"serving.kserve.io/v1beta1\" kind : \"InferenceService\" metadata : name : \"xgboost-iris\" spec : predictor : xgboost : protocolVersion : \"v2\" storageUri : \"gs://kfserving-samples/models/xgboost/iris\" Note that this makes the following assumptions: Your model weights (i.e. your model.bst file) have already been uploaded to a \"model repository\" (GCS in this example) and can be accessed as gs://kfserving-samples/models/xgboost/iris . There is a K8s cluster available, accessible through kubectl . KServe has already been installed in your cluster . Assuming that we've got a cluster accessible through kubectl with KServe already installed, we can deploy our model as: kubectl apply -f ./xgboost.yaml","title":"Deployment"},{"location":"modelserving/v1beta1/xgboost/#testing-deployed-model","text":"We can now test our deployed model by sending a sample request. Note that this request needs to follow the V2 Dataplane protocol . You can see an example payload below: { \"inputs\" : [ { \"name\" : \"input-0\" , \"shape\" : [ 2 , 4 ], \"datatype\" : \"FP32\" , \"data\" : [ [ 6.8 , 2.8 , 4.8 , 1.4 ], [ 6.0 , 3.4 , 4.5 , 1.6 ] ] } ] } Now, assuming that our ingress can be accessed at ${INGRESS_HOST}:${INGRESS_PORT} , we can use curl to send our inference request as: You can follow these instructions to find out your ingress IP and port. SERVICE_HOSTNAME = $( kubectl get inferenceservice xgboost-iris -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) curl -v \\ -H \"Host: ${ SERVICE_HOSTNAME } \" \\ -d @./iris-input.json \\ http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v2/models/xgboost-iris/infer The output will be something similar to: { \"id\" : \"4e546709-0887-490a-abd6-00cbc4c26cf4\" , \"model_name\" : \"xgboost-iris\" , \"model_version\" : \"v1.0.0\" , \"outputs\" : [ { \"data\" : [ 1.0 , 1.0 ], \"datatype\" : \"FP32\" , \"name\" : \"predict\" , \"parameters\" : null , \"shape\" : [ 2 ] } ] }","title":"Testing deployed model"}]}